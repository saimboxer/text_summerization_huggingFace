{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "anyio==4.4.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.1\n",
      "async-lru==2.0.4\n",
      "attrs==23.2.0\n",
      "Babel==2.15.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "Brotli==1.1.0\n",
      "certifi==2024.7.4\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "comm==0.2.2\n",
      "contourpy==1.2.1\n",
      "cycler==0.12.1\n",
      "datasets==2.20.0\n",
      "debugpy==1.8.2\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.8\n",
      "executing==2.0.1\n",
      "fastjsonschema==2.20.0\n",
      "filelock==3.15.4\n",
      "fonttools==4.53.1\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.5.0\n",
      "h11==0.14.0\n",
      "httpcore==1.0.5\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.24.2\n",
      "idna==3.7\n",
      "inflate64==1.0.0\n",
      "ipykernel==6.29.5\n",
      "ipython==8.26.0\n",
      "ipywidgets==8.1.3\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "json5==0.9.25\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.0.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.10.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "jupyter_server==2.14.2\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.2.4\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "jupyterlab_widgets==3.0.11\n",
      "kiwisolver==1.4.5\n",
      "lxml==5.2.2\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib==3.9.1\n",
      "matplotlib-inline==0.1.7\n",
      "mistune==3.0.2\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multiprocess==0.70.16\n",
      "multivolumefile==0.2.3\n",
      "nbclient==0.10.0\n",
      "nbconvert==7.16.4\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "nltk==3.8.1\n",
      "notebook==7.2.1\n",
      "notebook_shim==0.2.4\n",
      "numpy==1.26.4\n",
      "overrides==7.7.0\n",
      "packaging==24.1\n",
      "pandas==2.2.2\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pillow==10.4.0\n",
      "platformdirs==4.2.2\n",
      "portalocker==2.10.1\n",
      "prometheus_client==0.20.0\n",
      "prompt_toolkit==3.0.47\n",
      "protobuf==5.27.2\n",
      "psutil==6.0.0\n",
      "pure_eval==0.2.3\n",
      "py7zr==0.21.1\n",
      "pyarrow==17.0.0\n",
      "pyarrow-hotfix==0.6\n",
      "pybcj==1.0.2\n",
      "pycparser==2.22\n",
      "pycryptodomex==3.20.0\n",
      "Pygments==2.18.0\n",
      "pyparsing==3.1.2\n",
      "pyppmd==1.1.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-json-logger==2.0.7\n",
      "pytz==2024.1\n",
      "pywin32==306\n",
      "pywinpty==2.0.13\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "pyzstd==0.16.0\n",
      "qtconsole==5.5.2\n",
      "QtPy==2.4.1\n",
      "referencing==0.35.1\n",
      "regex==2024.5.15\n",
      "requests==2.32.3\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rouge_score==0.1.2\n",
      "rpds-py==0.19.0\n",
      "sacrebleu==2.4.2\n",
      "safetensors==0.4.3\n",
      "Send2Trash==1.8.3\n",
      "sentencepiece==0.2.0\n",
      "six==1.16.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.5\n",
      "stack-data==0.6.3\n",
      "sympy==1.13.1\n",
      "tabulate==0.9.0\n",
      "terminado==0.18.1\n",
      "texttable==1.7.0\n",
      "tinycss2==1.3.0\n",
      "tokenizers==0.19.1\n",
      "torch==2.2.1\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.43.2\n",
      "types-python-dateutil==2.9.0.20240316\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2024.1\n",
      "uri-template==1.3.0\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "webcolors==24.6.0\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "widgetsnbextension==4.0.11\n",
      "xxhash==3.4.1\n",
      "yarl==1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# # Install necessary packages for text summarization\n",
    "!pip install transformers[sentencepiece] sacrebleu -q\n",
    "!pip install requests==2.31.0\n",
    "!pip install \"pyarrow>=14.0.1,<15.0.0\"\n",
    "!pip install py7zr\n",
    "!pip install datasets\n",
    "!pip install rouge_score\n",
    "!pip install matplotlib\n",
    "!pip install pandas nltk tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pipeline and set_seed function from the transformers library\n",
    "# The pipeline function provides an easy way to use pretrained models for various tasks\n",
    "# such as text generation, summarization, translation, and more\n",
    "# The set_seed function is used to ensure reproducibility of results by setting a random seed\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Import matplotlib for plotting graphs and visualizations\n",
    "# This library is used to create static, animated, and interactive visualizations in Python\n",
    "# Useful for displaying data and model performance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "# Pandas is a powerful data analysis and manipulation library for Python\n",
    "# Useful for handling datasets, reading/writing CSV files, and data preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Import the AutoModelForSeq2SeqLM and AutoTokenizer classes from the transformers library\n",
    "# AutoModelForSeq2SeqLM is a generic model class for sequence-to-sequence language modeling\n",
    "# Useful for tasks such as translation, summarization, and text generation\n",
    "# AutoTokenizer is used for tokenizing input text to the format required by the model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Import the nltk library and the sent_tokenize function for sentence tokenization\n",
    "# nltk (Natural Language Toolkit) is a suite of libraries and programs for natural language processing\n",
    "# Useful for various text processing tasks like tokenization, lemmatization, and more\n",
    "# sent_tokenize is used to split a text into a list of sentences\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Import the tqdm library for creating progress bars\n",
    "# tqdm is used to show progress bars for loops, making it easier to track the progress of operations\n",
    "# Useful for monitoring the progress of tasks such as data processing and model training\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the torch library for PyTorch functionalities\n",
    "# PyTorch is an open-source machine learning library used for applications such as computer vision and natural language processing\n",
    "# Provides tools for tensor computation, automatic differentiation, and more\n",
    "import torch\n",
    "\n",
    "# Download the \"punkt\" tokenizer model from nltk\n",
    "# The \"punkt\" tokenizer is a pre-trained model for tokenizing text into sentences\n",
    "# Useful for splitting a large text into individual sentences for further processing\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to load datasets from the 'datasets' library\n",
    "# The 'datasets' library provides a wide range of datasets and tools for handling and processing data,\n",
    "# making it easier to access datasets from various sources including the Hugging Face hub or local files.\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import the function to load evaluation metrics from the 'datasets' library\n",
    "# Metrics are used to evaluate the performance of machine learning models. The 'datasets' library includes \n",
    "# several standard metrics, allowing you to assess how well your model's predictions align with human evaluations.\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing AutoModelForSeq2SeqLM and AutoTokenizer from the 'transformers' library\n",
    "\n",
    "# AutoModelForSeq2SeqLM is a class that provides a generic interface to any pre-trained sequence-to-sequence model.\n",
    "# Sequence-to-sequence models are used for tasks like text summarization, translation, and other tasks where \n",
    "# an input sequence is transformed into an output sequence.\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# AutoTokenizer is a class that provides a tokenizer for any pre-trained model.\n",
    "# Tokenizers convert text into a format that the model can understand (e.g., converting text to tokens or IDs).\n",
    "# This is a crucial step before passing text data to the model for processing.\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4496a9ebaa174115bafb106da728daf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saima\\.cache\\huggingface\\hub\\models--google--pegasus-cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7da4bc3ad814ca4b3c8aafa28a6b7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39d32e909394e21a4dbb665bc4ac4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bc7f869ada43d7afafb5786da56342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917278ddff4e451aa190648b4ead8cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d4e508244244bb99637d0d5ddc3c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using model checkpoint: google/pegasus-cnn_dailymail\n",
      "Tokenizer type: <class 'transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast'>\n",
      "Model type: <class 'transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "# Set the device to \"cuda\" if a GPU with CUDA support is available, otherwise use \"cpu\".\n",
    "# This allows you to leverage GPU acceleration for faster model training and inference if a compatible GPU is present.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Specify the model checkpoint identifier for a pre-trained model.\n",
    "# In this case, 'google/pegasus-cnn_dailymail' refers to a specific pre-trained PEGASUS model fine-tuned on the CNN/DailyMail dataset.\n",
    "# This model is used for tasks like text summarization, leveraging its pre-trained capabilities to generate summaries from input text.\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "# Load the tokenizer associated with the pre-trained model checkpoint.\n",
    "# The tokenizer converts text into tokens or IDs that the model can process.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Load the pre-trained model specified by the checkpoint and move it to the specified device (CPU or GPU).\n",
    "# The model is used for generating predictions based on the input data.\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using model checkpoint: {model_ckpt}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "print(f\"Model type: {type(model_pegasus)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"\n",
    "    Splits a list into smaller batches of a specified size.\n",
    "\n",
    "    This function is useful for processing large datasets in smaller,\n",
    "    more manageable chunks, especially when dealing with memory constraints\n",
    "    or parallelizing computations.\n",
    "\n",
    "    Args:\n",
    "        list_of_elements: The list to be split into batches.\n",
    "        batch_size: The desired size of each batch.\n",
    "\n",
    "    Yields:\n",
    "        Successive batch-sized chunks from the input list.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=8, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    \"\"\"\n",
    "    Evaluates a summarization model on a test dataset using the specified metric.\n",
    "\n",
    "    Args:\n",
    "    - dataset: The test dataset containing the text and summaries.\n",
    "    - metric: The evaluation metric (e.g., ROUGE) to compute.\n",
    "    - model: The pre-trained summarization model.\n",
    "    - tokenizer: The tokenizer associated with the model.\n",
    "    - batch_size: The number of samples to process in each batch.\n",
    "    - device: The device (CPU/GPU) to run the model on.\n",
    "    - column_text: The column name in the dataset containing the articles.\n",
    "    - column_summary: The column name in the dataset containing the reference summaries.\n",
    "\n",
    "    Returns:\n",
    "    - score: The computed metric score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the articles and summaries into batches\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    # Loop through each batch of articles and corresponding summaries\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        # Tokenize the articles in the batch\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
    "                           padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # Generate summaries using the model\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        # length_penalty ensures that the model does not generate sequences that are too long\n",
    "\n",
    "        # Decode the generated summaries into readable text\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                              clean_up_tokenization_spaces=True)\n",
    "                             for s in summaries]\n",
    "\n",
    "        # Replace empty strings with a space to avoid issues\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "        # Add the decoded summaries and the reference summaries to the metric\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    # Compute and return the final metric score\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the 'samsum' dataset using the 'datasets' library.\n",
    "# The 'samsum' dataset is a dialogue summarization dataset consisting of dialogues and their corresponding summaries.\n",
    "# This dataset is useful for training and evaluating models on tasks such as text summarization and dialogue summarization.\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "print(dataset_samsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first example from the training set of the 'samsum' dataset.\n",
    "# This allows you to inspect the format and contents of the dataset, including the text and summary fields.\n",
    "print(dataset_samsum[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for pandas DataFrames to show all rows and columns.\n",
    "# This is useful for inspecting entire DataFrames without truncation, especially when dealing with large or complex data.\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                           dialogue  \\\n",
      "0     13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
      "1     13728867  Olivia: Who are you voting for in this electio...   \n",
      "2     13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
      "3     13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4     13728094  Sam: hey  overheard rick say something\\r\\nSam:...   \n",
      "5     13716343  Neville: Hi there, does anyone remember what d...   \n",
      "6     13611672  John: Ave. Was there any homework for tomorrow...   \n",
      "7     13730463  Sarah: I found a song on youtube and I think y...   \n",
      "8     13809976  Noah: When and where are we meeting? :)\\r\\nMad...   \n",
      "9     13809912  Matt: Do you want to go for date?\\r\\nAgnes: Wo...   \n",
      "10    13727633  Lucas: Hey! How was your day?\\r\\nDemi: Hey the...   \n",
      "11    13729168  Mark: I just shipped the goods\\r\\nMark: Tomorr...   \n",
      "12    13864825  Anita: I'm at the station in Bologna\\nJenny: N...   \n",
      "13    13729567  Leon: did you find the job yet?\\r\\nArthur: no ...   \n",
      "14    13864634  Macca: i'm so exited today\\nAdrien: why?\\nMacc...   \n",
      "15    13815560  Isabella: fuck my life, I'm so not able to get...   \n",
      "16    13731403  Tina: I'd only like to remind you that you owe...   \n",
      "17    13729191  Betty: Please remind me next time that too muc...   \n",
      "18    13827937  Mary: Hi Mike!\\r\\nMike: Hello :)\\r\\nMary: do u...   \n",
      "19    13828064  Laura: ok , I'm done for today-)\\r\\nLaura: let...   \n",
      "20    13716048  Ashley: Guys, you have to read this book!  <fi...   \n",
      "21    13828741  Aria: You won't believe who I've just met!\\r\\n...   \n",
      "22    13681946  Anna: where are you\\r\\nOmenah: at home \\r\\nAnn...   \n",
      "23    13728653  Renee: Just saying Hi. Thought of you this mor...   \n",
      "24    13818918  Jonas: Iâ€™m running 10 minutes late. Could you ...   \n",
      "25    13810064  Julius: dude, your assessment of manutd\\r\\nLaw...   \n",
      "26    13729933  Jade: are you going to that trip\\r\\nWayne: was...   \n",
      "27    13830054  Natalie: are you still going to Thailand?\\r\\nJ...   \n",
      "28    13716981  Elisa: Who wants to come for drinks tonight at...   \n",
      "29    13728757  Hal: Have you got any homework for tomorrow?\\r...   \n",
      "30    13716573  Ray: Hey guys, I don't know if you heard but s...   \n",
      "31  13731370-1  Eric: Champions League is coming soon :D\\r\\nCu...   \n",
      "32    13730401  Gunther: did you pay for coffee?\\r\\nChandler: ...   \n",
      "33    13716152  Karen: <file_photo>\\r\\nJennifer: OH. MY. GOD. ...   \n",
      "34    13715883  Ted: Any news about weekend?\\r\\nJake: About th...   \n",
      "35    13809921  Bradley: It's very safe. Not like they make it...   \n",
      "36    13680760  Lucia: I need my hair cut.\\r\\nLucia: When can ...   \n",
      "37    13612028  Gabriella: Hey Jasmine, how are you doing?\\r\\n...   \n",
      "38    13727812  Grace: Hey lady\\r\\nGrace: Today I saw you in t...   \n",
      "39    13862922  Kerri: Did you see the apartment?\\nStella: yes...   \n",
      "40    13862359  Andrea: hey Babes, how's it going? I've got so...   \n",
      "41    13828591  Victor: do you want to go to the museum tonigh...   \n",
      "42    13728090  William: Hey. Today i saw you were arguing wit...   \n",
      "43    13682071  Henry: Hey Lily, tried to catch you on landlin...   \n",
      "44    13682379  Linda: I'm going to have my room painted\\r\\nLi...   \n",
      "45    13682313  Juliette: So what? Tell me\\r\\nJimmy: One minut...   \n",
      "46    13730056  Mary: Hi <3\\r\\nPeter: Hey gorgeous ;)\\r\\nMary:...   \n",
      "47    13728383  Francesca: It's 3 a.m. You're not asleep yet\\r...   \n",
      "48    13715767  Ted: Feeling sorry for.. myself! \\r\\nKelly: Wh...   \n",
      "49    13729983  Chloe: Hey, you told me about this cream that ...   \n",
      "50    13828128  Pitt: Hey Teddy! Have you received my message?...   \n",
      "51    13731319  Mary: Sorry, I didn't make it to your bday par...   \n",
      "52    13819305  Connor: I'm too tired to come to the meeting I...   \n",
      "53    13813481  Karen: Where'd you buy this dress you had at t...   \n",
      "54    13715772  Gabi: What do you guys want for Christmas? Let...   \n",
      "55    13828719  Daniel: Hi Jeff, I was just browsing thought y...   \n",
      "56  13611954-1  Client: Good afternoon. I suggest you adjust t...   \n",
      "57    13865453  Jeff: Are you ready for the hiking tomorrow?\\n...   \n",
      "58    13864440  Joyce: Guys, sorry I'm running late today! Wil...   \n",
      "59    13817541  Aiden: Have you subscribed my channel?\\r\\nMia:...   \n",
      "60    13864732  Oli: I've talked to some people from the third...   \n",
      "61    13680485  Blake: Why did you tell Steven all those stupi...   \n",
      "62    13828302  Roberta: <file_photo>\\r\\nRoberta: look what I ...   \n",
      "63    13680126  Sawyer: I hope I found you wrapped in a towel ...   \n",
      "64    13865216  Sam: I'm so sorry. I can't make it on time.\\nS...   \n",
      "65    13731409  Sam: I just got my 1st credit card!\\r\\nTom: We...   \n",
      "66    13727597  James: <file_video>\\r\\nJames: My girlfriend is...   \n",
      "67    13716473  Christopher: ok, people, Iâ€™ve got an idea\\r\\nM...   \n",
      "68    13727770  Francine: hey hon\\r\\nFrancine: how are you tod...   \n",
      "69    13821815  Maria: Guys, don't bring anything, I've cooked...   \n",
      "70    13865139  Hannah: The motherfucker took my spot again\\nP...   \n",
      "71    13680779  Sophia: What should I get them?\\r\\nMonica: How...   \n",
      "72    13729889  Yannick: I heard you are going to sing the ant...   \n",
      "73    13715995  Carter: <file_photo> Sure it's nearly Christma...   \n",
      "74    13862502  Ammalee: <file_photo>\\nAmmalee: This lasted ov...   \n",
      "75    13828775  Alan: sweetie when wil you be home?\\r\\nJoan: I...   \n",
      "76  13731314-1  Johny: That girl on Tuesday music video is sex...   \n",
      "77    13611794  Frank: Hi Peter has brought me to Sports Direc...   \n",
      "78    13828614  Kyle: hey u got maths homework?\\r\\nPatrick: um...   \n",
      "79    13680746  Ellen: Hi, honey, sorry I've been so unreachab...   \n",
      "80    13819839  Paola: Guys, as I was saying Iâ€™d like to take ...   \n",
      "81    13865472  Harriet: Where are you?\\nVincent: In the clois...   \n",
      "82    13716468  Hyatt: watch channel 6 now\\r\\nNickleby: whats ...   \n",
      "83    13810089  Leo: hey jess\\r\\nJessie: hey\\r\\nLeo: so what'r...   \n",
      "84    13730151  John: do you play tennis? :-)\\r\\nIrma: i don't...   \n",
      "85    13682432  Jack: Where are you? I can't find you.\\r\\nOliv...   \n",
      "86    13681293  Pete: Sup?\\r\\nLionel: I'm at a meeting, can't ...   \n",
      "87    13681024  Rob: need a car check, u know a good mechanic?...   \n",
      "88    13716734  Andrew: wow, weekend! finally!\\r\\nAndrew: this...   \n",
      "89    13731380  Olivia: hey\\r\\nTaylor: whats up?\\r\\nOlivia: I ...   \n",
      "90    13829580  John: Maybe some ride?\\r\\nIan: Always!!!\\r\\nIa...   \n",
      "91    13728061  Ali: dude i need that hard drive\\r\\nKane: come...   \n",
      "92    13680149  Joshua: look out the window\\r\\nNoah: what's th...   \n",
      "93    13612157  Greg: why donâ€™t you answer my calls?\\r\\nAva: c...   \n",
      "94    13728974  Shelly: I'm looking for a fall coat, are there...   \n",
      "95    13863020  Connor: hello can you tell me what songs did t...   \n",
      "96    13819450  Caleb: How are you guys?\\r\\nJeniffer: very goo...   \n",
      "97    13716378  Max: I'm so sorry Lucas. I don't know what got...   \n",
      "98    13862395  O'Neill: Is everything ok?\\nO'Neill: I didn't ...   \n",
      "99    13611608  Tom: Howâ€™s the weather in Poland now?\\r\\nJusti...   \n",
      "\n",
      "                                              summary  \n",
      "0   Amanda baked cookies and will bring Jerry some...  \n",
      "1   Olivia and Olivier are voting for liberals in ...  \n",
      "2   Kim may try the pomodoro technique recommended...  \n",
      "3   Edward thinks he is in love with Bella. Rachel...  \n",
      "4   Sam is confused, because he overheard Rick com...  \n",
      "5   Wyatt reminds Neville his wedding anniversary ...  \n",
      "6   John didn't show up for class due to some work...  \n",
      "7   Sarah sends James an instrumental song he migh...  \n",
      "8   Noah wants to meet, he quit his job, because h...  \n",
      "9   Matt invites Agnes for a date to get to know e...  \n",
      "10  Demi got promoted. She will celebrate that wit...  \n",
      "11  Mark just shipped the goods and he will send G...  \n",
      "12                       Anita is at Bologna station.  \n",
      "13  Arthur is still unemployed. Leon sends him a j...  \n",
      "14  Macca has done ice climbing for the first time...  \n",
      "15  Isabella feels bad after the Christmas party. ...  \n",
      "16  Lucy owes Tina 50 dollars. She made a transfer...  \n",
      "17  Betty feels remorse she got drunk last night a...  \n",
      "18  Mike and Mary are going to visit Mike's grandm...  \n",
      "19  Laura will pick up Kim from work around 7, and...  \n",
      "20  Erin is convinced by Ashley's book recommendat...  \n",
      "21  Aria has just run into Charlie Evans. He is no...  \n",
      "22  Omenah is at home, Anna will be there in a min...  \n",
      "23  Layla the dog misses Rachel. She is having a k...  \n",
      "24  Jonas will be 10 minutes late. Natalie will le...  \n",
      "25  Lawrence doesn't like the play of Manchester U...  \n",
      "26  The trip Wayne was going to go on was postpone...  \n",
      "27  Jason is going to Thailand next week. Natalie ...  \n",
      "28  Elisa, Sadie, Carol, Liam, Tom and John want t...  \n",
      "29  Amy and Hal will have dinner together when he ...  \n",
      "30  Ray's bike was stolen from the 9th street yest...  \n",
      "31  Eric wants to bet during the Champions League ...  \n",
      "32         Chandler will pay for his coffee tomorrow.  \n",
      "33  Karen looked very good on her 5th anniversary ...  \n",
      "34  Ted, Jake, Pia, Jessica and Jess are having a ...  \n",
      "35  Julianna tells Bradley about Europe's law and ...  \n",
      "36  Lucia needs a new hairstyle due to a change of...  \n",
      "37  Gabriella asked Jasmine to check her CV which ...  \n",
      "38  Grace saw Ruth in Galitos but she thought it w...  \n",
      "39  Stella wants to see the apartment in the morni...  \n",
      "40  Andrea must correct 50% of 20 short texts for ...  \n",
      "41  Charles will go with Victor to the museum toni...  \n",
      "42  Elizabeth had an argument with Blackett today,...  \n",
      "43  Henry and Lily take William with them to Riehe...  \n",
      "44  According to Brian, colors that match Linda's ...  \n",
      "45  Jimmy is going to take medication for a month ...  \n",
      "46  Peter will be back home in 2 days. Mary would ...  \n",
      "47  Neither Francesca nor Jacob can sleep. Jacob h...  \n",
      "48                         Ted has twisted his ankle.  \n",
      "49  Julie recommends the GlamRock cream from the G...  \n",
      "50        Teddy has a message from Pitt on Messenger.  \n",
      "51  Mary didn't come to Nick's birthday party. She...  \n",
      "52  Connor will not attend the meeting, he has to ...  \n",
      "53  Samara bought the dress she had at the wedding...  \n",
      "54  Mary, Kate, Peter and John want various items ...  \n",
      "55  Jeff will double check Mindy's reaction, conne...  \n",
      "56  Client informs Flix about a 40 minutes delay o...  \n",
      "57  Jeff, Ann, Corina, and Maria are going to hike...  \n",
      "58  Joyce and Andrew are running late. Carla and A...  \n",
      "59  Mia hasn't subscribed to Aiden's channel yet b...  \n",
      "60  Oli, Jacob and Marcia have to prepare for a ve...  \n",
      "61  Blake believes that Alexis is trying to turn t...  \n",
      "62  Roberta found Makoto's dress and will send it ...  \n",
      "63  When Eleanor meets Sawyer, she will be wearing...  \n",
      "64  Sam will be 30 minutes late. Sandra and StaÅ› w...  \n",
      "65                     Sam got her first credit card.  \n",
      "66  James's girlfriend made him put the trash bin ...  \n",
      "67  To Christopher's frustration, Matthew, Andrew ...  \n",
      "68  Francine and Jessie talked yesterday. The talk...  \n",
      "69  Maria has cooked a lot of food. Andrew will br...  \n",
      "70  Richard took Hannah's parking spot again. Anne...  \n",
      "71  Sophie doesn't know what to buy for a gift. Sh...  \n",
      "72     Nicki is going to sing the anthem at the game.  \n",
      "73  Christmas is coming. Chloe will buy Carter new...  \n",
      "74  Ammalee sent Maryann a photo of her nails that...  \n",
      "75  Alan is making his famous chicken wings and wo...  \n",
      "76  Johny wants to go out clubbing with Luke in or...  \n",
      "77  Rosie is on her way to meet Frank and will hav...  \n",
      "78  Kyle reminds Patrick about their math homework...  \n",
      "79  Ellen and Paul are doing the redecoration and ...  \n",
      "80  Paola and her friends are going to the theatre...  \n",
      "81  Vincent and Charlotte are studying in the cloi...  \n",
      "82                           Stu is on channel 6 now.  \n",
      "83  Jessie and Leo will hang out at the weekend, b...  \n",
      "84  Irma doesn't play tennis. John and Irma will e...  \n",
      "85  Jack has to find Olivia in carriage 3, because...  \n",
      "86             Lionel is in a meeting and can't talk.  \n",
      "87  Will gived Rob the number of the car mechanic ...  \n",
      "88  Andrew, Nicky and Rick had a hard week at work...  \n",
      "89  Olivia doesn't know how to caption the photo s...  \n",
      "90      Ian and John will meet at St. Monica at 8.00.  \n",
      "91       Ali is coming to Kane to get the hard drive.  \n",
      "92                              It's snowing outside.  \n",
      "93  Ava doesn't want to talk to Greg any more and ...  \n",
      "94  During early lunchtime, Shelly will come to Po...  \n",
      "95  Connor is looking for a playlist from the Berl...  \n",
      "96  Jeniffer and Brooke're in New York now. They'v...  \n",
      "97  Max is sorry about his behaviour so wants to m...  \n",
      "98  O'Neill is worried about not having heard from...  \n",
      "99  It's getting cooler in Poland, because winter ...  \n"
     ]
    }
   ],
   "source": [
    "# Convert the training set of the 'samsum' dataset to a pandas DataFrame for easier manipulation and inspection.\n",
    "# This DataFrame allows you to work with the dataset in a tabular format, which is useful for exploration and analysis.\n",
    "df_train = dataset_samsum[\"train\"].to_pandas()\n",
    "\n",
    "# Print the first 100 rows of the DataFrame to get an overview of the dataset.\n",
    "# Displaying a subset of rows helps in quickly examining the structure and contents of the data.\n",
    "print(df_train.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the number of examples in each split (e.g., train, validation, test) of the 'samsum' dataset.\n",
    "# This helps in understanding the size of each dataset split.\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "\n",
    "# Print the column names (features) for the training set to understand what data is available.\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "\n",
    "# Print a sample dialogue from the test set to inspect the format and content of the dialogues.\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "# Print the corresponding summary for the sample dialogue from the test set.\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating PEGASUS on SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "# Print the dialogue from the first example in the test split of the 'samsum' dataset\n",
    "print(\"Dialogue:\")\n",
    "print(dataset_samsum['test'][0]['dialogue'])\n",
    "\n",
    "# Print the corresponding summary from the first example in the test split\n",
    "print('\\nSummary:')\n",
    "print(dataset_samsum['test'][0]['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the summarization pipeline\n",
    "# The 'pipeline' function from the 'transformers' library simplifies the process of using pre-trained models.\n",
    "# 'summarization' indicates that we are creating a pipeline for generating summaries of text.\n",
    "# 'model=model_ckpt' specifies the pre-trained model to be used for the summarization task.\n",
    "pipe = pipeline('summarization', model=model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Generated Summary': \"Amanda: Ask Larry Amanda: He called her last time we were at the park together .\\nHannah: I'd rather you texted him .\\nAmanda: Just text him .\"}\n"
     ]
    }
   ],
   "source": [
    "# Apply the pre-trained summarization model pipeline to generate a summary for the given input text.\n",
    "# 'pipe' is a pipeline object created for the 'summarization' task with a specified model.\n",
    "# 'dataset_samsum['test'][0]['dialogue']' provides the input text (dialogue) from the test dataset.\n",
    "pipe_out = pipe(dataset_samsum['test'][0]['dialogue'])\n",
    "\n",
    "# Extract the generated summary from the pipeline output.\n",
    "# 'pipe_out' is a list of dictionaries where each dictionary contains the generated summary.\n",
    "# The key 'summary_text' holds the summary in text format.\n",
    "summary_text = pipe_out[0]['summary_text']\n",
    "\n",
    "# Format the summary text by replacing occurrences of a period followed by an optional space and <n> with a period and newline.\n",
    "# This helps in structuring the summary into more readable lines by adding line breaks.\n",
    "formatted_summary = re.sub(r'\\. ?<n>', '.\\n', summary_text)\n",
    "\n",
    "\n",
    "# Print the formatted summary to the console.\n",
    "# This will display the generated summary with line breaks for better readability.\n",
    "print({\"Generated Summary\": formatted_summary})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/103 [14:58:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m rouge_metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the ROUGE score on the test dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metric_on_test_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_samsum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The test dataset split\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouge_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# The ROUGE metric to use\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_pegasus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# The pre-trained summarization model\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# The corresponding tokenizer\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdialogue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# The column containing the input text\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The column containing the reference summaries\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Batch size for processing\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m, in \u001b[0;36mcalculate_metric_on_test_ds\u001b[1;34m(dataset, metric, model, tokenizer, batch_size, device, column_text, column_summary)\u001b[0m\n\u001b[0;32m     31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(article_batch, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m                    padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Generate summaries using the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# length_penalty ensures that the model does not generate sequences that are too long\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Decode the generated summaries into readable text\u001b[39;00m\n\u001b[0;32m     41\u001b[0m decoded_summaries \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(s, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m                                       clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m summaries]\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\generation\\utils.py:2028\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2021\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2022\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2023\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2024\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2025\u001b[0m     )\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2028\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2042\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2043\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2049\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2050\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\generation\\utils.py:3187\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3184\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   3186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3187\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3190\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1352\u001b[0m, in \u001b[0;36mPegasusForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1349\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1350\u001b[0m         )\n\u001b[1;32m-> 1352\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1369\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1371\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1214\u001b[0m, in \u001b[0;36mPegasusModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1208\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1209\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1210\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1211\u001b[0m     )\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1214\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1046\u001b[0m, in \u001b[0;36mPegasusDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1034\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1035\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         use_cache,\n\u001b[0;32m   1044\u001b[0m     )\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1046\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1059\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:425\u001b[0m, in \u001b[0;36mPegasusDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m    424\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:244\u001b[0m, in \u001b[0;36mPegasusAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    240\u001b[0m     attn_weights_reshaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    242\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 244\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the ROUGE metric for evaluation\n",
    "rouge_metric = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "# Calculate the ROUGE score on the test dataset\n",
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'],  # The test dataset split\n",
    "    rouge_metric,            # The ROUGE metric to use\n",
    "    model_pegasus,          # The pre-trained summarization model\n",
    "    tokenizer,              # The corresponding tokenizer\n",
    "    column_text='dialogue',   # The column containing the input text\n",
    "    column_summary='summary', # The column containing the reference summaries\n",
    "    batch_size=8             # Batch size for processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summarization_huggingFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
