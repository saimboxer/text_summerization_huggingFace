{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "anyio==4.4.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.1\n",
      "async-lru==2.0.4\n",
      "attrs==23.2.0\n",
      "Babel==2.15.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "Brotli==1.1.0\n",
      "certifi==2024.7.4\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "comm==0.2.2\n",
      "contourpy==1.2.1\n",
      "cycler==0.12.1\n",
      "datasets==2.20.0\n",
      "debugpy==1.8.2\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.8\n",
      "executing==2.0.1\n",
      "fastjsonschema==2.20.0\n",
      "filelock==3.15.4\n",
      "fonttools==4.53.1\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.5.0\n",
      "h11==0.14.0\n",
      "httpcore==1.0.5\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.24.2\n",
      "idna==3.7\n",
      "inflate64==1.0.0\n",
      "ipykernel==6.29.5\n",
      "ipython==8.26.0\n",
      "ipywidgets==8.1.3\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "json5==0.9.25\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.0.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.10.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "jupyter_server==2.14.2\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.2.4\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "jupyterlab_widgets==3.0.11\n",
      "kiwisolver==1.4.5\n",
      "lxml==5.2.2\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib==3.9.1\n",
      "matplotlib-inline==0.1.7\n",
      "mistune==3.0.2\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multiprocess==0.70.16\n",
      "multivolumefile==0.2.3\n",
      "nbclient==0.10.0\n",
      "nbconvert==7.16.4\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "nltk==3.8.1\n",
      "notebook==7.2.1\n",
      "notebook_shim==0.2.4\n",
      "numpy==1.26.4\n",
      "overrides==7.7.0\n",
      "packaging==24.1\n",
      "pandas==2.2.2\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pillow==10.4.0\n",
      "platformdirs==4.2.2\n",
      "portalocker==2.10.1\n",
      "prometheus_client==0.20.0\n",
      "prompt_toolkit==3.0.47\n",
      "protobuf==5.27.2\n",
      "psutil==6.0.0\n",
      "pure_eval==0.2.3\n",
      "py7zr==0.21.1\n",
      "pyarrow==17.0.0\n",
      "pyarrow-hotfix==0.6\n",
      "pybcj==1.0.2\n",
      "pycparser==2.22\n",
      "pycryptodomex==3.20.0\n",
      "Pygments==2.18.0\n",
      "pyparsing==3.1.2\n",
      "pyppmd==1.1.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-json-logger==2.0.7\n",
      "pytz==2024.1\n",
      "pywin32==306\n",
      "pywinpty==2.0.13\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "pyzstd==0.16.0\n",
      "qtconsole==5.5.2\n",
      "QtPy==2.4.1\n",
      "referencing==0.35.1\n",
      "regex==2024.5.15\n",
      "requests==2.32.3\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rouge_score==0.1.2\n",
      "rpds-py==0.19.0\n",
      "sacrebleu==2.4.2\n",
      "safetensors==0.4.3\n",
      "Send2Trash==1.8.3\n",
      "sentencepiece==0.2.0\n",
      "six==1.16.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.5\n",
      "stack-data==0.6.3\n",
      "sympy==1.13.1\n",
      "tabulate==0.9.0\n",
      "terminado==0.18.1\n",
      "texttable==1.7.0\n",
      "tinycss2==1.3.0\n",
      "tokenizers==0.19.1\n",
      "torch==2.2.1\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.43.2\n",
      "types-python-dateutil==2.9.0.20240316\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2024.1\n",
      "uri-template==1.3.0\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "webcolors==24.6.0\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "widgetsnbextension==4.0.11\n",
      "xxhash==3.4.1\n",
      "yarl==1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests==2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests==2.31.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests==2.31.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests==2.31.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests==2.31.0) (2024.7.4)\n",
      "Installing collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "Successfully installed requests-2.31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow<15.0.0,>=14.0.1\n",
      "  Using cached pyarrow-14.0.2-cp311-cp311-win_amd64.whl (24.6 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pyarrow<15.0.0,>=14.0.1) (1.26.4)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 17.0.0\n",
      "    Uninstalling pyarrow-17.0.0:\n",
      "      Successfully uninstalled pyarrow-17.0.0\n",
      "Successfully installed pyarrow-14.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.2 which is incompatible.\n",
      "datasets 2.20.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py7zr in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: texttable in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (0.16.0)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from py7zr) (6.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: requests, pyarrow\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "Successfully installed pyarrow-17.0.0 requests-2.32.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (3.9.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# # Install necessary packages for text summarization\n",
    "!pip install transformers[sentencepiece] sacrebleu -q\n",
    "!pip install requests==2.31.0\n",
    "!pip install \"pyarrow>=14.0.1,<15.0.0\"\n",
    "!pip install py7zr\n",
    "!pip install datasets\n",
    "!pip install rouge_score\n",
    "!pip install matplotlib\n",
    "!pip install pandas nltk tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pipeline and set_seed function from the transformers library\n",
    "# The pipeline function provides an easy way to use pretrained models for various tasks\n",
    "# such as text generation, summarization, translation, and more\n",
    "# The set_seed function is used to ensure reproducibility of results by setting a random seed\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Import matplotlib for plotting graphs and visualizations\n",
    "# This library is used to create static, animated, and interactive visualizations in Python\n",
    "# Useful for displaying data and model performance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "# Pandas is a powerful data analysis and manipulation library for Python\n",
    "# Useful for handling datasets, reading/writing CSV files, and data preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Import the AutoModelForSeq2SeqLM and AutoTokenizer classes from the transformers library\n",
    "# AutoModelForSeq2SeqLM is a generic model class for sequence-to-sequence language modeling\n",
    "# Useful for tasks such as translation, summarization, and text generation\n",
    "# AutoTokenizer is used for tokenizing input text to the format required by the model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Import the nltk library and the sent_tokenize function for sentence tokenization\n",
    "# nltk (Natural Language Toolkit) is a suite of libraries and programs for natural language processing\n",
    "# Useful for various text processing tasks like tokenization, lemmatization, and more\n",
    "# sent_tokenize is used to split a text into a list of sentences\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Import the tqdm library for creating progress bars\n",
    "# tqdm is used to show progress bars for loops, making it easier to track the progress of operations\n",
    "# Useful for monitoring the progress of tasks such as data processing and model training\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the torch library for PyTorch functionalities\n",
    "# PyTorch is an open-source machine learning library used for applications such as computer vision and natural language processing\n",
    "# Provides tools for tensor computation, automatic differentiation, and more\n",
    "import torch\n",
    "\n",
    "# Download the \"punkt\" tokenizer model from nltk\n",
    "# The \"punkt\" tokenizer is a pre-trained model for tokenizing text into sentences\n",
    "# Useful for splitting a large text into individual sentences for further processing\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to load datasets from the 'datasets' library\n",
    "# The 'datasets' library provides a wide range of datasets and tools for handling and processing data,\n",
    "# making it easier to access datasets from various sources including the Hugging Face hub or local files.\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import the function to load evaluation metrics from the 'datasets' library\n",
    "# Metrics are used to evaluate the performance of machine learning models. The 'datasets' library includes \n",
    "# several standard metrics, allowing you to assess how well your model's predictions align with human evaluations.\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing AutoModelForSeq2SeqLM and AutoTokenizer from the 'transformers' library\n",
    "\n",
    "# AutoModelForSeq2SeqLM is a class that provides a generic interface to any pre-trained sequence-to-sequence model.\n",
    "# Sequence-to-sequence models are used for tasks like text summarization, translation, and other tasks where \n",
    "# an input sequence is transformed into an output sequence.\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# AutoTokenizer is a class that provides a tokenizer for any pre-trained model.\n",
    "# Tokenizers convert text into a format that the model can understand (e.g., converting text to tokens or IDs).\n",
    "# This is a crucial step before passing text data to the model for processing.\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using model checkpoint: google/pegasus-cnn_dailymail\n",
      "Tokenizer type: <class 'transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast'>\n",
      "Model type: <class 'transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "# Set the device to \"cuda\" if a GPU with CUDA support is available, otherwise use \"cpu\".\n",
    "# This allows you to leverage GPU acceleration for faster model training and inference if a compatible GPU is present.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Specify the model checkpoint identifier for a pre-trained model.\n",
    "# In this case, 'google/pegasus-cnn_dailymail' refers to a specific pre-trained PEGASUS model fine-tuned on the CNN/DailyMail dataset.\n",
    "# This model is used for tasks like text summarization, leveraging its pre-trained capabilities to generate summaries from input text.\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "# Load the tokenizer associated with the pre-trained model checkpoint.\n",
    "# The tokenizer converts text into tokens or IDs that the model can process.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Load the pre-trained model specified by the checkpoint and move it to the specified device (CPU or GPU).\n",
    "# The model is used for generating predictions based on the input data.\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using model checkpoint: {model_ckpt}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "print(f\"Model type: {type(model_pegasus)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the SAMSum Dataset\n",
    "\n",
    "In this section, we load the 'samsum' dataset using the `load_dataset` function from the `datasets` library. The 'samsum' dataset contains dialogues along with their corresponding summaries. It is designed to be used for tasks like text summarization and dialogue summarization, making it a valuable resource for training and evaluating summarization models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "print(dataset_samsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the SAMSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first example from the training set of the 'samsum' dataset.\n",
    "# This allows you to inspect the format and contents of the dataset, including the text and summary fields.\n",
    "print(dataset_samsum[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for pandas DataFrames to show all rows and columns.\n",
    "# This is useful for inspecting entire DataFrames without truncation, especially when dealing with large or complex data.\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                           dialogue  \\\n",
      "0     13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
      "1     13728867  Olivia: Who are you voting for in this electio...   \n",
      "2     13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
      "3     13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4     13728094  Sam: hey  overheard rick say something\\r\\nSam:...   \n",
      "5     13716343  Neville: Hi there, does anyone remember what d...   \n",
      "6     13611672  John: Ave. Was there any homework for tomorrow...   \n",
      "7     13730463  Sarah: I found a song on youtube and I think y...   \n",
      "8     13809976  Noah: When and where are we meeting? :)\\r\\nMad...   \n",
      "9     13809912  Matt: Do you want to go for date?\\r\\nAgnes: Wo...   \n",
      "10    13727633  Lucas: Hey! How was your day?\\r\\nDemi: Hey the...   \n",
      "11    13729168  Mark: I just shipped the goods\\r\\nMark: Tomorr...   \n",
      "12    13864825  Anita: I'm at the station in Bologna\\nJenny: N...   \n",
      "13    13729567  Leon: did you find the job yet?\\r\\nArthur: no ...   \n",
      "14    13864634  Macca: i'm so exited today\\nAdrien: why?\\nMacc...   \n",
      "15    13815560  Isabella: fuck my life, I'm so not able to get...   \n",
      "16    13731403  Tina: I'd only like to remind you that you owe...   \n",
      "17    13729191  Betty: Please remind me next time that too muc...   \n",
      "18    13827937  Mary: Hi Mike!\\r\\nMike: Hello :)\\r\\nMary: do u...   \n",
      "19    13828064  Laura: ok , I'm done for today-)\\r\\nLaura: let...   \n",
      "20    13716048  Ashley: Guys, you have to read this book!  <fi...   \n",
      "21    13828741  Aria: You won't believe who I've just met!\\r\\n...   \n",
      "22    13681946  Anna: where are you\\r\\nOmenah: at home \\r\\nAnn...   \n",
      "23    13728653  Renee: Just saying Hi. Thought of you this mor...   \n",
      "24    13818918  Jonas: I’m running 10 minutes late. Could you ...   \n",
      "25    13810064  Julius: dude, your assessment of manutd\\r\\nLaw...   \n",
      "26    13729933  Jade: are you going to that trip\\r\\nWayne: was...   \n",
      "27    13830054  Natalie: are you still going to Thailand?\\r\\nJ...   \n",
      "28    13716981  Elisa: Who wants to come for drinks tonight at...   \n",
      "29    13728757  Hal: Have you got any homework for tomorrow?\\r...   \n",
      "30    13716573  Ray: Hey guys, I don't know if you heard but s...   \n",
      "31  13731370-1  Eric: Champions League is coming soon :D\\r\\nCu...   \n",
      "32    13730401  Gunther: did you pay for coffee?\\r\\nChandler: ...   \n",
      "33    13716152  Karen: <file_photo>\\r\\nJennifer: OH. MY. GOD. ...   \n",
      "34    13715883  Ted: Any news about weekend?\\r\\nJake: About th...   \n",
      "35    13809921  Bradley: It's very safe. Not like they make it...   \n",
      "36    13680760  Lucia: I need my hair cut.\\r\\nLucia: When can ...   \n",
      "37    13612028  Gabriella: Hey Jasmine, how are you doing?\\r\\n...   \n",
      "38    13727812  Grace: Hey lady\\r\\nGrace: Today I saw you in t...   \n",
      "39    13862922  Kerri: Did you see the apartment?\\nStella: yes...   \n",
      "40    13862359  Andrea: hey Babes, how's it going? I've got so...   \n",
      "41    13828591  Victor: do you want to go to the museum tonigh...   \n",
      "42    13728090  William: Hey. Today i saw you were arguing wit...   \n",
      "43    13682071  Henry: Hey Lily, tried to catch you on landlin...   \n",
      "44    13682379  Linda: I'm going to have my room painted\\r\\nLi...   \n",
      "45    13682313  Juliette: So what? Tell me\\r\\nJimmy: One minut...   \n",
      "46    13730056  Mary: Hi <3\\r\\nPeter: Hey gorgeous ;)\\r\\nMary:...   \n",
      "47    13728383  Francesca: It's 3 a.m. You're not asleep yet\\r...   \n",
      "48    13715767  Ted: Feeling sorry for.. myself! \\r\\nKelly: Wh...   \n",
      "49    13729983  Chloe: Hey, you told me about this cream that ...   \n",
      "50    13828128  Pitt: Hey Teddy! Have you received my message?...   \n",
      "51    13731319  Mary: Sorry, I didn't make it to your bday par...   \n",
      "52    13819305  Connor: I'm too tired to come to the meeting I...   \n",
      "53    13813481  Karen: Where'd you buy this dress you had at t...   \n",
      "54    13715772  Gabi: What do you guys want for Christmas? Let...   \n",
      "55    13828719  Daniel: Hi Jeff, I was just browsing thought y...   \n",
      "56  13611954-1  Client: Good afternoon. I suggest you adjust t...   \n",
      "57    13865453  Jeff: Are you ready for the hiking tomorrow?\\n...   \n",
      "58    13864440  Joyce: Guys, sorry I'm running late today! Wil...   \n",
      "59    13817541  Aiden: Have you subscribed my channel?\\r\\nMia:...   \n",
      "60    13864732  Oli: I've talked to some people from the third...   \n",
      "61    13680485  Blake: Why did you tell Steven all those stupi...   \n",
      "62    13828302  Roberta: <file_photo>\\r\\nRoberta: look what I ...   \n",
      "63    13680126  Sawyer: I hope I found you wrapped in a towel ...   \n",
      "64    13865216  Sam: I'm so sorry. I can't make it on time.\\nS...   \n",
      "65    13731409  Sam: I just got my 1st credit card!\\r\\nTom: We...   \n",
      "66    13727597  James: <file_video>\\r\\nJames: My girlfriend is...   \n",
      "67    13716473  Christopher: ok, people, I’ve got an idea\\r\\nM...   \n",
      "68    13727770  Francine: hey hon\\r\\nFrancine: how are you tod...   \n",
      "69    13821815  Maria: Guys, don't bring anything, I've cooked...   \n",
      "70    13865139  Hannah: The motherfucker took my spot again\\nP...   \n",
      "71    13680779  Sophia: What should I get them?\\r\\nMonica: How...   \n",
      "72    13729889  Yannick: I heard you are going to sing the ant...   \n",
      "73    13715995  Carter: <file_photo> Sure it's nearly Christma...   \n",
      "74    13862502  Ammalee: <file_photo>\\nAmmalee: This lasted ov...   \n",
      "75    13828775  Alan: sweetie when wil you be home?\\r\\nJoan: I...   \n",
      "76  13731314-1  Johny: That girl on Tuesday music video is sex...   \n",
      "77    13611794  Frank: Hi Peter has brought me to Sports Direc...   \n",
      "78    13828614  Kyle: hey u got maths homework?\\r\\nPatrick: um...   \n",
      "79    13680746  Ellen: Hi, honey, sorry I've been so unreachab...   \n",
      "80    13819839  Paola: Guys, as I was saying I’d like to take ...   \n",
      "81    13865472  Harriet: Where are you?\\nVincent: In the clois...   \n",
      "82    13716468  Hyatt: watch channel 6 now\\r\\nNickleby: whats ...   \n",
      "83    13810089  Leo: hey jess\\r\\nJessie: hey\\r\\nLeo: so what'r...   \n",
      "84    13730151  John: do you play tennis? :-)\\r\\nIrma: i don't...   \n",
      "85    13682432  Jack: Where are you? I can't find you.\\r\\nOliv...   \n",
      "86    13681293  Pete: Sup?\\r\\nLionel: I'm at a meeting, can't ...   \n",
      "87    13681024  Rob: need a car check, u know a good mechanic?...   \n",
      "88    13716734  Andrew: wow, weekend! finally!\\r\\nAndrew: this...   \n",
      "89    13731380  Olivia: hey\\r\\nTaylor: whats up?\\r\\nOlivia: I ...   \n",
      "90    13829580  John: Maybe some ride?\\r\\nIan: Always!!!\\r\\nIa...   \n",
      "91    13728061  Ali: dude i need that hard drive\\r\\nKane: come...   \n",
      "92    13680149  Joshua: look out the window\\r\\nNoah: what's th...   \n",
      "93    13612157  Greg: why don’t you answer my calls?\\r\\nAva: c...   \n",
      "94    13728974  Shelly: I'm looking for a fall coat, are there...   \n",
      "95    13863020  Connor: hello can you tell me what songs did t...   \n",
      "96    13819450  Caleb: How are you guys?\\r\\nJeniffer: very goo...   \n",
      "97    13716378  Max: I'm so sorry Lucas. I don't know what got...   \n",
      "98    13862395  O'Neill: Is everything ok?\\nO'Neill: I didn't ...   \n",
      "99    13611608  Tom: How’s the weather in Poland now?\\r\\nJusti...   \n",
      "\n",
      "                                              summary  \n",
      "0   Amanda baked cookies and will bring Jerry some...  \n",
      "1   Olivia and Olivier are voting for liberals in ...  \n",
      "2   Kim may try the pomodoro technique recommended...  \n",
      "3   Edward thinks he is in love with Bella. Rachel...  \n",
      "4   Sam is confused, because he overheard Rick com...  \n",
      "5   Wyatt reminds Neville his wedding anniversary ...  \n",
      "6   John didn't show up for class due to some work...  \n",
      "7   Sarah sends James an instrumental song he migh...  \n",
      "8   Noah wants to meet, he quit his job, because h...  \n",
      "9   Matt invites Agnes for a date to get to know e...  \n",
      "10  Demi got promoted. She will celebrate that wit...  \n",
      "11  Mark just shipped the goods and he will send G...  \n",
      "12                       Anita is at Bologna station.  \n",
      "13  Arthur is still unemployed. Leon sends him a j...  \n",
      "14  Macca has done ice climbing for the first time...  \n",
      "15  Isabella feels bad after the Christmas party. ...  \n",
      "16  Lucy owes Tina 50 dollars. She made a transfer...  \n",
      "17  Betty feels remorse she got drunk last night a...  \n",
      "18  Mike and Mary are going to visit Mike's grandm...  \n",
      "19  Laura will pick up Kim from work around 7, and...  \n",
      "20  Erin is convinced by Ashley's book recommendat...  \n",
      "21  Aria has just run into Charlie Evans. He is no...  \n",
      "22  Omenah is at home, Anna will be there in a min...  \n",
      "23  Layla the dog misses Rachel. She is having a k...  \n",
      "24  Jonas will be 10 minutes late. Natalie will le...  \n",
      "25  Lawrence doesn't like the play of Manchester U...  \n",
      "26  The trip Wayne was going to go on was postpone...  \n",
      "27  Jason is going to Thailand next week. Natalie ...  \n",
      "28  Elisa, Sadie, Carol, Liam, Tom and John want t...  \n",
      "29  Amy and Hal will have dinner together when he ...  \n",
      "30  Ray's bike was stolen from the 9th street yest...  \n",
      "31  Eric wants to bet during the Champions League ...  \n",
      "32         Chandler will pay for his coffee tomorrow.  \n",
      "33  Karen looked very good on her 5th anniversary ...  \n",
      "34  Ted, Jake, Pia, Jessica and Jess are having a ...  \n",
      "35  Julianna tells Bradley about Europe's law and ...  \n",
      "36  Lucia needs a new hairstyle due to a change of...  \n",
      "37  Gabriella asked Jasmine to check her CV which ...  \n",
      "38  Grace saw Ruth in Galitos but she thought it w...  \n",
      "39  Stella wants to see the apartment in the morni...  \n",
      "40  Andrea must correct 50% of 20 short texts for ...  \n",
      "41  Charles will go with Victor to the museum toni...  \n",
      "42  Elizabeth had an argument with Blackett today,...  \n",
      "43  Henry and Lily take William with them to Riehe...  \n",
      "44  According to Brian, colors that match Linda's ...  \n",
      "45  Jimmy is going to take medication for a month ...  \n",
      "46  Peter will be back home in 2 days. Mary would ...  \n",
      "47  Neither Francesca nor Jacob can sleep. Jacob h...  \n",
      "48                         Ted has twisted his ankle.  \n",
      "49  Julie recommends the GlamRock cream from the G...  \n",
      "50        Teddy has a message from Pitt on Messenger.  \n",
      "51  Mary didn't come to Nick's birthday party. She...  \n",
      "52  Connor will not attend the meeting, he has to ...  \n",
      "53  Samara bought the dress she had at the wedding...  \n",
      "54  Mary, Kate, Peter and John want various items ...  \n",
      "55  Jeff will double check Mindy's reaction, conne...  \n",
      "56  Client informs Flix about a 40 minutes delay o...  \n",
      "57  Jeff, Ann, Corina, and Maria are going to hike...  \n",
      "58  Joyce and Andrew are running late. Carla and A...  \n",
      "59  Mia hasn't subscribed to Aiden's channel yet b...  \n",
      "60  Oli, Jacob and Marcia have to prepare for a ve...  \n",
      "61  Blake believes that Alexis is trying to turn t...  \n",
      "62  Roberta found Makoto's dress and will send it ...  \n",
      "63  When Eleanor meets Sawyer, she will be wearing...  \n",
      "64  Sam will be 30 minutes late. Sandra and Staś w...  \n",
      "65                     Sam got her first credit card.  \n",
      "66  James's girlfriend made him put the trash bin ...  \n",
      "67  To Christopher's frustration, Matthew, Andrew ...  \n",
      "68  Francine and Jessie talked yesterday. The talk...  \n",
      "69  Maria has cooked a lot of food. Andrew will br...  \n",
      "70  Richard took Hannah's parking spot again. Anne...  \n",
      "71  Sophie doesn't know what to buy for a gift. Sh...  \n",
      "72     Nicki is going to sing the anthem at the game.  \n",
      "73  Christmas is coming. Chloe will buy Carter new...  \n",
      "74  Ammalee sent Maryann a photo of her nails that...  \n",
      "75  Alan is making his famous chicken wings and wo...  \n",
      "76  Johny wants to go out clubbing with Luke in or...  \n",
      "77  Rosie is on her way to meet Frank and will hav...  \n",
      "78  Kyle reminds Patrick about their math homework...  \n",
      "79  Ellen and Paul are doing the redecoration and ...  \n",
      "80  Paola and her friends are going to the theatre...  \n",
      "81  Vincent and Charlotte are studying in the cloi...  \n",
      "82                           Stu is on channel 6 now.  \n",
      "83  Jessie and Leo will hang out at the weekend, b...  \n",
      "84  Irma doesn't play tennis. John and Irma will e...  \n",
      "85  Jack has to find Olivia in carriage 3, because...  \n",
      "86             Lionel is in a meeting and can't talk.  \n",
      "87  Will gived Rob the number of the car mechanic ...  \n",
      "88  Andrew, Nicky and Rick had a hard week at work...  \n",
      "89  Olivia doesn't know how to caption the photo s...  \n",
      "90      Ian and John will meet at St. Monica at 8.00.  \n",
      "91       Ali is coming to Kane to get the hard drive.  \n",
      "92                              It's snowing outside.  \n",
      "93  Ava doesn't want to talk to Greg any more and ...  \n",
      "94  During early lunchtime, Shelly will come to Po...  \n",
      "95  Connor is looking for a playlist from the Berl...  \n",
      "96  Jeniffer and Brooke're in New York now. They'v...  \n",
      "97  Max is sorry about his behaviour so wants to m...  \n",
      "98  O'Neill is worried about not having heard from...  \n",
      "99  It's getting cooler in Poland, because winter ...  \n"
     ]
    }
   ],
   "source": [
    "# Convert the training set of the 'samsum' dataset to a pandas DataFrame.\n",
    "    df_train = dataset_samsum[\"train\"].to_pandas()\n",
    "\n",
    "    # Print the first 100 rows of the DataFrame to get an overview of the dataset.\n",
    "    print(df_train.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the number of examples in each split (e.g., train, validation, test) of the 'samsum' dataset.\n",
    "# This helps in understanding the size of each dataset split.\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "\n",
    "# Print the column names (features) for the training set to understand what data is available.\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "\n",
    "# Print a sample dialogue from the test set to inspect the format and content of the dialogues.\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "# Print the corresponding summary for the sample dialogue from the test set.\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating PEGASUS on SAMSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Evaluating Summarization Model Performance Using ROUGE Metric\n",
    "\n",
    "The provided functions are essential for evaluating the performance of a pre-trained summarization model on a test dataset using the ROUGE metric. \n",
    "\n",
    "- **`generate_batch_sized_chunks` Function**: This function splits a large list of elements into smaller, manageable batches. This batching is useful for processing data efficiently, especially when dealing with large datasets that need to be processed in smaller chunks due to memory constraints.\n",
    "\n",
    "- **`calculate_metric_on_test_ds` Function**: This function leverages the batches created by `generate_batch_sized_chunks` to evaluate the summarization model. It tokenizes the input text, generates summaries with the model, and computes the ROUGE scores by comparing the generated summaries with the reference summaries. This evaluation helps determine the effectiveness of the model's summarization capabilities before any fine-tuning.\n",
    "\n",
    "These functions together provide a structured approach to assess how well the pre-trained model performs on a given test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"\n",
    "    Splits a list into smaller batches of a specified size.\n",
    "\n",
    "    This function is useful for processing large datasets in smaller,\n",
    "    more manageable chunks, especially when dealing with memory constraints\n",
    "    or parallelizing computations.\n",
    "\n",
    "    Args:\n",
    "        list_of_elements: The list to be split into batches.\n",
    "        batch_size: The desired size of each batch.\n",
    "\n",
    "    Yields:\n",
    "        Successive batch-sized chunks from the input list.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=8, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    \"\"\"\n",
    "    Evaluates a summarization model on a test dataset using the specified metric.\n",
    "\n",
    "    Args:\n",
    "    - dataset: The test dataset containing the text and summaries.\n",
    "    - metric: The evaluation metric (e.g., ROUGE) to compute.\n",
    "    - model: The pre-trained summarization model.\n",
    "    - tokenizer: The tokenizer associated with the model.\n",
    "    - batch_size: The number of samples to process in each batch.\n",
    "    - device: The device (CPU/GPU) to run the model on.\n",
    "    - column_text: The column name in the dataset containing the articles.\n",
    "    - column_summary: The column name in the dataset containing the reference summaries.\n",
    "\n",
    "    Returns:\n",
    "    - score: The computed metric score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the articles and summaries into batches\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    # Loop through each batch of articles and corresponding summaries\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        # Tokenize the articles in the batch\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
    "                           padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # Generate summaries using the model\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        # length_penalty ensures that the model does not generate sequences that are too long\n",
    "\n",
    "        # Decode the generated summaries into readable text\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                              clean_up_tokenization_spaces=True)\n",
    "                             for s in summaries]\n",
    "\n",
    "        # Replace empty strings with a space to avoid issues\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "        # Add the decoded summaries and the reference summaries to the metric\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    # Compute and return the final metric score\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "# Print the dialogue from the first example in the test split of the 'samsum' dataset\n",
    "print(\"Dialogue:\")\n",
    "print(dataset_samsum['test'][0]['dialogue'])\n",
    "\n",
    "# Print the corresponding summary from the first example in the test split\n",
    "print('\\nSummary:')\n",
    "print(dataset_samsum['test'][0]['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the summarization pipeline\n",
    "# The 'pipeline' function from the 'transformers' library simplifies the process of using pre-trained models.\n",
    "# 'summarization' indicates that we are creating a pipeline for generating summaries of text.\n",
    "# 'model=model_ckpt' specifies the pre-trained model to be used for the summarization task.\n",
    "pipe = pipeline('summarization', model=model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Tasks That Can Be Performed Using the `pipeline` Function\n",
    "\n",
    "The `pipeline` function from the Hugging Face Transformers library can be used to perform a wide range of natural language processing tasks. Below are some of the key tasks you can achieve:\n",
    "\n",
    "- **Text Classification** (`sentiment-analysis`): Classify text into categories such as sentiment (positive, negative, neutral).\n",
    "- **Named Entity Recognition (NER)** (`ner`): Identify and classify named entities in text (e.g., names of people, organizations, locations).\n",
    "- **Question Answering** (`question-answering`): Answer questions based on a given context or passage of text.\n",
    "- **Text Generation** (`text-generation`): Generate coherent and contextually relevant text based on a given prompt.\n",
    "- **Translation** (`translation_xx_to_yy` e.g., `translation_en_to_fr`): Translate text from one language to another.\n",
    "- **Summarization** (`summarization`): Summarize long texts into shorter, concise versions.\n",
    "- **Text2Text Generation** (`text2text-generation`): Generate text based on input text, suitable for tasks like text transformation and paraphrasing.\n",
    "- **Feature Extraction** (`feature-extraction`): Extract features or embeddings from text for use in other models or tasks.\n",
    "- **Zero-Shot Classification** (`zero-shot-classification`): Classify text into categories without having seen any examples of those categories during training.\n",
    "- **Dialogue Summarization** (`summarization`): Summarize dialogues specifically, often used in chat and conversational settings.\n",
    "- **Conversational** (`conversational`): Handle and manage multi-turn conversations or dialogues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    }
   ],
   "source": [
    "pipe_out = pipe(dataset_samsum['test'][0]['dialogue'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Generated Summary': \"Amanda: Ask Larry Amanda: He called her last time we were at the park together .\\nHannah: I'd rather you texted him .\\nAmanda: Just text him .\"}\n",
      "\n",
      "\n",
      "{'Original Summary': \"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\"}\n"
     ]
    }
   ],
   "source": [
    "# Extract the generated summary from the pipeline output.\n",
    "# 'pipe_out' is a list of dictionaries where each dictionary contains the generated summary.\n",
    "# The key 'summary_text' holds the summary in text format.\n",
    "summary_text = pipe_out[0]['summary_text']\n",
    "\n",
    "# Format the summary text by replacing occurrences of a period followed by an optional space and <n> with a period and newline.\n",
    "# This helps in structuring the summary into more readable lines by adding line breaks.\n",
    "formatted_summary = re.sub(r'\\. ?<n>', '.\\n', summary_text)\n",
    "\n",
    "\n",
    "# Print the formatted summary to the console.\n",
    "# This will display the generated summary with line breaks and Original Summary for better readability.\n",
    "print({\"Generated Summary\": formatted_summary})\n",
    "print('\\n')\n",
    "print({\"Original Summary\": dataset_samsum['test'][0]['summary']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\saima\\documents\\virtualenvs\\text_summarization_huggingface\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/103 [06:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m rouge_metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the ROUGE score on the test dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metric_on_test_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_samsum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The test dataset split\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouge_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# The ROUGE metric to use\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_pegasus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# The pre-trained summarization model\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# The corresponding tokenizer\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdialogue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# The column containing the input text\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The column containing the reference summaries\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Batch size for processing\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m, in \u001b[0;36mcalculate_metric_on_test_ds\u001b[1;34m(dataset, metric, model, tokenizer, batch_size, device, column_text, column_summary)\u001b[0m\n\u001b[0;32m     31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(article_batch, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m                    padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Generate summaries using the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# length_penalty ensures that the model does not generate sequences that are too long\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Decode the generated summaries into readable text\u001b[39;00m\n\u001b[0;32m     41\u001b[0m decoded_summaries \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(s, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m                                       clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m summaries]\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\generation\\utils.py:2028\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2021\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2022\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2023\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2024\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2025\u001b[0m     )\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2028\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2042\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2043\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2049\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2050\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\generation\\utils.py:3187\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3184\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   3186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3187\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3190\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1352\u001b[0m, in \u001b[0;36mPegasusForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1349\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1350\u001b[0m         )\n\u001b[1;32m-> 1352\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1369\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1371\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1214\u001b[0m, in \u001b[0;36mPegasusModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1208\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1209\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1210\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1211\u001b[0m     )\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1214\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1046\u001b[0m, in \u001b[0;36mPegasusDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1034\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1035\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         use_cache,\n\u001b[0;32m   1044\u001b[0m     )\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1046\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1059\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:425\u001b[0m, in \u001b[0;36mPegasusDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m    424\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:244\u001b[0m, in \u001b[0;36mPegasusAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    240\u001b[0m     attn_weights_reshaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    242\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 244\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the ROUGE metric for evaluation\n",
    "rouge_metric = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "# Calculate the ROUGE score on the test dataset\n",
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'],  # The test dataset split\n",
    "    rouge_metric,            # The ROUGE metric to use\n",
    "    model_pegasus,          # The pre-trained summarization model\n",
    "    tokenizer,              # The corresponding tokenizer\n",
    "    column_text='dialogue',   # The column containing the input text\n",
    "    column_summary='summary', # The column containing the reference summaries\n",
    "    batch_size=8             # Batch size for processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rouge_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeLsum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmeasure\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrouge_names\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m rouge_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeLsum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((rn, \u001b[43mscore\u001b[49m[rn]\u001b[38;5;241m.\u001b[39mmid\u001b[38;5;241m.\u001b[39mfmeasure ) \u001b[38;5;28;01mfor\u001b[39;00m rn \u001b[38;5;129;01min\u001b[39;00m rouge_names )\n\u001b[0;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "pd.DataFrame(rouge_dict, index = ['pegasus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning \n",
    "### the model is not performing (see rough score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example dataset preprocessing for understanding of tokenization, features, patterns and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_eg = {\n",
    "    'train': {\n",
    "        'dialogue': [\n",
    "            \"Hello, how are you?\",\n",
    "            \"I'm fine, thank you!\",\n",
    "            \"What are you doing today?\",\n",
    "            \"I'm going to the park.\",\n",
    "            \"See you later!\"\n",
    "        ],\n",
    "        'summary': [\n",
    "            \"Greetings.\",\n",
    "            \"Fine, thanks.\",\n",
    "            \"Plans for today.\",\n",
    "            \"Going to the park.\",\n",
    "            \"Farewell.\"\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8087, 108, 199, 127, 119, 152, 1],\n",
       " [125, 131, 208, 1226, 108, 2041, 119, 147, 1],\n",
       " [463, 127, 119, 557, 380, 152, 1],\n",
       " [125, 131, 208, 313, 112, 109, 1669, 107, 1],\n",
       " [1883, 119, 678, 147, 1]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_token = [tokenizer.encode(s) for s in dataset_samsum_eg['train']['dialogue']]\n",
    "dialogue_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_token_len = len([tokenizer.encode(s) for s in dataset_samsum_eg['train']['dialogue']])\n",
    "dialogue_token_len\n",
    "# Result: 5 (number of dialogues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9, 7, 9, 5]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_token_lens = [len(tokenizer.encode(s)) for s in dataset_samsum_eg['train']['dialogue']]\n",
    "dialogue_token_lens\n",
    "# No of tokens in each dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram for Analyzing length of tokens of dialougs and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create subplots with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot histogram for dialogue token lengths\n",
    "axes[0].hist(dialogue_token_lens, bins=20, color='C0', edgecolor='C0')\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Display the histogram\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Preprocessing of dataset_samsum Dataset\n",
    "#### dialogue feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue 1: Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "Tokenized Dialogue 1: [12195, 151, 125, 7091, 3659, 107, 842, 119, 245, 181, 152, 10508, 151, 7435, 147, 12195, 151, 125, 131, 267, 650, 119, 3469, 29344, 1]\n",
      "\n",
      "Original Dialogue 2: Olivia: Who are you voting for in this election? \n",
      "Oliver: Liberals as always.\n",
      "Olivia: Me too!!\n",
      "Oliver: Great\n",
      "Tokenized Dialogue 2: [18038, 151, 2632, 127, 119, 6228, 118, 115, 136, 2974, 152, 10463, 151, 35884, 130, 329, 107, 18038, 151, 2587, 314, 1242, 10463, 151, 1509, 1]\n",
      "\n",
      "Original Dialogue 3: Tim: Hi, what's up?\n",
      "Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\n",
      "Tim: What did you plan on doing?\n",
      "Kim: Oh you know, uni stuff and unfucking my room\n",
      "Kim: Maybe tomorrow I'll move my ass and do everything\n",
      "Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\n",
      "Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\n",
      "Tim: It really helps\n",
      "Kim: thanks, maybe I'll do that\n",
      "Tim: I also like using post-its in kaban style\n",
      "Tokenized Dialogue 3: [4776, 151, 4451, 108, 180, 131, 116, 164, 152, 5377, 151, 6843, 4301, 83678, 108, 125, 140, 313, 112, 171, 1425, 113, 1549, 155, 2371, 164, 64428, 4776, 151, 463, 368, 119, 511, 124, 557, 152, 5377, 151, 4384, 119, 235, 108, 18857, 1549, 111, 1596, 2073, 56616, 161, 418, 5377, 151, 3183, 3469, 125, 131, 267, 696, 161, 130, 116, 111, 171, 579, 5377, 151, 184, 195, 313, 112, 38244, 114, 5713, 167, 1088, 113, 1553, 125, 131, 267, 1461, 181, 38244, 316, 9609, 4776, 151, 321, 557, 1549, 125, 1253, 881, 93882, 3111, 241, 4911, 207, 5033, 118, 557, 18576, 4776, 151, 168, 288, 1107, 5377, 151, 1516, 108, 1556, 125, 131, 267, 171, 120, 4776, 151, 125, 163, 172, 303, 450, 121, 12397, 115, 9994, 11485, 669, 1]\n",
      "\n",
      "Original Dialogue 4: Edward: Rachel, I think I'm in ove with Bella..\n",
      "rachel: Dont say anything else..\n",
      "Edward: What do you mean??\n",
      "rachel: Open your fu**ing door.. I'm outside\n",
      "Tokenized Dialogue 4: [7535, 151, 9199, 108, 125, 311, 125, 131, 208, 115, 4429, 261, 122, 14521, 107, 107, 110, 93297, 151, 39282, 416, 742, 997, 107, 107, 7535, 151, 463, 171, 119, 1021, 6425, 110, 93297, 151, 2207, 128, 22204, 8326, 273, 1059, 107, 107, 125, 131, 208, 833, 1]\n",
      "\n",
      "Original Dialogue 5: Sam: hey  overheard rick say something\n",
      "Sam: i don't know what to do :-/\n",
      "Naomi: what did he say??\n",
      "Sam: he was talking on the phone with someone\n",
      "Sam: i don't know who\n",
      "Sam: and he was telling them that he wasn't very happy here\n",
      "Naomi: damn!!!\n",
      "Sam: he was saying he doesn't like being my roommate\n",
      "Naomi: wow, how do you feel about it?\n",
      "Sam: i thought i was a good rommate\n",
      "Sam: and that we have a nice place\n",
      "Naomi: that's true man!!!\n",
      "Naomi: i used to love living with you before i moved in with me boyfriend\n",
      "Naomi: i don't know why he's saying that\n",
      "Sam: what should i do???\n",
      "Naomi: honestly if it's bothering you that much you should talk to him\n",
      "Naomi: see what's going on\n",
      "Sam: i don't want to get in any kind of confrontation though\n",
      "Sam: maybe i'll just let it go\n",
      "Sam: and see how it goes in the future\n",
      "Naomi: it's your choice sam\n",
      "Naomi: if i were you i would just talk to him and clear the air\n",
      "Tokenized Dialogue 5: [4037, 151, 14381, 61237, 110, 13137, 416, 364, 4037, 151, 532, 272, 131, 144, 235, 180, 112, 171, 42656, 191, 26893, 151, 180, 368, 178, 416, 6425, 4037, 151, 178, 140, 1767, 124, 109, 685, 122, 647, 4037, 151, 532, 272, 131, 144, 235, 170, 4037, 151, 111, 178, 140, 3403, 183, 120, 178, 1417, 131, 144, 221, 774, 264, 26893, 151, 12334, 2109, 4037, 151, 178, 140, 1609, 178, 591, 131, 144, 172, 270, 161, 25603, 26893, 151, 12359, 108, 199, 171, 119, 393, 160, 126, 152, 4037, 151, 532, 666, 532, 140, 114, 234, 30999, 12210, 4037, 151, 111, 120, 145, 133, 114, 948, 295, 26893, 151, 120, 131, 116, 921, 729, 2109, 26893, 151, 532, 263, 112, 298, 622, 122, 119, 269, 532, 1652, 115, 122, 213, 9066, 26893, 151, 532, 272, 131, 144, 235, 447, 178, 131, 116, 1609, 120, 4037, 151, 180, 246, 532, 171, 9623, 26893, 151, 7165, 175, 126, 131, 116, 33079, 119, 120, 249, 119, 246, 1002, 112, 342, 26893, 151, 236, 180, 131, 116, 313, 124, 4037, 151, 532, 272, 131, 144, 245, 112, 179, 115, 189, 660, 113, 21780, 577, 4037, 151, 1556, 532, 131, 267, 188, 538, 126, 275, 4037, 151, 111, 236, 199, 126, 1168, 115, 109, 533, 26893, 151, 126, 131, 116, 128, 814, 21718, 26893, 151, 175, 532, 195, 119, 532, 192, 188, 1002, 112, 342, 111, 786, 109, 694, 1]\n",
      "\n",
      "Original Dialogue 6: Neville: Hi there, does anyone remember what date I got married on?\n",
      "Don: Are you serious?\n",
      "Neville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\n",
      "Wyatt: Hang on, I'll ask my wife.\n",
      "Don: Haha, someone's in a lot of trouble :D\n",
      "Wyatt: September 17. I hope you remember the year ;)\n",
      "Tokenized Dialogue 6: [36160, 151, 4451, 186, 108, 358, 966, 1173, 180, 796, 125, 419, 2826, 124, 152, 1414, 151, 1706, 119, 1651, 152, 36160, 151, 7305, 1651, 107, 184, 131, 216, 124, 3010, 108, 111, 19990, 131, 116, 8593, 134, 213, 160, 364, 107, 125, 133, 114, 4768, 18234, 120, 136, 382, 133, 364, 112, 171, 122, 150, 1275, 4382, 108, 155, 125, 133, 9011, 112, 553, 107, 33360, 151, 21360, 124, 108, 125, 131, 267, 854, 161, 1750, 107, 1414, 151, 35501, 108, 647, 131, 116, 115, 114, 367, 113, 2954, 110, 151, 470, 33360, 151, 1338, 17244, 125, 715, 119, 1173, 109, 232, 26408, 1]\n",
      "\n",
      "Original Dialogue 7: John: Ave. Was there any homework for tomorrow?\n",
      "Cassandra: hello :D Of course, as always :D\n",
      "John: What exactly?\n",
      "Cassandra: I'm not sure so I'll check it for you in 20minutes. \n",
      "John: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\n",
      "Cassandra: No problem, what did he do this time?\n",
      "John: Nothing special, just the same as always, treating us like children, commanding to do this and that...\n",
      "Cassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\n",
      "John: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\n",
      "Cassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\n",
      "John: yeah I know...maybe some beer this week?\n",
      "Cassandra: Sure, but I got some time after classes only...this week is gonna be busy\n",
      "John: no problem, I can drive you home and we can go to some bar or whatever.\n",
      "Cassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\n",
      "John: gosh...I don't know if I'm smart enough to do it :'D\n",
      "Cassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\n",
      "John: ok...then I'll try my best :D\n",
      "Cassandra: sure, if you will have any questions just either text or call me and I'll help you.\n",
      "John: I hope I won't have to waste your time xD\n",
      "Cassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\n",
      "John: just kidding :D ok, so i guess we'll stay in touch then\n",
      "Cassandra: sure, have a nice evening :D\n",
      "John: you too, se ya\n",
      "Cassandra: Byeeeee\n",
      "Tokenized Dialogue 7: [1084, 151, 6032, 107, 6131, 186, 189, 6727, 118, 3469, 152, 40924, 151, 12253, 110, 151, 470, 1179, 422, 108, 130, 329, 110, 151, 470, 1084, 151, 463, 1270, 152, 40924, 151, 125, 131, 208, 146, 334, 167, 125, 131, 267, 553, 126, 118, 119, 115, 599, 40677, 107, 1084, 151, 7262, 108, 1516, 107, 9752, 125, 1826, 131, 144, 129, 186, 108, 155, 125, 140, 2117, 130, 3068, 13412, 401, 3809, 9049, 5388, 130, 329, 140, 847, 112, 48696, 213, 299, 40924, 151, 566, 575, 108, 180, 368, 178, 171, 136, 166, 152, 1084, 151, 6490, 548, 108, 188, 109, 310, 130, 329, 108, 6108, 214, 172, 404, 108, 25447, 112, 171, 136, 111, 120, 401, 40924, 151, 4984, 112, 1232, 120, 107, 155, 447, 272, 131, 144, 119, 188, 275, 112, 128, 3522, 111, 823, 342, 579, 152, 1084, 151, 125, 192, 108, 155, 125, 272, 131, 144, 133, 189, 337, 135, 536, 108, 157, 127, 172, 92291, 16343, 17588, 111, 12743, 120, 579, 131, 116, 1226, 401, 187, 131, 208, 146, 7471, 2374, 579, 118, 688, 40924, 151, 125, 630, 401, 24203, 108, 188, 508, 112, 6477, 342, 107, 125, 235, 126, 382, 1056, 10567, 130, 3068, 13412, 108, 155, 1254, 186, 131, 116, 1015, 154, 119, 137, 171, 107, 1084, 151, 10036, 125, 235, 401, 15804, 181, 3053, 136, 396, 152, 40924, 151, 7435, 108, 155, 125, 419, 181, 166, 244, 1745, 209, 401, 4775, 396, 117, 7471, 129, 2117, 1084, 151, 220, 575, 108, 125, 137, 919, 119, 238, 111, 145, 137, 275, 112, 181, 1290, 132, 2076, 107, 40924, 151, 1274, 107, 6514, 108, 125, 419, 136, 6727, 107, 126, 131, 116, 438, 738, 4026, 107, 280, 111, 296, 108, 125, 163, 1049, 109, 536, 112, 692, 372, 3697, 108, 704, 109, 10987, 135, 109, 221, 211, 1361, 107, 1205, 516, 126, 107, 1084, 151, 33792, 401, 187, 272, 131, 144, 235, 175, 125, 131, 208, 2103, 505, 112, 171, 126, 110, 151, 131, 470, 40924, 151, 119, 127, 108, 272, 131, 144, 2262, 110, 151, 969, 1205, 4846, 149, 109, 989, 119, 272, 131, 144, 235, 111, 145, 131, 267, 801, 124, 1491, 107, 1084, 151, 6514, 401, 11916, 125, 131, 267, 508, 161, 229, 110, 151, 470, 40924, 151, 334, 108, 175, 119, 138, 133, 189, 574, 188, 707, 1352, 132, 443, 213, 111, 125, 131, 267, 225, 119, 107, 1084, 151, 125, 715, 125, 576, 131, 144, 133, 112, 2160, 128, 166, 1026, 470, 40924, 151, 119, 131, 216, 146, 12416, 161, 166, 108, 125, 131, 208, 128, 2118, 108, 125, 131, 208, 264, 112, 225, 107, 182, 117, 180, 125, 179, 408, 118, 108, 163, 110, 151, 969, 1084, 151, 188, 18922, 110, 151, 470, 6514, 108, 167, 532, 2665, 145, 131, 267, 753, 115, 1266, 237, 40924, 151, 334, 108, 133, 114, 948, 1709, 110, 151, 470, 1084, 151, 119, 314, 108, 7258, 10182, 40924, 151, 1060, 71161, 1]\n",
      "\n",
      "Original Dialogue 8: Sarah: I found a song on youtube and I think you'll like it\n",
      "James: What song?\n",
      "Sarah: <file_other>\n",
      "James: Oh. I know it! \n",
      "James: I heard it before in some compilation\n",
      "Sarah: I can't stop playing it over and over\n",
      "James: That's exactly how I know lyrics to all of the songs on my playlist :D\n",
      "Sarah: Haha. No lyrics here though. Instrumental ;D\n",
      "James: Instrumental songs are different kind of music. \n",
      "James: But you have to remember that the activity you do when you listen to this song\n",
      "James: Is the actvity your brain will connect to the song\n",
      "James: And everytime you play this song at home\n",
      "James: You'll be thinking of your work\n",
      "Sarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\n",
      "James: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\n",
      "Sarah: Hahaha. True, true.\n",
      "Tokenized Dialogue 8: [5615, 151, 125, 374, 114, 1649, 124, 11909, 111, 125, 311, 119, 131, 267, 172, 126, 2133, 151, 463, 1649, 152, 5615, 151, 110, 105, 12014, 940, 11896, 2314, 2133, 151, 4384, 107, 125, 235, 126, 147, 2133, 151, 125, 1455, 126, 269, 115, 181, 12965, 5615, 151, 125, 137, 131, 144, 923, 1123, 126, 204, 111, 204, 2133, 151, 485, 131, 116, 1270, 199, 125, 235, 6958, 112, 149, 113, 109, 2195, 124, 161, 16078, 110, 151, 470, 5615, 151, 35501, 107, 566, 6958, 264, 577, 107, 59809, 110, 206, 470, 2133, 151, 59809, 2195, 127, 291, 660, 113, 534, 107, 2133, 151, 343, 119, 133, 112, 1173, 120, 109, 1383, 119, 171, 173, 119, 2427, 112, 136, 1649, 2133, 151, 125, 116, 109, 1787, 2075, 2922, 128, 2037, 138, 1820, 112, 109, 1649, 2133, 151, 325, 37528, 119, 462, 136, 1649, 134, 238, 2133, 151, 226, 131, 267, 129, 1234, 113, 128, 201, 5615, 151, 12098, 108, 125, 235, 120, 107, 485, 131, 116, 447, 145, 1254, 416, 233, 125, 263, 112, 172, 120, 1649, 108, 155, 239, 126, 188, 7944, 213, 113, 1025, 3489, 2133, 151, 39142, 107, 2317, 1139, 119, 411, 128, 1627, 108, 119, 133, 112, 179, 4297, 113, 128, 928, 534, 110, 151, 470, 5615, 151, 110, 52228, 107, 6893, 108, 921, 107, 1]\n",
      "\n",
      "Original Dialogue 9: Noah: When and where are we meeting? :)\n",
      "Madison: I thought you were busy...?\n",
      "Noah: Yeah, I WAS. I quit my job. \n",
      "Madison: No way! :o :o :o Why? I thought you liked it...?\n",
      "Noah: Well, I used to, until my boss turned into a complete cock... Long story.\n",
      "Tokenized Dialogue 9: [14780, 151, 434, 111, 241, 127, 145, 988, 152, 8537, 8367, 151, 125, 666, 119, 195, 2117, 28731, 14780, 151, 12098, 108, 125, 17740, 107, 125, 7209, 161, 494, 107, 8367, 151, 566, 230, 147, 110, 151, 554, 110, 151, 554, 110, 151, 554, 1807, 152, 125, 666, 119, 3495, 126, 28731, 14780, 151, 1894, 108, 125, 263, 112, 108, 430, 161, 5388, 1552, 190, 114, 573, 110, 17197, 401, 2859, 584, 107, 1]\n",
      "\n",
      "Original Dialogue 10: Matt: Do you want to go for date?\n",
      "Agnes: Wow! You caught me out with this question Matt.\n",
      "Matt: Why?\n",
      "Agnes: I simply didn't expect this from you.\n",
      "Matt: Well, expect the unexpected.\n",
      "Agnes: Can I think about it?\n",
      "Matt: What is there to think about?\n",
      "Agnes: Well, I don't really know you.\n",
      "Matt: This is the perfect time to get to know eachother\n",
      "Agnes: Well that's true.\n",
      "Matt: So let's go to the Georgian restaurant in Kazimierz.\n",
      "Agnes: Now your convincing me.\n",
      "Matt: Cool, saturday at 6pm?\n",
      "Agnes: That's fine.\n",
      "Matt: I can pick you up on the way to the restaurant.\n",
      "Agnes: That's really kind of you.\n",
      "Matt: No problem.\n",
      "Agnes: See you on saturday.\n",
      "Matt: Yes, looking forward to it.\n",
      "Agnes: Me too.\n",
      "Tokenized Dialogue 10: [4592, 151, 842, 119, 245, 112, 275, 118, 796, 152, 35746, 151, 12849, 147, 226, 2908, 213, 165, 122, 136, 906, 4592, 107, 4592, 151, 1807, 152, 35746, 151, 125, 705, 595, 131, 144, 1337, 136, 135, 119, 107, 4592, 151, 1894, 108, 1337, 109, 4160, 107, 35746, 151, 1526, 125, 311, 160, 126, 152, 4592, 151, 463, 117, 186, 112, 311, 160, 152, 35746, 151, 1894, 108, 125, 272, 131, 144, 288, 235, 119, 107, 4592, 151, 182, 117, 109, 512, 166, 112, 179, 112, 235, 77681, 35746, 151, 1894, 120, 131, 116, 921, 107, 4592, 151, 412, 538, 131, 116, 275, 112, 109, 19254, 1705, 115, 74793, 6120, 58898, 107, 35746, 151, 1032, 128, 12875, 213, 107, 4592, 151, 7262, 108, 47150, 134, 530, 1638, 152, 35746, 151, 485, 131, 116, 1226, 107, 4592, 151, 125, 137, 1293, 119, 164, 124, 109, 230, 112, 109, 1705, 107, 35746, 151, 485, 131, 116, 288, 660, 113, 119, 107, 4592, 151, 566, 575, 107, 35746, 151, 1883, 119, 124, 47150, 107, 4592, 151, 2657, 108, 383, 782, 112, 126, 107, 35746, 151, 2587, 314, 107, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dialogues in the training set\n",
    "dialogue_tokens = [tokenizer.encode(s) for s in dataset_samsum['train']['dialogue']]\n",
    "dialogue_token_lens = [len(tokenizer.encode(s)) for s in dataset_samsum['train']['dialogue']]\n",
    "\n",
    "\n",
    "# Print the first 10 original dialogues and their tokenized versions\n",
    "for i in range(10):\n",
    "    original_dialogue = dataset_samsum['train']['dialogue'][i]\n",
    "    tokenized_dialogue = dialogue_tokens[i]\n",
    "    print(f\"Original Dialogue {i+1}: {original_dialogue}\")\n",
    "    print(f\"Tokenized Dialogue {i+1}: {tokenized_dialogue}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of Dialogue : 14732\n",
      "Each Dialogue Tokens : [25, 26, 131, 48, 235, 105, 490, 220, 74, 186, 100, 23, 32, 153, 152, 206, 69, 95, 61, 59, 352, 297, 21, 346, 61, 108, 74, 141, 147, 145, 402, 121, 36, 204, 145, 171, 162, 192, 160, 246, 271, 255, 108, 361, 247, 180, 116, 174, 73, 334, 40, 140, 42, 135, 181, 97, 311, 156, 91, 91, 178, 390, 57, 238, 43, 57, 112, 116, 133, 46, 22, 64, 123, 100, 183, 75, 234, 69, 69, 362, 495, 63, 38, 171, 116, 139, 25, 88, 160, 178, 32, 27, 80, 83, 69, 40, 196, 112, 75, 59, 228, 26, 132, 141, 291, 56, 42, 90, 84, 94, 44, 38, 99, 79, 208, 161, 143, 114, 198, 262, 364, 51, 61, 76, 142, 74, 60, 398, 129, 83, 112, 189, 95, 49, 54, 89, 100, 135, 211, 48, 81, 77, 134, 121, 57, 167, 74, 144, 56, 92, 229, 255, 193, 228, 145, 61, 175, 81, 85, 239, 116, 64, 78, 300, 122, 168, 81, 34, 115, 222, 182, 124, 69, 128, 168, 71, 39, 178, 135, 79, 140, 61, 110, 286, 48, 154, 79, 107, 96, 35, 107, 128, 191, 120, 62, 117, 95, 77, 174, 199, 93, 237, 56, 54, 69, 258, 86, 81, 51, 41, 188, 240, 66, 94, 145, 80, 66, 44, 186, 42, 222, 146, 240, 122, 63, 160, 209, 213, 391, 92, 28, 49, 44, 196, 47, 132, 190, 42, 180, 36, 91, 174, 302, 94, 177, 36, 199, 140, 46, 287, 126, 106, 35, 253, 105, 97, 137, 98, 246, 40, 65, 29, 37, 52, 98, 64, 72, 48, 150, 25, 160, 84, 95, 47, 46, 134, 255, 124, 198, 129, 52, 113, 84, 24, 54, 158, 28, 45, 123, 70, 27, 76, 132, 56, 59, 219, 68, 96, 192, 186, 285, 185, 22, 167, 64, 91, 179, 211, 76, 44, 148, 43, 76, 67, 52, 127, 412, 96, 101, 297, 62, 163, 50, 233, 65, 186, 196, 54, 234, 127, 20, 151, 189, 31, 59, 188, 25, 78, 122, 33, 162, 237, 24, 27, 130, 19, 105, 239, 498, 21, 139, 103, 185, 69, 88, 110, 94, 31, 163, 288, 64, 158, 137, 112, 311, 23, 58, 244, 52, 28, 136, 47, 97, 67, 348, 278, 33, 44, 129, 80, 147, 59, 293, 150, 81, 37, 65, 52, 154, 198, 220, 100, 209, 82, 56, 484, 112, 64, 180, 530, 43, 94, 124, 81, 88, 30, 28, 111, 22, 29, 82, 117, 184, 178, 85, 103, 93, 108, 50, 159, 40, 60, 37, 254, 47, 127, 224, 81, 114, 104, 40, 85, 93, 150, 28, 152, 35, 39, 139, 81, 95, 58, 189, 150, 143, 247, 44, 201, 100, 321, 209, 93, 137, 60, 23, 109, 87, 193, 304, 106, 97, 541, 66, 157, 74, 61, 58, 218, 140, 206, 160, 94, 76, 148, 217, 167, 32, 159, 13, 132, 273, 406, 124, 44, 218, 225, 39, 148, 82, 108, 211, 432, 49, 68, 117, 188, 346, 232, 43, 112, 126, 207, 113, 41, 196, 29, 44, 71, 150, 43, 155, 206, 257, 155, 52, 76, 189, 331, 128, 224, 71, 96, 33, 165, 32, 74, 133, 109, 183, 207, 32, 171, 192, 67, 165, 213, 103, 319, 172, 145, 60, 148, 50, 114, 302, 25, 179, 57, 46, 136, 18, 75, 114, 36, 123, 42, 81, 141, 41, 309, 185, 284, 125, 173, 87, 67, 136, 24, 74, 25, 55, 186, 84, 111, 50, 150, 101, 40, 88, 403, 22, 44, 57, 365, 40, 81, 115, 203, 30, 175, 32, 225, 349, 238, 30, 100, 96, 53, 163, 312, 64, 237, 130, 214, 26, 78, 80, 65, 116, 72, 198, 55, 23, 44, 57, 66, 29, 103, 287, 78, 286, 359, 80, 168, 71, 142, 155, 203, 132, 32, 138, 151, 166, 119, 153, 469, 24, 145, 154, 78, 50, 60, 52, 41, 86, 196, 130, 43, 39, 131, 58, 142, 22, 319, 166, 101, 166, 51, 87, 55, 412, 93, 44, 118, 145, 198, 282, 79, 23, 64, 42, 72, 213, 143, 195, 31, 64, 246, 231, 120, 76, 62, 126, 99, 41, 80, 191, 163, 38, 44, 70, 74, 356, 202, 61, 91, 350, 42, 117, 161, 155, 60, 18, 321, 58, 31, 64, 33, 298, 247, 47, 26, 58, 26, 140, 213, 152, 93, 74, 443, 42, 77, 238, 43, 99, 305, 228, 103, 139, 396, 37, 34, 212, 124, 48, 279, 50, 373, 122, 88, 35, 143, 179, 149, 406, 218, 240, 34, 268, 83, 94, 129, 70, 55, 21, 60, 165, 68, 73, 235, 39, 315, 299, 371, 35, 372, 108, 270, 205, 195, 245, 108, 29, 138, 110, 222, 62, 35, 57, 66, 305, 37, 59, 110, 208, 102, 281, 33, 118, 62, 518, 120, 152, 205, 160, 83, 56, 178, 203, 33, 126, 79, 153, 249, 240, 128, 46, 62, 194, 85, 281, 202, 167, 267, 89, 329, 95, 39, 34, 117, 25, 141, 62, 44, 173, 95, 78, 96, 137, 52, 66, 48, 84, 163, 255, 73, 316, 181, 50, 90, 34, 286, 185, 119, 57, 102, 73, 107, 166, 38, 206, 74, 204, 72, 64, 112, 53, 185, 184, 111, 53, 61, 198, 53, 33, 596, 192, 290, 200, 33, 85, 66, 64, 150, 95, 292, 104, 151, 399, 211, 85, 54, 55, 205, 85, 73, 120, 116, 207, 145, 29, 41, 36, 356, 128, 28, 101, 35, 275, 27, 298, 208, 147, 289, 125, 51, 127, 63, 53, 377, 110, 103, 191, 264, 305, 76, 87, 259, 62, 104, 63, 100, 159, 223, 174, 414, 222, 256, 189, 170, 355, 114, 74, 78, 33, 124, 83, 107, 54, 117, 61, 47, 68, 201, 256, 124, 340, 60, 126, 306, 206, 60, 59, 249, 247, 87, 75, 170, 348, 62, 160, 84, 66, 177, 20, 53, 197, 71, 40, 383, 213, 216, 91, 252, 76, 31, 102, 53, 166, 22, 135, 73, 197, 56, 150, 436, 84, 79, 57, 33, 70, 240, 44, 111, 147, 379, 61, 20, 151, 20, 271, 70, 356, 30, 47, 112, 107, 173, 141, 214, 44, 74, 258, 284, 56, 63, 153, 50, 125, 515, 174, 68, 260, 545, 196, 51, 356, 392, 312, 51, 86, 89, 45, 153, 97, 44, 117, 18, 60, 79, 184, 510, 98, 168, 70, 89, 12, 225, 114, 114, 36, 61, 44, 62, 195, 221, 27, 211, 442, 116, 66, 60, 142, 274, 192, 121, 89, 146, 80, 35, 113, 23, 88, 31, 23, 90, 163, 180, 123, 99, 241, 215, 90, 94, 56, 23, 38, 36, 103, 134, 89, 63, 107, 141, 26, 267, 27, 171, 74, 164, 36, 245, 197, 159, 67, 354, 140, 111, 48, 24, 301, 100, 92, 108, 43, 26, 126, 189, 158, 44, 76, 15, 42, 46, 176, 35, 305, 49, 45, 77, 100, 363, 153, 97, 37, 292, 77, 124, 192, 308, 61, 217, 21, 127, 645, 24, 126, 185, 441, 98, 149, 43, 89, 147, 114, 43, 416, 54, 155, 57, 33, 174, 98, 286, 150, 37, 335, 214, 198, 52, 78, 293, 165, 189, 219, 141, 34, 31, 319, 70, 30, 214, 298, 103, 161, 67, 74, 114, 18, 108, 282, 134, 73, 136, 145, 260, 27, 190, 133, 46, 38, 83, 33, 339, 215, 93, 389, 150, 41, 145, 87, 278, 34, 398, 341, 60, 18, 221, 26, 92, 134, 13, 246, 222, 38, 274, 427, 65, 196, 218, 103, 277, 119, 22, 101, 28, 110, 323, 35, 140, 115, 191, 60, 87, 74, 64, 19, 261, 151, 35, 35, 28, 367, 222, 55, 157, 254, 70, 77, 47, 137, 22, 203, 262, 126, 74, 47, 215, 20, 55, 81, 166, 125, 91, 232, 49, 441, 42, 36, 124, 378, 108, 176, 230, 163, 44, 63, 70, 56, 127, 168, 241, 36, 58, 95, 28, 226, 351, 138, 133, 183, 38, 183, 158, 39, 237, 186, 57, 21, 139, 83, 143, 164, 26, 27, 64, 159, 238, 131, 231, 65, 419, 95, 99, 106, 156, 184, 136, 113, 294, 24, 111, 115, 106, 73, 148, 95, 89, 18, 76, 92, 67, 64, 162, 155, 61, 67, 39, 35, 136, 142, 276, 91, 122, 264, 101, 133, 135, 38, 48, 81, 218, 194, 158, 87, 88, 369, 140, 63, 231, 102, 27, 34, 20, 168, 50, 85, 79, 194, 165, 77, 54, 227, 213, 172, 77, 357, 71, 62, 202, 295, 153, 50, 572, 107, 77, 214, 201, 49, 121, 282, 127, 235, 84, 202, 176, 80, 38, 219, 68, 153, 84, 131, 74, 134, 120, 88, 84, 95, 90, 26, 83, 60, 345, 156, 205, 119, 256, 535, 45, 86, 67, 15, 288, 98, 159, 223, 327, 159, 90, 69, 121, 69, 28, 289, 126, 93, 55, 48, 17, 204, 226, 115, 153, 276, 117, 337, 213, 62, 283, 61, 149, 39, 149, 160, 47, 82, 254, 93, 121, 82, 100, 264, 31, 148, 176, 76, 280, 185, 141, 44, 127, 238, 210, 45, 154, 291, 152, 222, 254, 186, 105, 24, 155, 282, 126, 79, 194, 60, 39, 201, 155, 103, 142, 116, 266, 384, 62, 127, 21, 86, 143, 245, 101, 232, 46, 309, 153, 41, 104, 34, 176, 128, 52, 270, 99, 121, 138, 55, 59, 137, 75, 57, 31, 301, 59, 304, 206, 141, 186, 50, 69, 106, 190, 210, 194, 210, 95, 33, 41, 105, 43, 168, 33, 43, 217, 45, 88, 38, 174, 168, 76, 317, 176, 100, 64, 24, 300, 177, 105, 47, 141, 187, 69, 91, 133, 163, 81, 229, 121, 52, 69, 105, 72, 215, 477, 31, 161, 42, 116, 197, 182, 100, 172, 239, 76, 91, 203, 62, 197, 190, 371, 62, 92, 180, 101, 29, 120, 182, 83, 133, 71, 36, 192, 110, 340, 221, 74, 149, 128, 40, 335, 64, 37, 283, 102, 65, 78, 156, 33, 119, 75, 30, 236, 70, 85, 67, 161, 391, 54, 55, 18, 72, 25, 128, 158, 180, 203, 120, 42, 515, 128, 39, 186, 83, 26, 73, 377, 116, 142, 38, 47, 429, 56, 82, 155, 120, 121, 99, 334, 93, 70, 58, 97, 202, 84, 76, 64, 22, 87, 61, 178, 62, 178, 65, 147, 190, 201, 53, 259, 76, 100, 151, 39, 66, 192, 139, 35, 360, 427, 191, 121, 45, 78, 227, 174, 118, 51, 23, 437, 42, 30, 120, 49, 54, 98, 199, 124, 30, 57, 25, 516, 214, 220, 533, 58, 29, 60, 334, 103, 124, 346, 150, 65, 57, 30, 87, 120, 365, 73, 445, 81, 279, 256, 39, 105, 351, 75, 230, 364, 261, 128, 82, 43, 194, 111, 211, 124, 208, 62, 80, 141, 41, 92, 264, 112, 53, 112, 146, 70, 60, 50, 119, 98, 207, 68, 76, 175, 174, 102, 354, 204, 171, 152, 32, 75, 27, 22, 39, 57, 37, 84, 89, 307, 43, 70, 258, 338, 207, 101, 116, 146, 100, 253, 42, 62, 177, 120, 24, 64, 79, 59, 188, 95, 193, 136, 97, 47, 144, 66, 147, 96, 34, 187, 91, 199, 24, 34, 141, 50, 189, 124, 225, 61, 21, 237, 97, 22, 297, 372, 276, 89, 211, 172, 134, 123, 178, 58, 122, 43, 365, 49, 76, 74, 146, 47, 38, 80, 130, 70, 43, 44, 206, 64, 221, 230, 340, 62, 157, 222, 182, 109, 49, 113, 44, 50, 85, 155, 106, 26, 32, 38, 303, 80, 72, 105, 76, 96, 60, 464, 46, 54, 75, 53, 101, 91, 183, 143, 197, 73, 179, 37, 244, 156, 49, 228, 38, 98, 87, 230, 37, 19, 69, 181, 68, 204, 593, 398, 116, 96, 85, 86, 223, 275, 107, 173, 255, 86, 23, 112, 96, 104, 282, 150, 75, 125, 34, 32, 69, 39, 47, 175, 83, 284, 64, 330, 421, 101, 142, 173, 80, 67, 467, 262, 41, 167, 141, 142, 78, 39, 57, 89, 163, 239, 60, 383, 134, 365, 104, 45, 183, 114, 381, 195, 157, 100, 97, 85, 369, 88, 179, 166, 69, 165, 121, 273, 116, 25, 130, 64, 67, 139, 268, 119, 38, 194, 113, 208, 235, 91, 59, 71, 111, 92, 165, 215, 25, 99, 247, 376, 42, 65, 120, 53, 63, 263, 65, 77, 105, 123, 60, 131, 35, 218, 67, 406, 107, 19, 115, 58, 112, 182, 21, 27, 154, 21, 84, 177, 91, 63, 157, 66, 106, 225, 153, 284, 129, 233, 187, 47, 123, 428, 73, 65, 130, 31, 51, 154, 138, 135, 43, 54, 135, 96, 195, 129, 34, 198, 222, 122, 545, 60, 203, 80, 59, 212, 224, 200, 290, 190, 248, 48, 76, 34, 133, 316, 41, 53, 172, 34, 331, 188, 48, 172, 41, 125, 95, 202, 421, 201, 142, 154, 48, 134, 87, 133, 79, 83, 113, 43, 134, 360, 274, 219, 54, 54, 45, 73, 61, 218, 107, 91, 92, 255, 191, 122, 46, 39, 175, 251, 175, 89, 184, 25, 58, 121, 42, 69, 154, 65, 51, 53, 82, 162, 94, 63, 36, 82, 23, 128, 177, 119, 339, 34, 272, 213, 92, 216, 273, 68, 81, 23, 593, 241, 161, 68, 77, 28, 61, 34, 225, 46, 98, 124, 67, 32, 392, 36, 42, 106, 43, 76, 41, 73, 273, 133, 277, 52, 280, 17, 86, 80, 76, 130, 88, 71, 163, 79, 514, 182, 94, 271, 246, 128, 63, 183, 46, 40, 66, 318, 41, 84, 186, 106, 48, 16, 167, 60, 127, 49, 103, 482, 62, 79, 50, 199, 335, 78, 146, 34, 217, 178, 27, 63, 106, 197, 311, 296, 23, 200, 160, 196, 231, 377, 224, 228, 124, 156, 119, 185, 149, 282, 51, 182, 103, 110, 54, 95, 165, 244, 172, 52, 44, 40, 105, 45, 58, 63, 65, 289, 32, 107, 22, 166, 38, 465, 65, 55, 269, 98, 29, 72, 110, 24, 146, 80, 34, 22, 56, 48, 246, 64, 258, 242, 26, 83, 201, 77, 39, 41, 165, 220, 31, 139, 92, 239, 132, 164, 51, 141, 175, 64, 178, 398, 102, 167, 58, 193, 191, 28, 213, 258, 97, 197, 61, 61, 175, 20, 111, 401, 59, 196, 47, 84, 85, 26, 190, 117, 52, 378, 42, 78, 99, 37, 13, 314, 94, 22, 48, 67, 74, 82, 133, 208, 136, 96, 47, 74, 405, 30, 256, 67, 132, 58, 120, 75, 89, 33, 254, 287, 51, 336, 62, 37, 73, 51, 54, 133, 155, 39, 199, 92, 158, 253, 144, 39, 121, 46, 59, 112, 24, 110, 77, 56, 405, 25, 45, 105, 48, 133, 159, 148, 285, 116, 216, 85, 37, 296, 96, 183, 128, 81, 46, 88, 47, 88, 109, 72, 40, 208, 251, 24, 64, 218, 38, 268, 160, 136, 48, 64, 194, 38, 21, 24, 188, 32, 58, 44, 114, 52, 86, 25, 57, 200, 76, 237, 182, 41, 23, 262, 486, 139, 201, 28, 241, 76, 45, 247, 156, 33, 24, 70, 76, 84, 82, 66, 84, 72, 22, 41, 35, 188, 59, 54, 58, 61, 69, 53, 115, 80, 152, 76, 182, 194, 33, 75, 139, 93, 39, 200, 122, 45, 49, 123, 159, 125, 29, 135, 127, 123, 123, 146, 19, 95, 51, 153, 118, 43, 95, 430, 186, 135, 358, 19, 193, 189, 100, 172, 106, 61, 47, 242, 78, 44, 140, 42, 16, 35, 65, 32, 84, 162, 97, 69, 106, 364, 176, 51, 25, 43, 127, 61, 71, 209, 108, 150, 242, 46, 172, 58, 28, 223, 100, 184, 227, 170, 143, 238, 92, 92, 150, 21, 57, 45, 48, 68, 21, 55, 86, 176, 29, 54, 75, 110, 95, 292, 49, 238, 25, 159, 63, 41, 67, 51, 45, 197, 31, 50, 371, 222, 122, 133, 112, 127, 40, 99, 425, 161, 252, 235, 126, 90, 126, 250, 63, 187, 246, 346, 113, 234, 315, 256, 99, 138, 76, 48, 30, 291, 224, 71, 133, 134, 134, 55, 216, 52, 186, 324, 17, 55, 154, 347, 43, 105, 35, 327, 72, 127, 234, 119, 123, 62, 50, 23, 68, 277, 112, 69, 29, 48, 23, 106, 41, 181, 70, 48, 389, 78, 108, 41, 30, 57, 49, 171, 72, 81, 139, 192, 118, 81, 70, 76, 322, 144, 130, 60, 65, 101, 135, 141, 63, 123, 116, 79, 19, 21, 235, 100, 246, 191, 31, 177, 48, 110, 406, 65, 359, 108, 37, 207, 241, 45, 217, 70, 106, 168, 291, 262, 41, 41, 120, 59, 162, 111, 76, 117, 28, 98, 29, 71, 49, 261, 31, 27, 80, 109, 60, 80, 170, 85, 187, 81, 365, 144, 65, 52, 147, 237, 46, 95, 199, 95, 66, 90, 45, 66, 101, 70, 118, 78, 244, 61, 288, 105, 189, 57, 72, 131, 56, 213, 249, 138, 101, 19, 45, 137, 31, 409, 70, 447, 134, 71, 188, 78, 177, 30, 24, 257, 52, 231, 148, 183, 54, 63, 213, 113, 105, 233, 54, 155, 222, 318, 172, 217, 24, 116, 309, 209, 346, 203, 494, 105, 194, 203, 59, 150, 31, 103, 169, 132, 137, 221, 217, 87, 141, 137, 89, 70, 64, 73, 47, 110, 179, 103, 409, 75, 55, 112, 82, 178, 129, 160, 74, 144, 19, 153, 77, 97, 86, 88, 143, 104, 188, 78, 30, 340, 112, 27, 182, 128, 122, 126, 50, 150, 89, 81, 78, 164, 596, 56, 151, 55, 105, 80, 186, 218, 135, 66, 63, 29, 85, 205, 69, 71, 39, 215, 208, 337, 76, 19, 211, 35, 27, 60, 92, 119, 170, 207, 279, 59, 25, 180, 53, 41, 209, 305, 90, 70, 142, 341, 168, 368, 342, 63, 64, 98, 109, 63, 58, 142, 117, 83, 48, 101, 57, 41, 416, 19, 292, 23, 39, 45, 158, 159, 104, 147, 77, 306, 86, 245, 67, 27, 320, 49, 205, 183, 61, 93, 76, 167, 40, 204, 193, 65, 249, 272, 105, 282, 87, 32, 34, 128, 53, 63, 20, 30, 59, 284, 355, 267, 388, 89, 66, 31, 305, 78, 135, 33, 514, 38, 398, 141, 121, 49, 147, 141, 48, 756, 219, 158, 28, 36, 93, 404, 371, 58, 155, 190, 67, 65, 68, 59, 131, 212, 451, 156, 27, 99, 28, 36, 145, 36, 155, 561, 372, 67, 177, 121, 208, 61, 489, 209, 186, 121, 28, 234, 47, 135, 164, 55, 93, 228, 279, 119, 223, 110, 119, 84, 152, 123, 59, 143, 123, 42, 236, 120, 91, 91, 185, 136, 59, 211, 143, 52, 63, 64, 225, 48, 240, 62, 148, 107, 47, 75, 38, 40, 56, 262, 125, 238, 116, 228, 529, 128, 220, 167, 56, 31, 54, 44, 45, 184, 67, 143, 77, 50, 120, 74, 179, 291, 22, 117, 22, 33, 61, 73, 226, 63, 87, 88, 166, 374, 131, 165, 199, 247, 477, 93, 593, 158, 158, 244, 181, 62, 72, 173, 52, 296, 348, 145, 35, 29, 98, 121, 60, 220, 314, 376, 124, 83, 163, 129, 22, 55, 135, 151, 175, 43, 121, 55, 238, 139, 67, 31, 138, 116, 81, 288, 65, 184, 44, 72, 41, 24, 198, 160, 252, 122, 43, 94, 329, 58, 34, 131, 25, 292, 137, 50, 103, 175, 424, 198, 50, 28, 37, 287, 163, 232, 240, 88, 115, 125, 179, 158, 80, 18, 234, 150, 48, 172, 211, 172, 141, 64, 445, 178, 112, 61, 62, 80, 77, 57, 367, 89, 62, 68, 74, 31, 317, 196, 40, 51, 69, 49, 71, 56, 119, 192, 165, 83, 188, 201, 36, 228, 154, 39, 34, 35, 85, 152, 176, 267, 303, 148, 116, 39, 39, 187, 126, 77, 47, 44, 47, 36, 53, 66, 50, 49, 38, 260, 180, 27, 47, 256, 64, 58, 53, 41, 43, 236, 161, 39, 186, 276, 151, 172, 75, 31, 126, 192, 268, 160, 103, 92, 154, 119, 26, 127, 431, 54, 266, 120, 57, 80, 121, 208, 291, 195, 102, 167, 120, 61, 342, 68, 307, 101, 505, 35, 169, 328, 160, 243, 81, 85, 78, 120, 41, 112, 201, 156, 160, 36, 66, 126, 146, 91, 190, 70, 164, 18, 143, 131, 145, 62, 155, 218, 423, 89, 369, 148, 80, 91, 104, 41, 20, 178, 46, 183, 37, 54, 75, 398, 67, 53, 356, 199, 51, 174, 81, 102, 158, 55, 107, 104, 56, 166, 106, 144, 24, 369, 80, 65, 64, 107, 29, 101, 370, 533, 135, 101, 508, 229, 49, 88, 221, 78, 48, 48, 90, 144, 73, 35, 306, 139, 38, 405, 61, 153, 44, 43, 126, 119, 120, 113, 184, 56, 146, 227, 197, 104, 99, 66, 327, 208, 54, 57, 104, 183, 236, 305, 56, 253, 35, 530, 212, 219, 157, 112, 57, 195, 30, 108, 131, 66, 95, 105, 110, 28, 163, 39, 67, 236, 105, 148, 55, 120, 73, 93, 62, 121, 62, 74, 191, 168, 111, 62, 165, 107, 165, 128, 151, 280, 525, 67, 126, 19, 60, 61, 238, 296, 48, 45, 115, 63, 73, 160, 192, 72, 154, 129, 78, 156, 267, 51, 155, 189, 91, 118, 276, 119, 86, 148, 25, 482, 93, 121, 408, 54, 101, 26, 346, 77, 59, 29, 120, 125, 40, 42, 91, 55, 192, 180, 87, 48, 179, 114, 112, 55, 28, 82, 287, 112, 258, 103, 442, 397, 161, 123, 241, 41, 138, 56, 56, 238, 109, 61, 19, 20, 45, 241, 21, 258, 244, 72, 195, 62, 262, 46, 182, 105, 81, 69, 534, 65, 52, 167, 31, 143, 121, 85, 50, 101, 104, 61, 64, 274, 90, 182, 25, 127, 98, 28, 73, 82, 200, 78, 56, 87, 308, 21, 62, 71, 123, 87, 238, 34, 84, 229, 36, 17, 72, 150, 43, 143, 42, 72, 132, 95, 86, 47, 97, 224, 166, 61, 68, 28, 73, 72, 334, 135, 43, 156, 121, 30, 85, 106, 30, 94, 228, 57, 188, 107, 97, 115, 284, 167, 406, 146, 70, 158, 57, 114, 55, 56, 253, 152, 49, 133, 43, 136, 85, 198, 279, 130, 93, 34, 30, 70, 50, 39, 172, 205, 55, 251, 62, 190, 99, 138, 70, 161, 232, 20, 153, 235, 203, 108, 67, 142, 243, 61, 192, 37, 83, 205, 100, 62, 203, 342, 129, 64, 168, 39, 38, 290, 63, 40, 58, 52, 43, 162, 64, 285, 596, 135, 237, 249, 59, 25, 232, 163, 173, 82, 58, 278, 109, 55, 40, 223, 87, 285, 105, 26, 57, 338, 467, 270, 193, 80, 165, 141, 118, 28, 117, 52, 143, 156, 60, 142, 34, 205, 230, 111, 28, 232, 60, 74, 84, 159, 47, 212, 212, 389, 55, 268, 55, 250, 134, 68, 55, 224, 67, 166, 75, 252, 55, 130, 44, 138, 32, 189, 145, 112, 77, 445, 41, 37, 40, 112, 48, 32, 233, 193, 82, 65, 74, 101, 54, 97, 345, 52, 295, 44, 142, 104, 112, 348, 78, 168, 30, 101, 56, 83, 386, 217, 36, 19, 99, 138, 730, 28, 276, 39, 245, 39, 58, 80, 273, 160, 193, 34, 76, 114, 139, 50, 151, 34, 73, 72, 54, 89, 161, 27, 105, 190, 249, 101, 22, 85, 72, 61, 340, 138, 106, 112, 45, 62, 35, 257, 71, 126, 86, 260, 262, 119, 209, 80, 33, 165, 390, 125, 54, 115, 41, 219, 88, 81, 73, 193, 114, 55, 20, 24, 79, 211, 52, 162, 37, 328, 123, 240, 319, 355, 260, 140, 159, 36, 138, 67, 30, 165, 140, 216, 81, 88, 114, 262, 25, 155, 148, 173, 62, 150, 40, 34, 167, 169, 57, 111, 719, 328, 143, 276, 302, 138, 116, 274, 99, 33, 54, 26, 172, 52, 398, 168, 35, 81, 142, 62, 111, 35, 297, 48, 109, 40, 226, 51, 87, 53, 44, 239, 207, 250, 321, 143, 94, 124, 86, 59, 318, 172, 115, 28, 21, 412, 304, 47, 131, 24, 52, 162, 43, 25, 100, 196, 80, 87, 74, 43, 37, 85, 73, 55, 136, 221, 84, 36, 48, 215, 162, 177, 310, 131, 190, 17, 165, 187, 177, 57, 21, 53, 136, 136, 52, 95, 33, 267, 169, 468, 101, 168, 109, 131, 163, 33, 92, 87, 48, 109, 35, 103, 63, 62, 103, 191, 49, 111, 51, 111, 224, 158, 114, 71, 59, 203, 40, 148, 136, 391, 36, 69, 202, 127, 67, 142, 463, 69, 174, 73, 21, 39, 303, 190, 90, 110, 296, 242, 252, 332, 62, 143, 129, 22, 164, 298, 144, 154, 196, 311, 236, 86, 45, 176, 98, 449, 208, 212, 128, 309, 33, 115, 249, 175, 188, 43, 42, 81, 151, 325, 154, 32, 75, 49, 113, 50, 577, 150, 144, 188, 139, 43, 175, 479, 150, 196, 53, 70, 72, 262, 61, 181, 82, 158, 36, 123, 112, 166, 242, 147, 348, 162, 328, 30, 175, 23, 294, 26, 83, 143, 197, 239, 37, 116, 78, 32, 105, 324, 276, 238, 40, 35, 96, 59, 219, 46, 220, 33, 72, 21, 117, 24, 45, 260, 33, 94, 96, 173, 61, 205, 79, 142, 30, 30, 214, 204, 71, 66, 30, 146, 53, 165, 106, 287, 281, 593, 60, 49, 42, 96, 169, 174, 45, 280, 186, 119, 151, 85, 96, 189, 293, 63, 102, 61, 151, 69, 129, 112, 196, 145, 28, 53, 119, 164, 63, 85, 103, 59, 117, 109, 242, 230, 31, 124, 125, 205, 99, 63, 51, 342, 31, 131, 205, 124, 132, 57, 168, 112, 176, 239, 170, 30, 102, 279, 37, 43, 210, 227, 37, 38, 61, 159, 150, 59, 193, 59, 222, 153, 55, 258, 35, 212, 190, 44, 137, 222, 26, 32, 62, 15, 43, 96, 214, 43, 16, 246, 87, 274, 103, 77, 114, 34, 93, 111, 37, 19, 199, 416, 77, 138, 48, 186, 88, 131, 45, 61, 123, 362, 50, 124, 61, 173, 20, 24, 115, 94, 31, 288, 227, 112, 60, 210, 149, 172, 71, 170, 80, 23, 58, 219, 217, 1044, 46, 25, 70, 124, 21, 143, 177, 102, 89, 137, 246, 106, 78, 37, 67, 112, 510, 28, 44, 46, 201, 143, 31, 273, 131, 176, 58, 40, 172, 290, 76, 137, 233, 290, 30, 144, 178, 69, 50, 119, 125, 208, 20, 195, 104, 131, 125, 163, 29, 191, 123, 93, 62, 121, 267, 41, 112, 116, 56, 38, 66, 33, 143, 132, 32, 502, 99, 208, 398, 429, 425, 80, 218, 80, 41, 190, 18, 314, 70, 174, 46, 22, 59, 202, 54, 95, 51, 209, 96, 65, 213, 42, 123, 273, 344, 513, 45, 173, 118, 82, 81, 164, 52, 108, 170, 70, 205, 96, 20, 165, 283, 76, 427, 126, 31, 109, 200, 429, 42, 149, 136, 59, 77, 54, 50, 98, 187, 330, 175, 100, 177, 225, 59, 105, 150, 194, 147, 49, 86, 21, 38, 97, 71, 35, 174, 174, 191, 199, 181, 43, 77, 93, 103, 127, 412, 79, 81, 132, 158, 67, 157, 167, 73, 149, 207, 35, 28, 237, 29, 52, 44, 124, 82, 76, 373, 74, 41, 237, 79, 130, 139, 40, 82, 86, 56, 69, 107, 105, 101, 139, 210, 204, 206, 105, 301, 39, 112, 255, 205, 54, 190, 197, 38, 133, 453, 31, 30, 65, 199, 126, 63, 51, 100, 248, 54, 82, 75, 146, 81, 134, 64, 101, 72, 68, 59, 43, 259, 50, 171, 28, 129, 78, 30, 149, 135, 49, 73, 63, 69, 75, 564, 25, 297, 28, 115, 35, 80, 183, 57, 35, 175, 279, 36, 84, 44, 126, 37, 315, 107, 73, 234, 163, 114, 106, 162, 194, 169, 305, 61, 401, 223, 173, 130, 40, 241, 156, 137, 68, 52, 40, 301, 107, 103, 210, 71, 172, 27, 26, 62, 109, 54, 123, 88, 47, 73, 166, 408, 165, 215, 69, 284, 188, 48, 66, 33, 26, 408, 51, 212, 96, 39, 41, 173, 211, 38, 69, 132, 89, 185, 340, 186, 182, 266, 126, 125, 39, 101, 247, 196, 43, 66, 69, 129, 77, 42, 113, 154, 132, 43, 46, 174, 38, 273, 69, 146, 297, 48, 292, 49, 37, 56, 37, 86, 111, 47, 72, 88, 65, 299, 38, 40, 60, 96, 71, 133, 155, 189, 155, 42, 134, 59, 100, 116, 117, 139, 401, 71, 62, 57, 147, 274, 79, 202, 115, 125, 214, 82, 32, 82, 230, 79, 71, 171, 200, 217, 38, 92, 173, 251, 111, 106, 251, 274, 64, 63, 81, 179, 106, 487, 242, 64, 82, 269, 149, 126, 210, 168, 304, 17, 321, 58, 172, 97, 168, 214, 133, 51, 33, 122, 45, 166, 271, 97, 58, 87, 127, 237, 63, 326, 133, 62, 222, 43, 112, 42, 71, 153, 175, 82, 275, 29, 90, 68, 45, 63, 222, 98, 110, 48, 73, 112, 24, 65, 44, 45, 61, 171, 31, 48, 191, 130, 144, 55, 248, 18, 651, 114, 293, 54, 302, 65, 67, 69, 202, 83, 142, 141, 284, 269, 58, 216, 324, 145, 167, 32, 96, 178, 27, 55, 126, 72, 120, 234, 32, 292, 210, 37, 163, 93, 122, 56, 84, 57, 184, 88, 142, 110, 148, 42, 124, 139, 201, 70, 215, 163, 221, 118, 299, 33, 64, 25, 193, 181, 30, 272, 58, 30, 93, 173, 272, 31, 151, 123, 108, 204, 193, 234, 42, 55, 28, 256, 189, 97, 140, 106, 329, 50, 31, 299, 133, 86, 39, 164, 98, 195, 96, 190, 158, 61, 100, 87, 98, 62, 204, 112, 23, 61, 79, 211, 70, 123, 24, 523, 92, 82, 85, 24, 137, 265, 500, 109, 163, 74, 186, 253, 37, 18, 80, 107, 20, 63, 221, 135, 24, 66, 99, 92, 39, 89, 113, 76, 63, 136, 257, 134, 137, 84, 121, 53, 42, 34, 35, 54, 149, 266, 27, 48, 240, 89, 87, 167, 70, 37, 69, 81, 125, 79, 364, 365, 197, 105, 216, 30, 50, 211, 128, 144, 36, 252, 252, 29, 282, 115, 57, 63, 27, 50, 63, 306, 70, 22, 137, 116, 276, 117, 74, 114, 162, 103, 40, 84, 129, 122, 129, 177, 275, 63, 284, 107, 172, 79, 99, 316, 144, 93, 227, 53, 249, 165, 81, 77, 124, 164, 30, 54, 64, 210, 112, 113, 113, 19, 70, 112, 297, 47, 68, 198, 65, 109, 37, 101, 145, 264, 148, 75, 30, 22, 102, 146, 99, 93, 83, 179, 268, 193, 322, 186, 356, 25, 33, 280, 106, 127, 87, 119, 30, 151, 61, 56, 174, 117, 139, 93, 81, 264, 413, 57, 35, 43, 167, 128, 80, 60, 34, 62, 78, 371, 71, 76, 27, 86, 404, 179, 124, 40, 62, 78, 294, 101, 80, 211, 166, 121, 40, 36, 171, 228, 208, 57, 116, 105, 68, 60, 316, 139, 35, 353, 19, 29, 49, 439, 56, 49, 87, 43, 110, 192, 89, 87, 348, 63, 173, 27, 213, 66, 435, 96, 209, 514, 124, 399, 159, 109, 78, 48, 78, 287, 40, 202, 52, 348, 42, 180, 186, 23, 41, 80, 124, 30, 90, 39, 335, 50, 136, 156, 65, 231, 30, 110, 206, 65, 46, 82, 189, 292, 123, 146, 47, 72, 192, 420, 57, 39, 21, 132, 51, 129, 358, 46, 212, 238, 19, 304, 138, 97, 55, 85, 103, 58, 184, 176, 152, 195, 49, 46, 25, 46, 265, 202, 53, 68, 96, 60, 108, 208, 43, 247, 27, 226, 66, 33, 56, 191, 15, 28, 236, 34, 47, 256, 50, 144, 59, 230, 57, 31, 155, 71, 267, 54, 23, 154, 53, 33, 182, 35, 126, 280, 181, 176, 24, 53, 227, 86, 79, 29, 40, 149, 186, 96, 137, 317, 63, 238, 144, 204, 287, 120, 41, 42, 198, 182, 128, 150, 232, 23, 36, 180, 57, 168, 101, 21, 54, 29, 244, 156, 159, 77, 225, 60, 141, 26, 50, 88, 126, 69, 265, 152, 126, 231, 216, 560, 57, 24, 51, 477, 27, 20, 32, 64, 91, 127, 24, 75, 17, 19, 289, 50, 135, 35, 149, 142, 313, 62, 130, 83, 39, 382, 50, 223, 115, 160, 51, 121, 187, 346, 94, 162, 139, 200, 50, 109, 20, 192, 90, 184, 108, 265, 133, 274, 102, 70, 137, 29, 24, 190, 117, 115, 63, 232, 75, 327, 109, 250, 164, 135, 164, 382, 103, 28, 60, 21, 84, 156, 128, 118, 358, 65, 114, 159, 88, 250, 437, 29, 171, 182, 27, 106, 245, 85, 145, 55, 45, 50, 54, 126, 137, 63, 61, 36, 80, 138, 97, 30, 163, 44, 101, 38, 276, 63, 256, 270, 121, 54, 44, 220, 42, 333, 37, 48, 43, 155, 107, 143, 107, 24, 363, 65, 45, 44, 122, 194, 53, 161, 70, 68, 48, 177, 39, 65, 302, 260, 117, 100, 134, 37, 56, 205, 61, 106, 70, 339, 54, 102, 92, 208, 128, 68, 83, 157, 83, 62, 30, 187, 39, 133, 47, 170, 223, 61, 192, 260, 178, 109, 48, 43, 100, 70, 70, 262, 104, 43, 88, 111, 94, 173, 34, 200, 104, 53, 129, 91, 53, 253, 179, 30, 38, 42, 75, 81, 322, 34, 257, 38, 66, 206, 72, 211, 30, 258, 83, 268, 89, 124, 99, 41, 23, 28, 262, 82, 169, 35, 103, 52, 79, 172, 44, 165, 79, 147, 41, 93, 127, 60, 36, 62, 37, 70, 69, 125, 176, 378, 109, 337, 274, 107, 268, 187, 25, 107, 134, 176, 36, 500, 261, 116, 58, 96, 38, 187, 135, 139, 101, 114, 263, 154, 57, 58, 32, 24, 49, 109, 103, 83, 169, 166, 285, 56, 116, 154, 190, 48, 39, 143, 141, 139, 101, 122, 218, 26, 50, 49, 357, 43, 57, 243, 303, 35, 29, 71, 128, 28, 39, 464, 59, 54, 67, 79, 85, 132, 110, 226, 103, 62, 72, 49, 269, 164, 236, 145, 303, 159, 234, 22, 109, 43, 38, 135, 259, 177, 88, 42, 94, 599, 187, 192, 223, 23, 332, 187, 195, 139, 169, 34, 63, 77, 31, 72, 39, 147, 31, 207, 205, 133, 83, 269, 205, 35, 35, 61, 48, 64, 80, 78, 67, 45, 51, 198, 350, 199, 220, 244, 242, 258, 113, 113, 128, 178, 267, 85, 120, 98, 55, 104, 141, 162, 92, 94, 24, 54, 174, 120, 86, 65, 77, 102, 147, 112, 95, 24, 278, 344, 55, 94, 86, 69, 42, 146, 94, 61, 235, 122, 144, 39, 56, 231, 275, 130, 63, 36, 65, 34, 437, 308, 93, 141, 74, 33, 267, 67, 31, 200, 94, 75, 68, 86, 269, 168, 141, 280, 170, 38, 49, 61, 61, 307, 70, 55, 105, 113, 72, 520, 162, 110, 37, 61, 151, 149, 29, 131, 60, 185, 99, 163, 88, 82, 135, 92, 194, 248, 149, 98, 78, 316, 34, 33, 149, 154, 69, 58, 183, 21, 127, 247, 194, 187, 44, 81, 262, 223, 97, 133, 55, 98, 50, 187, 101, 61, 123, 134, 280, 219, 170, 137, 49, 133, 168, 104, 52, 378, 103, 396, 40, 160, 114, 286, 182, 33, 103, 30, 85, 288, 33, 190, 52, 389, 37, 198, 307, 101, 32, 401, 66, 251, 45, 50, 61, 174, 382, 366, 17, 32, 157, 78, 28, 216, 197, 204, 34, 64, 172, 46, 66, 63, 138, 149, 115, 164, 67, 368, 44, 163, 115, 87, 359, 80, 51, 84, 97, 240, 44, 78, 186, 169, 60, 21, 190, 153, 71, 234, 161, 165, 130, 178, 235, 59, 126, 139, 97, 73, 53, 179, 89, 38, 57, 74, 69, 37, 27, 107, 146, 36, 45, 329, 49, 231, 270, 84, 141, 201, 98, 36, 117, 264, 44, 86, 39, 57, 216, 47, 44, 102, 46, 548, 61, 59, 52, 34, 154, 126, 89, 35, 171, 174, 174, 91, 224, 237, 183, 50, 60, 149, 185, 149, 28, 126, 74, 83, 56, 51, 95, 159, 34, 176, 18, 157, 119, 147, 171, 21, 308, 32, 113, 58, 54, 219, 183, 77, 21, 155, 126, 144, 101, 58, 49, 89, 64, 72, 180, 260, 65, 52, 464, 90, 154, 178, 117, 43, 36, 28, 103, 55, 48, 92, 135, 16, 38, 85, 109, 93, 50, 52, 119, 125, 297, 45, 505, 55, 106, 32, 69, 52, 304, 189, 49, 111, 72, 206, 72, 130, 108, 46, 263, 20, 244, 20, 157, 412, 124, 133, 296, 67, 53, 64, 59, 56, 70, 33, 177, 24, 109, 63, 154, 63, 147, 59, 72, 161, 122, 42, 216, 167, 357, 63, 38, 150, 87, 169, 165, 124, 27, 130, 140, 86, 208, 233, 67, 127, 124, 293, 77, 214, 280, 36, 248, 264, 33, 56, 273, 152, 32, 75, 198, 148, 62, 278, 134, 172, 36, 86, 212, 67, 154, 164, 142, 363, 23, 213, 424, 201, 197, 21, 170, 100, 129, 125, 98, 25, 24, 296, 40, 345, 79, 27, 232, 1, 97, 120, 148, 151, 425, 284, 62, 219, 130, 66, 93, 103, 281, 182, 27, 41, 268, 179, 200, 75, 139, 204, 60, 150, 51, 29, 176, 280, 149, 57, 98, 253, 238, 135, 124, 118, 41, 145, 201, 142, 91, 24, 84, 170, 132, 62, 608, 30, 216, 27, 41, 113, 47, 24, 144, 78, 49, 70, 184, 139, 60, 39, 211, 80, 168, 153, 103, 111, 52, 286, 118, 231, 34, 345, 98, 79, 154, 169, 133, 309, 234, 208, 279, 98, 150, 23, 56, 67, 78, 109, 27, 203, 41, 149, 53, 85, 180, 310, 70, 240, 58, 111, 139, 189, 206, 136, 105, 157, 128, 71, 83, 108, 203, 73, 53, 128, 93, 112, 22, 107, 138, 149, 121, 131, 254, 44, 104, 77, 234, 76, 284, 100, 48, 112, 41, 412, 155, 299, 164, 300, 88, 133, 98, 188, 468, 41, 46, 271, 131, 63, 283, 52, 56, 196, 77, 154, 160, 206, 79, 147, 255, 274, 142, 186, 141, 23, 123, 76, 247, 114, 71, 117, 19, 168, 260, 66, 32, 125, 75, 62, 41, 50, 104, 102, 76, 92, 130, 128, 92, 145, 60, 237, 212, 107, 807, 64, 88, 99, 66, 41, 557, 108, 91, 89, 250, 256, 229, 276, 141, 164, 47, 377, 132, 37, 138, 174, 215, 214, 99, 88, 135, 124, 133, 60, 138, 73, 61, 46, 228, 152, 276, 102, 109, 84, 53, 115, 126, 167, 116, 223, 90, 169, 37, 29, 140, 29, 86, 108, 151, 36, 192, 30, 240, 19, 83, 36, 51, 130, 135, 78, 131, 54, 262, 47, 62, 53, 70, 88, 53, 111, 142, 92, 193, 91, 137, 154, 231, 59, 245, 101, 194, 268, 176, 27, 48, 63, 85, 174, 31, 202, 34, 56, 240, 42, 32, 83, 106, 358, 134, 66, 24, 33, 31, 58, 72, 211, 171, 156, 97, 32, 141, 233, 355, 124, 50, 154, 257, 206, 153, 35, 219, 128, 178, 319, 119, 66, 24, 62, 281, 184, 123, 67, 112, 173, 81, 202, 67, 29, 55, 109, 215, 85, 166, 261, 37, 415, 91, 108, 38, 55, 55, 59, 429, 462, 130, 48, 56, 40, 280, 36, 185, 81, 111, 147, 206, 92, 23, 35, 340, 83, 292, 26, 60, 111, 211, 21, 170, 150, 168, 44, 44, 15, 107, 125, 336, 88, 69, 107, 23, 376, 65, 54, 161, 16, 173, 49, 28, 140, 63, 199, 245, 99, 108, 32, 125, 121, 358, 78, 90, 124, 109, 132, 69, 341, 93, 68, 63, 99, 49, 251, 211, 131, 49, 170, 192, 192, 104, 165, 215, 114, 141, 138, 24, 258, 74, 142, 87, 137, 159, 96, 197, 55, 122, 177, 54, 69, 100, 141, 156, 134, 334, 213, 39, 173, 230, 102, 237, 107, 111, 95, 31, 312, 322, 187, 220, 65, 30, 215, 45, 83, 83, 155, 118, 22, 99, 30, 156, 82, 23, 206, 69, 101, 60, 173, 181, 98, 85, 395, 95, 140, 196, 185, 25, 182, 128, 155, 161, 206, 48, 115, 133, 124, 144, 85, 156, 71, 140, 127, 37, 69, 46, 27, 45, 121, 159, 230, 226, 120, 23, 50, 254, 225, 387, 237, 25, 211, 64, 92, 148, 107, 49, 87, 387, 30, 354, 103, 169, 435, 277, 95, 75, 29, 141, 74, 44, 115, 261, 86, 130, 221, 82, 142, 48, 38, 101, 159, 134, 104, 118, 48, 160, 86, 365, 68, 68, 122, 162, 139, 65, 82, 181, 37, 134, 385, 98, 124, 197, 29, 41, 190, 87, 71, 52, 112, 239, 105, 109, 106, 48, 95, 98, 45, 253, 37, 277, 351, 121, 137, 270, 145, 60, 58, 190, 69, 190, 35, 234, 511, 147, 27, 317, 161, 183, 103, 68, 77, 87, 58, 87, 143, 239, 151, 101, 242, 153, 54, 268, 141, 28, 244, 59, 49, 102, 179, 93, 90, 179, 40, 157, 62, 22, 59, 161, 242, 62, 164, 115, 72, 64, 80, 50, 192, 151, 155, 37, 126, 55, 98, 57, 53, 117, 43, 240, 77, 149, 49, 101, 236, 100, 324, 208, 236, 50, 165, 46, 54, 34, 148, 91, 149, 312, 115, 168, 36, 439, 128, 264, 54, 33, 176, 217, 69, 55, 223, 123, 52, 90, 25, 104, 19, 179, 125, 124, 91, 57, 116, 31, 551, 25, 137, 166, 75, 509, 66, 43, 236, 25, 39, 67, 54, 70, 69, 102, 71, 29, 75, 96, 214, 36, 105, 507, 33, 129, 62, 170, 165, 59, 142, 181, 143, 216, 116, 59, 46, 111, 42, 217, 25, 103, 32, 293, 72, 201, 33, 115, 228, 121, 105, 123, 210, 206, 143, 262, 104, 72, 143, 133, 73, 157, 194, 54, 72, 102, 404, 25, 258, 302, 301, 94, 273, 220, 89, 83, 212, 150, 33, 171, 66, 47, 43, 87, 96, 20, 52, 164, 85, 220, 161, 42, 58, 241, 135, 52, 464, 73, 215, 229, 61, 30, 101, 135, 159, 286, 22, 64, 139, 82, 305, 166, 179, 57, 46, 45, 43, 91, 75, 81, 64, 102, 110, 177, 228, 22, 44, 37, 186, 28, 59, 130, 50, 211, 146, 61, 61, 164, 44, 74, 47, 72, 92, 75, 141, 118, 157, 194, 18, 333, 50, 107, 117, 115, 190, 210, 148, 91, 169, 27, 102, 50, 55, 292, 163, 80, 140, 277, 139, 203, 50, 285, 100, 146, 203, 131, 115, 157, 192, 114, 55, 137, 89, 45, 49, 188, 218, 58, 377, 227, 116, 37, 32, 140, 228, 29, 105, 148, 193, 121, 160, 550, 65, 36, 186, 65, 99, 42, 194, 202, 226, 166, 29, 350, 287, 216, 278, 43, 172, 177, 432, 241, 187, 373, 217, 83, 204, 73, 61, 245, 63, 59, 81, 118, 53, 71, 316, 53, 325, 67, 63, 116, 131, 126, 165, 63, 139, 72, 284, 66, 50, 135, 128, 167, 57, 167, 368, 82, 147, 61, 80, 77, 84, 42, 224, 76, 65, 74, 39, 185, 53, 62, 190, 91, 25, 84, 61, 185, 255, 154, 71, 297, 148, 76, 439, 316, 186, 51, 48, 301, 41, 62, 192, 264, 63, 79, 61, 172, 125, 203, 19, 93, 78, 135, 83, 200, 96, 145, 107, 135, 92, 38, 25, 462, 261, 89, 38, 173, 110, 25, 136, 66, 216, 190, 251, 132, 200, 43, 103, 184, 305, 36, 182, 156, 333, 133, 54, 143, 98, 106, 96, 133, 139, 91, 261, 65, 312, 56, 84, 47, 86, 321, 364, 119, 184, 46, 146, 136, 150, 43, 59, 218, 481, 325, 32, 139, 109, 205, 66, 154, 33, 265, 74, 92, 39, 69, 64, 104, 187, 105, 48, 94, 56, 77, 85, 75, 134, 30, 102, 20, 79, 107, 61, 75, 42, 386, 128, 35, 42, 93, 108, 51, 62, 25, 53, 79, 87, 182, 30, 107, 265, 31, 198, 121, 327, 51, 83, 44, 86, 128, 211, 57, 183, 118, 88, 125, 49, 201, 79, 48, 97, 44, 127, 114, 355, 68, 85, 198, 226, 62, 23, 306, 26, 131, 39, 268, 92, 98, 157, 22, 158, 95, 94, 168, 166, 70, 64, 315, 123, 151, 177, 97, 84, 120, 80, 98, 134, 89, 48, 57, 387, 345, 252, 132, 109, 319, 165, 237, 252, 77, 126, 59, 95, 154, 56, 85, 75, 321, 156, 91, 247, 539, 95, 171, 40, 26, 230, 54, 59, 225, 102, 216, 292, 87, 22, 61, 165, 30, 255, 36, 212, 337, 56, 129, 75, 25, 319, 55, 36, 93, 28, 71, 122, 76, 183, 159, 222, 46, 68, 154, 27, 75, 125, 233, 52, 73, 197, 128, 367, 23, 162, 50, 109, 343, 33, 67, 75, 102, 68, 166, 39, 112, 149, 128, 48, 21, 150, 60, 62, 181, 120, 34, 139, 107, 34, 184, 80, 254, 120, 51, 106, 32, 53, 39, 259, 75, 185, 264, 121, 184, 56, 167, 44, 206, 71, 69, 106, 156, 183, 185, 93, 102, 195, 173, 92, 130, 140, 280, 131, 58, 58, 64, 187, 367, 42, 232, 156, 19, 117, 112, 159, 45, 39, 48, 257, 205, 142, 120, 82, 31, 21, 44, 157, 27, 51, 143, 202, 60, 172, 258, 30, 55, 21, 191, 317, 375, 125, 141, 74, 112, 301, 35, 438, 70, 71, 85, 97, 207, 84, 38, 358, 100, 60, 135, 72, 59, 48, 37, 86, 25, 149, 312, 138, 55, 353, 82, 212, 228, 223, 101, 67, 102, 42, 103, 123, 161, 194, 55, 223, 152, 92, 41, 30, 138, 21, 161, 291, 83, 41, 265, 284, 35, 61, 65, 32, 41, 272, 108, 41, 126, 82, 144, 64, 27, 102, 545, 85, 181, 34, 84, 106, 106, 94, 98, 76, 524, 120, 64, 147, 178, 189, 208, 91, 38, 112, 65, 33, 47, 460, 164, 78, 234, 34, 245, 41, 27, 93, 63, 101, 18, 19, 44, 61, 89, 274, 318, 92, 63, 51, 143, 154, 172, 211, 52, 151, 39, 110, 242, 278, 100, 49, 65, 38, 20, 90, 71, 46, 60, 72, 51, 149, 121, 116, 67, 66, 165, 232, 186, 196, 124, 80, 72, 66, 206, 65, 235, 51, 78, 172, 177, 27, 22, 176, 108, 66, 32, 141, 147, 219, 80, 33, 132, 128, 49, 44, 82, 142, 188, 42, 73, 29, 45, 93, 252, 167, 353, 248, 154, 48, 129, 432, 76, 46, 147, 46, 52, 52, 464, 163, 164, 187, 116, 37, 170, 42, 59, 118, 45, 83, 59, 152, 67, 273, 177, 30, 101, 62, 100, 187, 207, 276, 216, 39, 154, 114, 169, 52, 106, 45, 68, 89, 36, 63, 117, 251, 108, 411, 83, 142, 84, 425, 132, 41, 138, 53, 319, 83, 134, 37, 149, 34, 158, 38, 76, 355, 68, 300, 110, 72, 42, 256, 34, 53, 190, 85, 201, 151, 74, 135, 53, 27, 75, 110, 170, 206, 115, 103, 97, 106, 203, 55, 49, 116, 56, 74, 55, 43, 96, 83, 146, 244, 241, 59, 58, 48, 22, 112, 46, 335, 282, 30, 230, 49, 28, 62, 55, 22, 45, 252, 66, 213, 50, 277, 83, 136, 54, 110, 57, 42, 43, 54, 174, 78, 74, 110, 37, 47, 55, 76, 191, 279, 118, 92, 125, 76, 57, 117, 81, 60, 22, 222, 31, 51, 109, 152, 123, 68, 125, 69, 224, 276, 11, 148, 57, 35, 84, 146, 176, 81, 123, 115, 282, 50, 27, 36, 82, 173, 103, 231, 197, 152, 74, 82, 200, 124, 21, 109, 255, 57, 83, 227, 70, 68, 133, 75, 34, 123, 270, 28, 38, 147, 60, 98, 269, 336, 183, 62, 26, 238, 33, 44, 65, 122, 147, 47, 206, 59, 93, 50, 36, 22, 102, 261, 104, 156, 148, 137, 119, 141, 121, 31, 39, 148, 487, 79, 149, 102, 44, 159, 111, 104, 44, 106, 22, 31, 212, 164, 152, 80, 158, 167, 44, 158, 213, 519, 203, 334, 220, 217, 38, 129, 65, 301, 123, 57, 19, 190, 117, 77, 67, 316, 362, 32, 98, 39, 84, 88, 177, 119, 180, 146, 104, 71, 163, 284, 348, 31, 51, 286, 76, 120, 183, 360, 27, 349, 59, 146, 29, 74, 99, 104, 30, 36, 56, 77, 144, 86, 54, 338, 141, 168, 232, 70, 104, 80, 18, 58, 178, 81, 82, 166, 284, 20, 114, 119, 24, 49, 94, 118, 188, 92, 38, 71, 70, 230, 29, 124, 233, 114, 49, 84, 123, 54, 64, 194, 141, 200, 255, 143, 29, 171, 128, 180, 36, 106, 282, 217, 123, 47, 104, 84, 177, 17, 175, 72, 36, 113, 29, 117, 126, 113, 208, 109, 230, 64, 178, 274, 17, 190, 94, 270, 171, 85, 144, 110, 354, 74, 101, 19, 165, 64, 201, 21, 86, 305, 41, 60, 56, 99, 49, 123, 126, 60, 146, 32, 116, 76, 99, 106, 171, 181, 94, 139, 40, 198, 272, 130, 46, 143, 240, 47, 64, 145, 56, 92, 111, 107, 58, 55, 99, 77, 40, 216, 139, 29, 64, 103, 24, 37, 489, 23, 62, 66, 212, 138, 180, 106, 213, 24, 24, 45, 113, 252, 21, 142, 125, 67, 246, 51, 29, 104, 69, 120, 43, 186, 101, 52, 222, 130, 69, 36, 96, 156, 354, 236, 20, 217, 32, 252, 53, 62, 214, 32, 36, 70, 203, 74, 271, 100, 461, 169, 27, 309, 64, 44, 66, 216, 209, 54, 38, 36, 44, 124, 106, 72, 153, 105, 215, 398, 28, 123, 192, 203, 68, 78, 40, 300, 185, 38, 27, 78, 46, 97, 410, 43, 159, 29, 283, 38, 32, 145, 125, 92, 45, 152, 41, 125, 163, 275, 338, 16, 83, 18, 307, 55, 97, 185, 129, 107, 246, 133, 50, 111, 46, 180, 98, 145, 184, 169, 255, 139, 276, 46, 92, 197, 112, 27, 139, 172, 97, 152, 101, 368, 214, 279, 50, 112, 45, 48, 42, 193, 42, 53, 191, 87, 354, 41, 216, 178, 184, 117, 211, 70, 53, 83, 34, 201, 158, 20, 57, 19, 101, 140, 266, 142, 60, 423, 328, 83, 89, 296, 96, 674, 348, 112, 196, 329, 96, 46, 96, 129, 32, 349, 65, 149, 176, 83, 397, 60, 56, 83, 206, 128, 91, 30, 85, 111, 78, 213, 188, 54, 63, 64, 58, 43, 166, 150, 167, 46, 46, 242, 42, 233, 26, 148, 200, 184, 159, 36, 131, 203, 165, 180, 287, 59, 142, 254, 212, 56, 75, 48, 199, 157, 59, 163, 52, 227, 95, 30, 101, 194, 243, 130, 66, 74, 44, 236, 93, 16, 116, 231, 92, 22, 186, 99, 94, 232, 181, 85, 26, 189, 186, 234, 194, 225, 102, 107, 70, 54, 166, 261, 323, 93, 388, 125, 78, 57, 68, 102, 349, 208, 31, 22, 149, 39, 52, 76, 38, 85, 169, 98, 53, 124, 159, 312, 41, 314, 523, 86, 121, 163, 83, 65, 116, 395, 281, 56, 296, 207, 66, 105, 57, 69, 291, 240, 187, 241, 468, 327, 267, 189, 341, 37, 152, 54, 190, 356, 170, 61, 187, 43, 118, 32, 108, 188, 41, 341, 24, 82, 49, 24, 74, 183, 53, 334, 109, 192, 95, 400, 138, 57, 29, 190, 61, 60, 43, 59, 257, 134, 20, 270, 65, 174, 63, 169, 203, 291, 146, 102, 49, 46, 69, 110, 27, 73, 23, 78, 29, 136, 61, 297, 99, 67, 363, 75, 152, 98, 157, 115, 53, 209, 107, 31, 35, 54, 209, 119, 49, 183, 66, 49, 20, 121, 45, 100, 101, 126, 364, 284, 108, 150, 82, 121, 294, 36, 285, 263, 271, 79, 226, 326, 38, 150, 93, 36, 43, 248, 404, 133, 41, 187, 28, 328, 42, 151, 44, 84, 76, 207, 179, 124, 54, 182, 137, 451, 69, 150, 68, 245, 225, 200, 41, 24, 28, 345, 158, 24, 234, 209, 57, 337, 66, 143, 373, 152, 88, 153, 325, 241, 73, 82, 155, 354, 175, 199, 20, 55, 78, 216, 184, 31, 73, 86, 94, 69, 51, 32, 97, 205, 153, 35, 40, 214, 77, 28, 41, 75, 47, 138, 189, 152, 211, 177, 40, 36, 245, 20, 41, 103, 328, 40, 74, 491, 65, 103, 29, 285, 87, 106, 294, 362, 100, 48, 76, 129, 123, 131, 30, 38, 315, 217, 51, 153, 43, 183, 273, 229, 53, 66, 73, 71, 197, 57, 42, 251, 189, 417, 18, 268, 127, 169, 39, 256, 50, 126, 57, 143, 92, 145, 116, 340, 28, 66, 118, 133, 23, 116, 224, 41, 39, 300, 150, 390, 35, 45, 23, 306, 75, 222, 22, 160, 453, 301, 494, 191, 56, 87, 185, 111, 112, 63, 146, 52, 45, 50, 24, 143, 26, 35, 341, 148, 256, 325, 251, 36, 486, 134, 91, 84, 136, 65, 63, 152, 57, 63, 173, 328, 199, 210, 222, 226, 244, 33, 211, 61, 179, 40, 170, 128, 61, 84, 134, 114, 109, 279, 65, 67, 84, 209, 205, 178, 50, 21, 213, 36, 57, 143, 106, 29, 158, 26, 89, 338, 88, 114, 48, 119, 57, 245, 48, 169, 97, 489, 68, 104, 103, 177, 196, 205, 109, 82, 351, 149, 224, 139, 94, 76, 220, 167, 60, 50, 53, 204, 107, 329, 262, 48, 23, 44, 54, 86, 82, 147, 33, 124, 46, 42, 284, 206, 215, 157, 300, 34, 138, 140, 31, 66, 115, 27, 119, 56, 174, 75, 100, 76, 135, 341, 62, 83, 30, 42, 34, 132, 28, 22, 277, 184, 70, 98, 400, 245, 44, 211, 112, 164, 62, 75, 106, 56, 40, 29, 233, 42, 233, 204, 134, 79, 239, 91, 231, 37, 257, 45, 302, 148, 213, 54, 246, 82, 60, 90, 180, 68, 137, 27, 242, 45, 110, 137, 77, 29, 155, 82, 63, 121, 239, 54, 221, 619, 21, 55, 264, 41, 177, 78, 178, 136, 15, 85, 186, 24, 144, 781, 80, 102, 259, 172, 79, 183, 318, 57, 101, 265, 89, 74, 148, 182, 76, 45, 116, 552, 89, 215, 183, 36, 101, 29, 83, 103, 209, 152, 59, 47, 182, 121, 93, 70, 282, 53, 118, 92, 168, 71, 26, 66, 275, 198, 30, 146, 126, 252, 274, 30, 136, 91, 22, 144, 46, 112, 641, 233, 148, 82, 93, 151, 43, 45, 138, 154, 236, 99, 116, 16, 32, 160, 88, 47, 50, 26, 90, 247, 92, 141, 98, 27, 57, 329, 179, 258, 183, 27, 48, 31, 166, 130, 225, 31, 88, 167, 62, 71, 29, 97, 92, 251, 102, 263, 86, 38, 216, 179, 241, 144, 43, 32, 55, 34, 149, 126, 22, 24, 116, 226, 134, 38, 81, 120, 40, 306, 53, 44, 105, 261, 142, 90, 329, 36, 135, 166, 84, 65, 152, 84, 154, 37, 22, 63, 106, 18, 107, 232, 44, 85, 47, 358, 84, 131, 133, 91, 76, 63, 30, 88, 204, 147, 154, 133, 60, 52, 68, 137, 439, 119, 48, 160, 149, 235, 84, 403, 51, 141, 278, 129, 44, 243, 162, 122, 277, 125, 250, 309, 91, 67, 156, 231, 193, 61, 52, 165, 62, 115, 86, 42, 75, 108, 39, 144, 80, 103, 41, 156, 90, 283, 62, 60, 168, 65, 298, 81, 88, 79, 21, 137, 25, 153, 214, 82, 39, 74, 216, 112, 102, 82, 518, 59, 24, 33, 160, 229, 93, 80, 115, 231, 55, 38, 155, 53, 50, 73, 39, 106, 153, 108, 41, 191, 217, 306, 28, 210, 55, 149, 222, 69, 95, 322, 108, 247, 151, 94, 250, 88, 98, 26, 25, 39, 27, 58, 38, 382, 24, 136, 85, 158, 69, 77, 42, 101, 158, 225, 128, 137, 144, 124, 165, 111, 115, 109, 62, 53, 82, 92, 205, 221, 38, 63, 115, 39, 35, 168, 463, 111, 119, 206, 60, 197, 132, 230, 90, 216, 48, 31, 43, 100, 25, 56, 107, 213, 170, 211, 74, 21, 38, 57, 173, 245, 186, 138, 218, 100, 45, 117, 43, 101, 52, 109, 177, 152, 461, 53, 149, 95, 202, 94, 38, 69, 231, 350, 112, 41, 83, 104, 48, 50, 29, 194, 219, 60, 66, 90, 102, 112, 23, 166, 96, 66, 77, 197, 72, 124, 305, 38, 148, 60, 106, 144, 274, 213, 129, 394, 247, 107, 106, 152, 236, 160, 54, 191, 47, 184, 128, 116, 84, 393, 48, 498, 20, 63, 378, 223, 145, 93, 195, 325, 40, 29, 111, 486, 63, 197, 90, 167, 377, 66, 30, 57, 83, 27, 25, 155, 321, 149, 233, 249, 57, 45, 150, 137, 173, 280, 67, 46, 68, 21, 171, 43, 241, 57, 148, 58, 55, 65, 16, 450, 293, 244, 65, 130, 319, 125, 96, 32, 50, 212, 99, 180, 182, 81, 158, 126, 85, 200, 140, 86, 136, 73, 470, 59, 96, 82, 209, 46, 108, 86, 249, 141, 307, 20, 41, 226, 40, 89, 111, 317, 75, 205, 111, 150, 25, 74, 46, 157, 302, 190, 195, 196, 48, 373, 67, 24, 103, 103, 176, 308, 190, 250, 268, 112, 64, 212, 39, 247, 117, 52, 37, 260, 101, 85, 60, 154, 92, 40, 284, 142, 175, 137, 437, 120, 103, 67, 64, 289, 17, 162, 36, 105, 125, 321, 174, 234, 113, 119, 25, 26, 66, 69, 151, 50, 45, 103, 145, 45, 44, 132, 221, 162, 93, 269, 23, 387, 138, 66, 159, 123, 114, 287, 144, 229, 28, 64, 358, 156, 104, 97, 86, 170, 176, 50, 198, 24, 71, 111, 89, 65, 83, 128, 54, 40, 100, 90, 105, 255, 230, 87, 151, 225, 180, 163, 39, 66, 77, 143, 85, 53, 162, 68, 63, 55, 30, 194, 393, 273, 30, 229, 26, 88, 113, 239, 33, 161, 32, 182, 214, 211, 25, 265, 190, 127, 104, 92, 53, 151, 135, 105, 69, 152, 220, 190, 24, 67, 203, 52, 276, 182, 104, 255, 92, 117, 40, 305, 122, 67, 40, 34, 77, 32, 37, 70, 146, 77, 336, 98, 144, 117, 76, 53, 69, 128, 139, 100, 919, 145, 401, 33, 57, 248, 250, 294, 137, 185, 41, 68, 53, 70, 44, 61, 163, 365, 134, 210, 252, 71, 80, 134, 148, 229, 99, 241, 159, 77, 57, 222, 47, 188, 76, 304, 26, 46, 91, 71, 64, 200, 104, 171, 25, 65, 244, 95, 337, 200, 36, 99, 42, 339, 105, 240, 24, 170, 406, 90, 189, 30, 426, 474, 66, 45, 49, 228, 141, 23, 128, 81, 119, 54, 66, 141, 67, 213, 108, 251, 82, 52, 55, 99, 44, 148, 144, 54, 81, 110, 88, 282, 79, 46, 150, 114, 22, 50, 146, 86, 55, 180, 41, 67, 137, 692, 117, 331, 53, 131, 42, 236, 124, 158, 141, 205, 103, 284, 38, 81, 580, 33, 52, 50, 65, 77, 31, 50, 25, 266, 162, 76, 115, 23, 206, 229, 74, 211, 264, 213, 184, 207, 377, 316, 96, 51, 26, 54, 437, 158, 119, 137, 112, 25, 51, 296, 31, 61, 288, 85, 102, 40, 41, 114, 26, 89, 280, 302, 175, 637, 88, 342, 173, 113, 208, 376, 117, 92, 466, 142, 35, 175, 148, 205, 84, 64, 309, 156, 33, 24, 144, 105, 54, 44, 403, 41, 91, 141, 84, 138, 51, 41, 75, 41, 144, 138, 165, 236, 553, 36, 78, 77, 438, 83, 124, 107, 49, 57, 428, 159, 173, 30, 66, 315, 44, 354, 178, 109, 289, 181, 124, 226, 66, 36, 175, 175, 201, 267, 138, 240, 35, 93, 111, 81, 30, 345, 405, 130, 206, 219, 97, 37, 109, 181, 33, 156, 62, 110, 170, 27, 83, 126, 41, 86, 282, 215, 160, 158, 90, 38, 30, 204, 206, 189, 314, 324, 192, 358, 74, 86, 30, 111, 118, 184, 225, 23, 133, 114, 130, 21, 20, 58, 181, 51, 49, 158, 98, 164, 107, 35, 166, 364, 113, 82, 106, 28, 334, 46, 236, 135, 34, 186, 58, 207, 151, 59, 54, 117, 165, 159, 458, 49, 123, 45, 110, 153, 76, 153, 80, 66, 47, 51, 175, 79, 70, 96, 260, 97, 390, 53, 208, 119, 144, 234, 59, 130, 220, 95, 67, 161, 103, 113, 98, 468, 53, 55, 53, 115, 84, 239, 190, 210, 44, 229, 75, 205, 45, 15, 334, 70, 323, 199, 152, 94, 279, 132, 37, 256, 189, 125, 265, 67, 107, 202, 316, 104, 33, 48, 135, 183, 408, 103, 68, 27, 58, 22, 54, 193, 29, 40, 96, 25, 141, 58, 278, 51, 152, 214, 110, 111, 49, 31, 190, 99, 184, 192, 69, 92, 29, 359, 435, 78, 29, 387, 25, 44, 100, 213, 34, 45, 122, 39, 250, 64, 218, 118, 69, 58, 85, 674, 94, 247, 348, 26, 111, 37, 160, 37, 158, 282, 101, 38, 22, 143, 91, 100, 167, 38, 33, 60, 73, 175, 64, 400, 87, 80, 95, 62, 78, 35, 25, 34, 46, 95, 46, 38, 188, 94, 25, 105, 37, 124, 144, 157, 168, 141, 41, 118, 62, 74, 49, 132, 40, 149, 108, 217, 80, 41, 146, 103, 31, 304, 41, 25, 96, 82, 159, 67, 220, 160, 32, 156, 71, 51, 178, 190, 22, 87, 167, 128, 110, 67, 105, 209, 299, 247, 340, 67, 49, 151, 140, 97, 46, 129, 68, 408, 94, 43, 169, 244, 251, 115, 26, 128, 64, 107, 77, 108, 38, 78, 405, 117, 220, 165, 93, 91, 266, 359, 210, 51, 146, 117, 224, 205, 258, 89, 63, 239, 164, 121, 151, 139, 43, 78, 293, 163, 315, 40, 232, 67, 179, 47, 23, 203, 39, 592, 80, 80, 319, 69, 518, 195, 98, 43, 181, 34, 240, 39, 145, 360, 49, 88, 25, 106, 194, 42, 127, 45, 168, 49, 16, 28, 304, 55, 36, 154, 105, 163, 164, 20, 31, 15, 35, 110, 66, 21, 58, 63, 35, 42, 84, 129, 17, 39, 79, 347, 289, 56, 23, 174, 25, 201, 195, 135, 16, 292, 95, 47, 65, 32, 63, 232, 29, 34, 446, 240, 50, 230, 128, 73, 50, 102, 42, 182, 268, 112, 135, 194, 84, 151, 133, 47, 254, 191, 48, 18, 168, 195, 158, 152, 37, 260, 21, 137, 30, 122, 122, 335, 192, 93, 49, 322, 149, 101, 342, 354, 51, 87, 176, 450, 62, 155, 101, 264, 81, 89, 142, 131, 55, 164, 169, 101, 114, 168, 74, 123, 364, 136, 151, 138, 91, 546, 171, 68, 136, 22, 137, 353, 126, 242, 88, 234, 28, 35, 26, 146, 72, 25, 68, 120, 82, 65, 111, 29, 81, 216, 85, 252, 63, 199, 229, 62, 320, 81, 292, 275, 108, 289, 145, 89, 120, 38, 329, 295, 136, 108, 272, 104, 56, 185, 219, 35, 93, 42, 64, 76, 13, 127, 39, 189, 88, 179, 87, 55, 157, 355, 212, 85, 31, 136, 58, 75, 133, 97, 230, 60, 217, 37, 88, 181, 198, 78, 118, 37, 148, 178, 229, 64, 344, 321, 80, 157, 158, 216, 41, 75, 43, 115, 181, 233, 48, 112, 72, 31, 88, 70, 101, 370, 47, 281, 306, 24, 97, 43, 63, 58, 64, 62, 35, 194, 73, 246, 69, 114, 303, 69, 190, 76, 136, 82, 57, 414, 23, 312, 41, 192, 61, 84, 138, 64, 48, 87, 101, 299, 180, 210, 62, 92, 195, 22, 54, 24, 69, 115, 388, 35, 59, 102, 77, 85, 86, 24, 64, 128, 101, 132, 34, 368, 191, 38, 58, 376, 191, 60, 71, 140, 214, 216, 106, 271, 172, 113, 157, 63, 50, 82, 61, 46, 174, 199, 30, 112, 162, 110, 300, 20, 220, 206, 255, 33, 42, 273, 32, 214, 19, 140, 101, 70, 197, 16, 204, 196, 181, 330, 154, 24, 147, 89, 61, 157, 128, 61, 107, 98, 83, 54, 85, 122, 54, 205, 133, 34, 59, 41, 273, 106, 110, 378, 59, 41, 31, 93, 16, 118, 144, 121, 146, 72, 44, 309, 195, 147, 58, 233, 52, 167, 108, 77, 84, 247, 61, 140, 41, 18, 102, 156, 250, 88, 52, 184, 112, 209, 101, 214, 29, 375, 134, 111, 104, 212, 85, 40, 27, 106, 51, 86, 20, 149, 212, 42, 212, 84, 81, 25, 46, 164, 164, 137, 261, 138, 67, 75, 180, 53, 94, 93, 179, 149, 38, 151, 74, 129, 151, 168, 346, 49, 86, 124, 164, 207, 180, 36, 30, 82, 133, 37, 36, 252, 55, 32, 215, 166, 61, 326, 45, 275, 326, 82, 115, 111, 106, 240, 24, 61, 38, 322, 49, 45, 162, 291, 68, 61, 256, 166, 239, 35, 70, 125, 147, 117, 190, 164, 322, 101, 44, 152, 80, 40, 34, 91, 72, 61, 337, 157, 91, 54, 705, 266, 54, 128, 71, 34, 31, 264, 148, 134, 123, 66, 261, 15, 60, 61, 135, 102, 256, 171, 373, 268, 340, 114, 90, 38, 33, 122, 210, 92, 51, 149, 246, 29, 32, 155, 179, 80, 119, 35, 186, 281, 190, 230, 191, 183, 114, 116, 91, 125, 204, 63, 50, 52, 28, 34, 255, 91, 66, 271, 81, 34, 181, 27, 101, 165, 232, 41, 46, 149, 42, 49, 24, 62, 42, 84, 188, 275, 30, 17, 165, 31, 66, 36, 170, 27, 434, 340, 62, 97, 56, 62, 359, 237, 98, 17, 62, 82, 29, 183, 216, 73, 70, 51, 43, 139, 164, 75, 42, 139, 67, 69, 82, 112, 100, 56, 198, 415, 165, 134, 28, 67, 43, 111, 95, 141, 31, 111, 195, 483, 85, 175, 180, 20, 52, 136, 156, 114, 305, 165, 82, 244, 41, 81, 224, 52, 64, 151, 98, 300, 29, 44, 47, 205, 296, 282, 68, 57, 72, 173, 96, 56, 401, 36, 62, 133, 100, 168, 42, 58, 121, 129, 50, 170, 170, 53, 115, 309, 83, 201, 179, 108, 95, 25, 47, 34, 83, 31, 79, 102, 33, 87, 54, 39, 120, 112, 278, 129, 108, 236, 133, 56, 152, 315, 269, 62, 288, 40, 124, 183, 90, 127, 188, 57, 157, 78, 110, 310, 52, 141, 155, 93, 119, 161, 111, 90, 133, 70, 133, 194, 134, 194, 253, 184, 115, 348, 151, 37, 102, 116, 36, 34, 291, 41, 30, 44, 50, 61, 81, 45, 380, 153, 172, 193, 38, 122, 188, 464, 159, 144, 72, 329, 131, 109, 73, 14, 98, 106, 64, 257, 244, 105, 25, 80, 141, 79, 188, 64, 39, 148, 29, 84, 216, 154, 55, 159, 48, 59, 78, 98, 165, 168, 84, 31, 67, 256, 55, 110, 651, 106, 125, 38, 140, 131, 78, 299, 61, 45, 83, 102, 262, 238, 65, 239, 141, 56, 60, 29, 33, 346, 84, 192, 59, 106, 91, 75, 52, 31, 241, 46, 177, 130, 133, 100, 35, 177, 178, 240, 68, 116, 53, 111, 78, 127, 19, 294, 244, 157, 39, 46, 211, 179, 52, 111, 82, 252, 158, 35, 518, 57, 172, 104, 72, 44, 34, 172, 324, 24, 220, 34, 73, 55, 169, 118, 142, 281, 94, 31, 29, 19, 25, 65, 25, 155, 189, 26, 332, 51, 39, 121, 79, 73, 175, 28, 231, 281, 266, 359, 154, 106, 109, 262, 112, 175, 122, 19, 189, 58, 194, 83, 166, 53, 261, 25, 82, 49, 75, 103, 179, 153, 35, 75, 57, 68, 214, 201, 23, 118, 75, 190, 67, 40, 66, 75, 117, 131, 328, 77, 223, 64, 62, 141, 177, 180, 207, 72, 79, 50, 198, 270, 139, 339, 212, 168, 73, 111, 114, 270, 341, 115, 206, 233, 332, 88, 53, 136, 97, 60, 138, 169, 218, 71, 109, 53, 59, 64, 33, 74, 20, 120, 53, 25, 92, 128, 77, 130, 252, 61, 164, 146, 56, 68, 300, 189, 259, 98, 90, 156, 35, 69, 21, 165, 357, 35, 149, 72, 43, 53, 63, 72, 76, 41, 22, 165, 44, 87, 140, 36, 120, 36, 105, 112, 107, 239, 154, 158, 146, 236, 50, 94, 142, 77, 206, 195, 179, 148, 99, 130, 27, 49, 227, 113, 174, 79, 31, 63, 141, 318, 284, 60, 102, 43, 30, 29, 21, 37, 246, 45, 75, 103, 177, 42, 145, 31, 67, 49, 72, 32, 217, 115, 115, 265, 64, 298, 123, 87, 62, 171, 31, 83, 175, 156, 30, 190, 46, 43, 54, 62, 300, 74, 74, 27, 46, 260, 120, 168, 153, 88, 129, 84, 160, 113, 173, 267, 261, 173, 325, 156, 208, 49, 400, 57, 119, 83, 348, 54, 128, 116, 49, 168, 143, 242, 128, 97, 112, 154, 25, 102, 55, 146, 194, 138, 126, 20, 193, 269, 542, 150, 299, 82, 134, 130, 200, 44, 43, 199, 155, 32, 281, 263, 41, 63, 189, 374, 202, 84, 119, 37, 25, 69, 53, 180, 163, 43, 190, 97, 138, 162, 165, 59, 183, 59, 110, 700, 108, 45, 18, 107, 377, 117, 75, 87, 151, 213, 78, 119, 120, 280, 53, 121, 192, 503, 272, 101, 96, 186, 74, 63, 51, 213, 227, 111, 147, 132, 21, 351, 99, 145, 93, 252, 486, 51, 34, 68, 150, 136, 330, 326, 93, 36, 292, 137, 63, 583, 138, 191, 136, 421, 32, 71, 323, 89, 44, 272, 328, 84, 175, 73, 34, 110, 164, 199, 130, 28, 34, 489, 93, 37, 98, 253, 43, 67, 47, 106, 116, 76, 266, 29, 104, 151, 33, 135, 409, 183, 125, 185, 37, 193, 154, 152, 101, 60, 44, 373, 77, 47, 75, 163, 33, 232, 173, 125, 33, 102, 18, 76, 57, 39, 150, 67, 251, 367, 205, 159, 25, 62, 82, 64, 104, 121, 154, 139, 457, 232, 252, 144, 47, 213, 257, 229, 21, 486, 22, 24, 98, 293, 214, 27, 20, 111, 63, 33, 61, 199, 69, 76, 244, 30, 154, 22, 199, 157, 490, 174, 66, 171, 338, 36, 58, 79, 240, 91, 85, 75, 44, 115, 218, 93, 399, 132, 59, 121, 45, 40, 84, 73, 145, 26, 85, 131, 37, 93, 204, 108, 16, 204, 245, 154, 224, 177, 58, 177, 380, 102, 101, 150, 159, 186, 125, 49, 405, 113, 55, 50, 466, 113, 75, 50, 81, 186, 212, 54, 150, 122, 63, 97, 35, 69, 74, 82, 86, 175, 68, 184, 51, 330, 46, 63, 115, 38, 150, 130, 343, 113, 157, 32, 44, 216, 358, 108, 200, 40, 58, 302, 144, 274, 208, 123, 124, 41, 209, 62, 217, 22, 61, 149, 162, 310, 150, 88, 202, 59, 128, 44, 84, 100, 117, 105, 113, 150, 301, 208, 73, 61, 48, 209, 175, 53, 62, 226, 231, 75, 168, 165, 72, 223, 42, 261, 66, 84, 127, 141, 32, 220, 99, 51, 50, 119, 71, 235, 81, 26, 82, 62, 127, 160, 324, 69, 20, 200, 258, 194, 163, 59, 54, 300, 76, 183, 301, 102, 358, 78, 32, 77, 23, 491, 25, 148, 82, 41, 61, 426, 21, 128, 21, 24, 262, 92, 80, 103, 52, 136, 67, 88, 311, 38, 211, 55, 181, 62, 65, 29, 60, 44, 33, 100, 92, 67, 67, 37, 220, 214, 89, 248, 87, 167, 65, 84, 240, 165, 79, 67, 323, 56, 98, 43, 39, 84, 67, 60, 51, 86, 321, 181, 82, 158, 116, 105, 68, 148, 71, 247, 48, 342, 141, 172, 147, 121, 29, 115, 83, 201, 69, 64, 74, 111, 53, 26, 163, 182, 179, 27, 213, 81, 191, 74, 152, 142, 64, 169, 454, 73, 63, 242, 49, 175, 149, 57, 231, 39, 216, 67, 43, 89, 91, 39, 59, 133, 191, 80, 408, 30, 187, 55, 144, 22, 224, 248, 119, 285, 71, 240, 107, 100, 241, 93, 53, 29, 45, 276, 48, 35, 230, 149, 220, 163, 56, 104, 96, 140, 275, 33, 51, 25, 204, 88, 95, 98, 158, 63, 165, 626, 194, 169, 51, 78, 46, 43, 198, 29, 133, 162, 81, 170, 61, 157, 65, 366, 50, 153, 65, 95, 60, 218, 49, 70, 140, 288, 44, 109, 178, 121, 30, 106, 88, 188, 80, 63, 45, 81, 31, 42, 215, 110, 95, 181, 291, 117, 58, 95, 21, 243, 78, 57, 140, 73, 24, 137, 131, 133, 169, 416, 62, 269, 118, 65, 54, 197, 289, 71, 74, 105, 58, 62, 24, 57, 127, 84, 38, 314, 403, 30, 74, 216, 148, 118, 186, 30, 71, 293, 103, 318, 185, 109, 44, 214, 55, 33, 196, 523, 81, 48, 66, 64, 94, 325, 93, 90, 66, 168, 332, 227, 592, 302, 57, 213, 313, 41, 251, 34, 114, 20, 182, 80, 80, 187, 320, 67, 537, 204, 63, 44, 90, 152, 121, 59, 262, 266, 105, 369, 264, 149, 93, 62, 31, 369, 130, 156, 56, 227, 34, 46, 69, 204, 170, 180, 259, 282, 50, 374, 186, 117, 253, 59, 400, 95, 170, 51, 152, 31, 177, 161, 52, 180, 50, 37, 64, 332, 231, 202, 92, 197, 319, 113, 36, 185, 76, 148, 33, 421, 238, 139, 87, 67, 216, 78, 129, 44, 229, 86, 50, 226, 41, 80, 329, 109, 96, 89, 186, 65, 205, 139, 85, 106, 104, 260, 112, 59, 143, 65, 123, 309, 112, 37, 151, 23, 121, 121, 22, 281, 85, 102, 52, 92, 345, 581, 65, 83, 33, 49, 114, 51, 177, 317, 57, 112, 21, 401, 24, 173, 20, 165, 116, 201, 95, 86, 270, 221, 198, 109, 147, 503, 58, 156, 116, 729, 183, 507, 120, 176, 106, 101, 26, 36, 107, 234, 115, 43, 74, 74, 92, 130, 127, 67, 133, 183, 152, 120, 41, 83, 291, 192, 45, 200, 122, 94, 166, 105, 71, 192, 136, 55, 93, 183, 22, 61, 75, 138, 84, 48, 384, 142, 42, 145, 20, 115, 82, 128, 270, 125, 74, 42, 168, 78, 207, 29, 62, 170, 151, 57, 26, 55, 43, 160, 107, 46, 54, 51, 28, 261, 135, 368, 27, 64, 71, 148, 73, 173, 109, 235, 62, 311, 54, 83, 64, 81, 24, 118, 116, 58, 311, 97, 101, 341, 98, 288, 207, 115, 63, 344, 31, 56, 52, 105, 80, 40, 186, 56, 196, 141, 75, 57, 254, 190, 236, 142, 140, 201, 177, 33, 222, 55, 82, 230, 143, 230, 220, 346, 28, 30, 39, 121, 112, 145, 220, 14, 125, 57, 72, 41, 240, 183, 203, 54, 44, 87, 126, 41, 160, 168, 27, 391, 66, 125, 39, 84, 170, 30, 111, 43, 93, 138, 139, 111, 115, 61, 293, 202, 222, 174, 255, 337, 104, 82, 81, 45, 166, 61, 129, 102, 111, 40, 179, 77, 118, 149, 198, 139, 591, 66, 297, 28, 42, 173, 145, 145, 81, 98, 69, 245, 226, 80, 285, 83, 78, 70, 63, 195, 123, 89, 216, 74, 374, 89, 80, 89, 144, 66, 91, 503, 223, 172, 48, 92, 135, 137, 100, 262, 114, 134, 62, 152, 150, 36, 46, 71, 51, 112, 147, 53, 47, 36, 73, 99, 22, 134, 95, 30, 52, 53, 247, 163, 248, 189, 44, 347, 86, 42, 244, 252, 127, 42, 82, 268, 29, 75, 115, 65, 76, 97, 46, 61, 31, 20, 42, 38, 159, 235, 270, 185, 353, 299, 56, 42, 272, 88, 76, 54, 281, 66, 112, 43, 30, 184, 27, 89, 45, 94, 499, 130, 340, 110, 241, 235, 170, 196, 47, 54, 156, 40, 157, 52, 99, 137, 89, 101, 244, 203, 417, 291, 479, 36, 109, 67, 172, 28, 220, 38, 310, 19, 43, 140, 139, 144, 34, 68, 32, 138, 132, 191, 87, 304, 154, 296, 295, 67, 72, 143, 113, 108, 171, 55, 352, 60, 85, 72, 75, 78, 283, 140, 140, 66, 138, 196, 122, 61, 181, 139, 206, 28, 202, 93, 257, 64, 127, 31, 107, 47, 40, 175, 31, 133, 35, 403, 86, 88, 91, 201, 28, 113, 45, 104, 112, 37, 162, 93, 131, 163, 139, 277, 293, 57, 124, 169, 23, 204, 175, 119, 58, 126, 70, 222, 107, 212, 140, 425, 165, 74, 79, 40, 208, 263, 50, 36, 115, 127, 84, 256, 258, 172, 61, 151, 33, 113, 124, 131, 64, 112, 189, 160, 269, 90, 172, 33, 116, 120, 51, 52, 176, 56, 153, 20, 33, 84, 33, 66, 205, 400, 114, 47, 73, 70, 329, 20, 118, 204, 103, 175, 114, 136, 93, 257, 83, 43, 192, 25, 57, 41, 256, 137, 33, 41, 59, 57, 48, 57, 836, 177, 77, 22, 183, 123, 54, 99, 57, 135, 50, 28, 157, 52, 205, 139, 131, 55, 281, 422, 88, 65, 167, 95, 73, 105, 147, 39, 52, 55, 54, 51, 181, 164, 505, 168, 32, 231, 79, 27, 439, 56, 58, 226, 170, 116, 284, 103, 57, 227, 37, 193, 32, 90, 37, 110, 275, 112, 150, 25, 261, 190, 28, 273, 51, 47, 118, 127, 48, 34, 74, 68, 75, 54, 155, 126, 471, 31, 360, 177, 73, 117, 75, 101, 102, 101, 125, 215, 34, 89, 111, 28, 99, 66, 82, 40, 224, 192, 132, 70, 83, 74, 135, 59, 72, 24, 204, 211, 89, 92, 166, 278, 312, 42, 60, 88, 171, 53, 66, 70, 45, 48, 122, 142, 307, 100, 128, 71, 76, 101, 73, 134, 122, 41, 82, 30, 38, 256, 181, 94, 140, 46, 46, 56, 73, 59, 226, 100, 161, 363, 128, 57, 356, 247, 26, 213, 504, 339, 327, 141, 69, 196, 279, 100, 116, 163, 55, 223, 207, 113, 61, 101, 97, 626, 233, 27, 36, 53, 139, 56, 152, 145, 185, 60, 108, 197, 92, 24, 64, 84, 303, 162, 169, 180, 89, 137, 173, 204, 139, 279, 96, 147, 238, 81, 103, 127, 48, 110, 479, 127, 40, 83, 214, 16, 73, 57, 110, 74, 26, 53, 151, 136, 37, 163, 94, 152, 277, 316, 31, 66, 62, 50, 143, 124, 331, 395, 581, 156, 203, 38, 197, 70, 121, 141, 33, 48, 127, 15, 360, 165, 153, 155, 162, 52, 203, 76, 45, 58, 102, 229, 33, 199, 35, 240, 108, 112, 128, 32, 419, 49, 333, 67, 57, 307, 136, 175, 244, 54, 51, 162, 144, 57, 86, 356, 412, 42, 254, 103, 163, 84, 194, 26, 147, 32, 171, 110, 34, 827, 53, 321, 104, 21, 230, 101, 130, 29, 122, 115, 32, 61, 18, 142, 71, 78, 181, 207, 99, 81, 180, 66, 174, 107, 112, 64, 163, 53, 73, 24, 23, 60, 176, 376, 51, 336, 112, 186, 368, 47, 196, 47, 105, 41, 129, 37, 180, 47, 111, 25, 322, 172, 121, 45, 88, 22, 51, 183, 52, 74, 107, 274, 273, 105, 68, 31, 142, 152, 34, 164, 93, 49, 203, 30, 307, 48, 43, 86, 92, 115, 58, 101, 394, 16, 178, 89, 86, 127, 232, 84, 30, 201, 58, 53, 92, 199, 188, 167, 45, 33, 222, 193, 98, 61, 45, 111, 70, 72, 156, 209, 75, 112, 60, 61, 67, 78, 110, 237, 52, 198, 85, 217, 28, 118, 200, 59, 68, 99, 26, 304, 184, 37, 170, 79, 127, 117, 56, 27, 45, 56, 87, 61, 87, 117, 181, 216, 57, 23, 80, 255, 85, 22, 299, 295, 37, 107, 36, 84, 244, 96, 253, 90, 63, 176, 227, 192, 259, 251, 264, 38, 86, 39, 152, 67, 78, 129, 74, 127, 64, 54, 51, 227, 65, 138, 165, 320, 60, 261, 113, 141, 241, 114, 418, 38, 69, 97, 299, 142, 109, 99, 77, 117, 116, 45, 88, 95, 51, 108, 159, 75, 260, 55, 71, 44, 68, 92, 213, 207, 94, 272, 100, 113, 214, 107, 167, 29, 42, 200, 101, 151, 141, 114, 303, 48, 53, 341, 121, 260, 35, 262, 47, 309, 196, 163, 50, 58, 45, 276, 257, 135, 127, 44, 94, 346, 44, 30, 75, 111, 233, 59, 20, 120, 48, 95, 41, 218, 90, 61, 333, 306, 53, 137, 34, 143, 45, 63, 199, 235, 56, 29, 266, 251, 380, 90, 61, 290, 39, 31, 177, 91, 83, 314, 103, 61, 24, 72, 72, 95, 49, 63, 213, 124, 35, 30, 61, 214, 163, 48, 71, 181, 272, 315, 106, 31, 206, 39, 94, 248, 245, 44, 76, 139, 56, 46, 32, 49, 78, 248, 113, 257, 207, 108, 45, 221, 120, 119, 79, 174, 68, 125, 64, 65, 77, 165, 70, 272, 224, 247, 54, 21, 130, 79, 23, 250, 319, 124, 187, 22, 169, 75, 135, 24, 135, 194, 160, 93, 131, 235, 92, 225, 286, 198, 478, 82, 226, 70, 66, 43, 31, 81, 66, 68, 37, 226, 652, 32, 102, 24, 158, 208, 43, 46, 127, 262, 320, 122, 45, 161, 329, 130, 18, 479, 67, 145, 176, 50, 38, 378, 273, 45, 268, 607, 65, 102, 194, 147, 188, 57, 52, 143, 77, 351, 39, 100, 210, 50, 35, 52, 120, 186, 84, 42, 58, 47, 68, 53, 526, 144, 71, 75, 128, 142, 78, 39, 86, 24, 196, 75, 86, 24, 88, 323, 77, 58, 301, 28, 257, 148, 156, 50, 68, 39, 67, 59, 36, 22, 81, 329, 40, 115, 119, 89, 93, 225, 178, 158, 106, 66, 77, 139, 36, 87, 300, 170, 209, 153, 78, 98, 46, 71, 31, 42, 400, 123, 222, 24, 44, 214, 137, 57, 96, 96, 44, 296, 67, 65, 266, 376, 155, 56, 176, 32, 128, 26, 86, 199, 37, 104, 69, 307, 314, 74, 200, 173, 246, 36, 96, 109, 18, 179, 162, 111, 143, 126, 190, 23, 69, 87, 63, 291, 646, 384, 205, 200, 180, 32, 72, 53, 29, 178, 46, 322, 155, 42, 137, 32, 76, 64, 72, 267, 276, 233, 252, 199, 181, 124, 46, 29, 107, 23, 101, 106, 61, 508, 220, 295, 171, 272, 143, 325, 164, 102, 406, 51, 49, 208, 34, 52, 126, 118, 213, 32, 71, 206, 118, 130, 111, 57, 46, 57, 486, 94, 63, 109, 57, 21, 40, 207, 252, 179, 74, 66, 94, 265, 153, 25, 140, 72, 164, 58, 174, 52, 23, 331, 268, 99, 32, 54, 98, 159, 89, 172, 243, 140, 152, 51, 53, 135, 102, 470, 55, 32, 93, 130, 79, 68, 172, 99, 200, 151, 38, 90, 179, 52, 202, 170, 60, 114, 87, 88, 66, 139, 509, 30, 40, 112, 461, 159, 153, 185, 25, 178, 296, 208, 38, 170, 39, 146, 15, 158, 54, 224, 26, 85, 196, 329, 224, 126, 57, 59, 99, 264, 52, 72, 84, 155, 239, 109, 90, 19, 41, 152, 109, 142, 299, 138, 381, 85, 83, 130, 34, 36, 233, 211, 72, 41, 40, 229, 324, 84, 149, 117, 307, 38, 86, 73, 140, 50, 219, 78, 141, 92, 20, 136, 115, 34, 192, 25, 84, 82, 49, 30, 52, 46, 67, 47, 25, 33, 149, 203, 423, 112, 98, 255, 312, 229, 139, 113, 258, 50, 23, 198, 73, 76, 77, 69, 137, 19, 78, 69, 54, 150, 21, 146, 172, 254, 108, 64, 149, 71, 56, 213, 88, 187, 286, 110, 107, 54, 267, 69, 88, 363, 268, 30, 39, 77, 202, 193, 189, 90, 164, 257, 82, 276, 299, 111, 187, 74, 138, 64, 39, 46, 47, 40, 104, 37, 168, 44, 75, 223, 56, 265, 405, 40, 133, 39, 72, 47, 31, 126, 403, 37, 76, 66, 76, 116, 215, 57, 108, 160, 109, 77, 93, 98, 53, 56, 257, 63, 71, 426, 37, 24, 165, 46, 33, 344, 77, 271, 122, 251, 147, 189, 31, 72, 90, 159, 29, 262, 119, 45, 143, 47, 43, 33, 155, 78, 407, 178, 26, 62, 86, 245, 63, 190, 41, 367, 39, 88, 66, 36, 198, 79, 246, 105, 137, 43, 126, 172, 72, 111, 27, 45, 28, 63, 83, 55, 173, 27, 327, 36, 238, 118, 80, 198, 104, 147, 135, 87, 89, 188, 23, 132, 34, 336, 195, 49, 203, 95, 171, 44, 104, 247, 132, 331, 71, 128, 98, 72, 78, 102, 186, 54, 41, 288, 125, 166, 133, 69, 207, 196, 46, 91, 50, 227, 28, 89, 278, 180, 30, 184, 36, 58, 86, 153, 85, 125, 70, 39, 24, 413, 236, 172, 63, 300, 75, 64, 88, 51, 175, 204, 88, 314, 70, 241, 72, 127, 254, 99, 67, 128, 354, 106, 27, 166, 50, 87, 20, 251, 80, 117, 152, 156, 227, 93, 144, 43, 174, 285, 75, 126, 49, 140, 16, 20, 409, 116, 49, 194, 116, 56, 97, 96, 43, 123, 51, 23, 27, 501, 23, 61, 68, 153, 206, 102, 83, 68, 105, 90, 43, 40, 23, 58, 125, 129, 200, 35, 231, 121, 56, 80, 172, 67, 129, 57, 130, 132, 251, 152, 173, 172, 147, 62, 43, 132, 160, 49, 258, 105, 434, 120, 28, 41, 101, 181, 180, 40, 44, 245, 22, 168, 21, 217, 48, 159, 157, 100, 61, 195, 156, 50, 86, 35, 67, 594, 40, 186, 62, 53, 486, 184, 126, 65, 192, 30, 61, 137, 168, 149, 190, 127, 60, 315, 112, 342, 113, 122, 313, 142, 66, 204, 261, 66, 217, 94, 101, 65, 365, 45, 239, 534, 19, 56, 376, 38, 107, 218, 128, 350, 40, 31, 136, 137, 107, 31, 125, 48, 109, 67, 131, 211, 138, 36, 86, 109, 78, 44, 309, 51, 117, 252, 42, 274, 143, 37, 221, 63, 30, 217, 200, 103, 211, 59, 155, 74, 146, 69, 208, 256, 143, 199, 158, 69, 64, 77, 85, 78, 65, 32, 29, 94, 188, 115, 42, 157, 70, 195, 121, 413, 120, 284, 141, 49, 37, 348, 153, 232, 158, 310, 51, 102, 61, 191, 297, 40, 170, 28, 110, 106, 83, 168, 208, 209, 60, 227, 33, 30, 46, 148, 105, 224, 133, 132, 96, 189, 60, 93, 68, 165, 195, 63, 305, 150, 446, 128, 32, 64, 205, 344, 31, 206, 137, 87, 33, 351, 33, 83, 185, 60, 174, 116, 160, 113, 232, 167, 199, 31, 96, 137, 87, 211, 26, 46, 140, 195, 251, 68, 67, 67, 510, 213, 22, 229, 114, 106, 90, 155, 69, 136, 305, 90, 69, 24, 114, 70, 189, 68, 185, 218, 87, 277, 27, 73, 119, 228, 247, 73, 104, 67, 236, 125, 93, 72, 289, 166, 303, 237, 190, 40, 255, 14, 91, 32, 88, 101, 28, 380, 214, 135, 156, 153, 164, 189, 54, 97, 154, 25, 284, 98, 52, 17, 117, 261, 79, 66, 222, 162, 57, 69, 83, 67, 233, 33, 351, 82, 29, 189, 73, 51, 100, 124, 52, 198, 82, 285, 34, 85, 165, 98, 193, 280, 319, 177, 129, 278, 80, 24, 60, 23, 142, 50, 260, 126, 189, 52, 88, 63, 62, 52, 49, 43, 54, 53, 159, 110, 180, 163, 70, 224, 184, 49, 310, 106, 81, 187, 186, 95, 170, 72, 43, 179, 225, 55, 244, 112, 113, 168, 206, 191, 174, 210, 212, 46, 96, 135, 29, 302, 156, 153, 49, 139, 170, 177, 132, 236, 88, 58, 99, 408, 157, 86, 135, 136, 128, 115, 108, 218, 261, 39, 60, 53, 291, 190, 503, 125, 50, 72, 199, 101, 103, 59, 37, 56, 175, 124, 58, 136, 63, 138, 49, 160, 166, 192, 51, 236, 68, 207, 99, 99, 69, 68, 141, 92, 283, 133, 38, 114, 59, 22, 24, 98, 235, 221, 147, 65, 349, 40, 94, 36, 51, 38, 295, 24, 59, 60, 100, 20, 56, 222, 263, 96, 32, 44, 274, 157, 199, 50, 164, 50, 126, 52, 64, 157, 90, 63, 55, 35, 70, 57, 215, 164, 44, 464, 34, 49, 260, 82, 65, 128, 123, 90, 569, 123, 133, 298, 105, 194, 253, 42, 104, 34, 135, 198, 59, 182, 33, 134, 202, 18, 153, 46, 348, 89, 93, 101, 237, 187, 47, 79, 49, 66, 100, 258, 169, 172, 232, 132, 163, 243, 146, 72, 62, 40, 642, 249, 42, 260, 39, 56, 125, 36, 30, 178, 54, 97, 388, 119, 88, 100, 210, 86, 91, 30, 103, 140, 419, 73, 32, 63, 150, 23, 316, 60, 35, 220, 266, 40, 304, 31, 264, 64, 243, 571, 104, 76, 128, 71, 20, 177, 64, 36, 57, 100, 71, 366, 36, 148, 48, 78, 78, 48, 46, 86, 168, 417, 16, 174, 18, 43, 86, 24, 51, 72, 283, 122, 246, 75, 55, 36, 204, 75, 211, 155, 52, 97, 255, 140, 29, 65, 81, 24, 27, 280, 380, 141, 107, 17, 311, 221, 122, 152, 141, 141, 44, 51, 246, 73, 105, 20, 263, 291, 135, 105, 35, 194, 74, 132, 81, 255, 114, 679, 146, 31, 50, 61, 116, 97, 159, 35, 191, 57, 202, 100, 61, 70, 186, 66, 47, 156, 45, 287, 110, 137, 211, 284, 42, 186, 69, 120, 332, 138, 213, 24, 257, 111, 259, 221, 147, 584, 197, 121, 227, 87, 87, 86, 67, 181, 341, 30, 132, 411, 89, 62, 241, 53, 248, 110, 162, 68, 509, 85, 251, 88, 127, 157, 59, 240, 134, 94, 134, 60, 141, 62, 193, 101, 266, 141, 34, 91, 165, 147, 167, 105, 252, 50, 238, 85, 21, 49, 115, 144, 55, 39, 303, 202, 91, 343, 142, 24, 540, 41, 45, 333, 199, 219, 51, 43, 45, 23, 131, 167, 575, 134, 283, 22, 29, 51, 104, 136, 364, 169, 323, 358, 72, 54, 171, 63, 262, 86, 62, 52, 90, 134, 66, 53, 230, 153, 197, 59, 16, 98, 81, 113, 109, 119, 126, 90, 74, 39, 373, 62, 59, 31, 27, 98, 56, 144, 98, 210, 185, 195, 78, 60, 40, 181, 30, 256, 41, 43, 91, 211, 161, 82, 112, 36, 345, 360, 114, 68, 124, 40, 458, 246, 260, 390, 40, 63, 41, 22, 102, 52, 53, 35, 78, 346, 135, 207, 198, 184, 115, 302, 133, 96, 66, 112, 50, 84, 36, 158, 40, 37, 29, 96, 82, 38, 109, 110, 325, 30, 211, 53, 87, 211, 69, 73, 95, 206, 77, 278, 184, 378, 333]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total No. of Dialogue : {len(dialogue_tokens)}\")\n",
    "print(f\"Each Dialogue Tokens : {dialogue_token_lens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue 1: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "Tokenized Dialogue 1: [12195, 7091, 3659, 111, 138, 650, 10508, 181, 3469, 107, 1]\n",
      "\n",
      "Original Dialogue 2: Olivia and Olivier are voting for liberals in this election. \n",
      "Tokenized Dialogue 2: [18038, 111, 34296, 127, 6228, 118, 33195, 115, 136, 2974, 107, 1]\n",
      "\n",
      "Original Dialogue 3: Kim may try the pomodoro technique recommended by Tim to get more stuff done.\n",
      "Tokenized Dialogue 3: [5377, 218, 508, 109, 891, 93882, 3111, 2087, 141, 4776, 112, 179, 154, 1549, 479, 107, 1]\n",
      "\n",
      "Original Dialogue 4: Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. \n",
      "Tokenized Dialogue 4: [7535, 7234, 178, 117, 115, 298, 122, 14521, 107, 9199, 1728, 7535, 112, 428, 169, 1059, 107, 9199, 117, 833, 107, 1]\n",
      "\n",
      "Original Dialogue 5: Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.\n",
      "Tokenized Dialogue 5: [4037, 117, 6436, 108, 262, 178, 61237, 7888, 14545, 160, 342, 130, 114, 25603, 107, 26893, 7234, 4037, 246, 1002, 112, 7888, 107, 4037, 117, 146, 334, 180, 112, 171, 107, 1]\n",
      "\n",
      "Original Dialogue 6: Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\n",
      "Tokenized Dialogue 6: [33360, 7944, 36160, 169, 1275, 4382, 117, 124, 109, 1689, 307, 113, 1338, 107, 36160, 131, 116, 1750, 117, 7385, 111, 126, 382, 129, 262, 36160, 8301, 160, 153, 4382, 107, 1]\n",
      "\n",
      "Original Dialogue 7: John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \n",
      "Tokenized Dialogue 7: [1084, 595, 131, 144, 403, 164, 118, 755, 640, 112, 181, 201, 618, 122, 169, 5388, 107, 40924, 108, 169, 2118, 898, 342, 162, 4764, 112, 171, 108, 111, 162, 3697, 112, 692, 107, 322, 127, 313, 112, 670, 164, 118, 114, 3053, 9455, 136, 396, 244, 755, 107, 1]\n",
      "\n",
      "Original Dialogue 8: Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.\n",
      "Tokenized Dialogue 8: [5615, 9274, 2133, 142, 9053, 1649, 178, 382, 172, 107, 2133, 2368, 109, 1649, 107, 139, 2037, 8223, 109, 2195, 112, 109, 2956, 157, 195, 1421, 115, 111, 2053, 112, 653, 109, 1589, 3489, 107, 1]\n",
      "\n",
      "Original Dialogue 9: Noah wants to meet, he quit his job, because his boss was a dick.\n",
      "Tokenized Dialogue 9: [14780, 1728, 112, 670, 108, 178, 7209, 169, 494, 108, 262, 169, 5388, 140, 114, 110, 35402, 107, 1]\n",
      "\n",
      "Original Dialogue 10: Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\n",
      "Tokenized Dialogue 10: [4592, 9435, 35746, 118, 114, 796, 112, 179, 112, 235, 276, 176, 340, 107, 322, 131, 267, 275, 112, 109, 19254, 1705, 115, 74793, 6120, 58898, 124, 1327, 134, 530, 2874, 108, 111, 178, 131, 267, 1293, 215, 164, 124, 109, 230, 112, 109, 295, 107, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the summary in the training set\n",
    "summary_tokens = [tokenizer.encode(s) for s in dataset_samsum['train']['summary']]\n",
    "summary_token_lens = [len(tokenizer.encode(s)) for s in dataset_samsum['train']['summary']]\n",
    "\n",
    "# Print the first 10 original summaries and their tokenized versions\n",
    "for i in range(10):\n",
    "    original_summary = dataset_samsum['train']['summary'][i]\n",
    "    tokenized_summary = summary_tokens[i]\n",
    "    print(f\"Original Dialogue {i+1}: {original_summary}\")\n",
    "    print(f\"Tokenized Dialogue {i+1}: {tokenized_summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of summary : 14732\n",
      "Each summary Tokens : [11, 12, 17, 22, 32, 32, 50, 36, 19, 47, 19, 16, 7, 22, 23, 22, 41, 15, 21, 18, 19, 68, 16, 40, 20, 25, 21, 36, 45, 16, 69, 27, 9, 14, 25, 19, 26, 39, 27, 20, 63, 33, 22, 31, 17, 16, 23, 13, 7, 43, 10, 29, 19, 23, 36, 21, 44, 55, 31, 27, 33, 45, 18, 39, 17, 8, 26, 20, 30, 16, 14, 37, 12, 27, 54, 31, 44, 19, 11, 39, 59, 20, 8, 22, 21, 16, 12, 21, 41, 24, 13, 12, 7, 21, 23, 24, 48, 23, 25, 28, 41, 12, 22, 28, 38, 26, 16, 28, 15, 14, 24, 13, 28, 21, 33, 35, 19, 17, 40, 54, 28, 34, 10, 18, 26, 8, 11, 52, 23, 19, 16, 16, 23, 18, 21, 17, 10, 67, 11, 13, 23, 7, 25, 32, 7, 42, 9, 12, 15, 31, 40, 61, 19, 40, 27, 19, 57, 18, 45, 26, 28, 15, 12, 9, 14, 36, 21, 13, 23, 24, 27, 29, 11, 29, 20, 20, 21, 21, 21, 17, 23, 19, 16, 24, 26, 19, 12, 18, 26, 10, 25, 61, 34, 30, 27, 15, 13, 22, 59, 28, 28, 23, 13, 15, 9, 47, 30, 17, 23, 17, 17, 56, 18, 28, 41, 19, 19, 21, 30, 25, 37, 18, 46, 16, 26, 40, 55, 36, 18, 20, 13, 11, 8, 29, 10, 51, 38, 11, 37, 12, 21, 46, 40, 17, 38, 19, 26, 32, 21, 32, 31, 11, 15, 58, 29, 10, 44, 19, 42, 17, 26, 11, 20, 16, 19, 29, 18, 14, 15, 10, 31, 42, 18, 16, 9, 35, 36, 12, 41, 27, 13, 27, 33, 11, 15, 28, 11, 15, 22, 27, 8, 22, 24, 18, 18, 29, 10, 13, 40, 65, 17, 23, 11, 28, 31, 20, 51, 20, 19, 19, 22, 21, 35, 22, 13, 46, 55, 23, 22, 24, 24, 51, 17, 16, 21, 53, 40, 26, 52, 18, 5, 35, 34, 17, 32, 16, 11, 16, 21, 18, 15, 59, 16, 18, 27, 17, 18, 54, 51, 15, 33, 32, 39, 24, 13, 21, 20, 9, 41, 31, 11, 35, 12, 12, 36, 7, 11, 34, 16, 9, 32, 13, 21, 18, 59, 18, 13, 11, 38, 24, 30, 12, 9, 48, 21, 7, 12, 20, 31, 41, 21, 26, 42, 7, 14, 36, 21, 13, 37, 62, 18, 38, 40, 30, 15, 12, 14, 32, 9, 13, 22, 26, 21, 34, 34, 24, 33, 36, 22, 9, 11, 20, 13, 24, 24, 37, 69, 25, 33, 16, 19, 39, 38, 18, 12, 31, 16, 14, 24, 28, 25, 27, 33, 37, 42, 49, 11, 32, 11, 57, 40, 9, 21, 26, 12, 8, 23, 25, 55, 25, 20, 71, 8, 24, 20, 30, 30, 8, 43, 25, 34, 27, 13, 43, 28, 22, 12, 18, 9, 18, 43, 70, 37, 18, 36, 54, 13, 20, 10, 23, 40, 26, 18, 21, 18, 39, 61, 12, 21, 14, 22, 52, 24, 16, 26, 12, 10, 21, 38, 17, 32, 22, 55, 26, 15, 16, 26, 24, 28, 36, 23, 28, 16, 33, 13, 20, 55, 21, 31, 31, 13, 24, 38, 12, 23, 59, 27, 26, 57, 49, 31, 37, 37, 20, 62, 14, 28, 27, 19, 27, 7, 25, 23, 17, 35, 12, 13, 22, 12, 64, 18, 52, 32, 28, 36, 11, 17, 9, 24, 13, 17, 31, 30, 27, 12, 27, 34, 8, 23, 16, 11, 19, 36, 69, 11, 31, 26, 38, 18, 27, 11, 45, 9, 17, 16, 16, 21, 13, 71, 25, 28, 35, 31, 31, 20, 27, 18, 15, 23, 32, 42, 12, 16, 18, 18, 15, 9, 21, 38, 16, 30, 56, 10, 33, 27, 13, 13, 27, 33, 7, 24, 21, 24, 34, 11, 41, 11, 36, 18, 26, 22, 25, 22, 12, 26, 31, 51, 16, 16, 17, 24, 26, 17, 43, 32, 13, 32, 26, 17, 23, 53, 17, 7, 13, 46, 30, 22, 15, 11, 11, 10, 16, 20, 24, 19, 9, 20, 50, 47, 30, 21, 22, 28, 44, 19, 17, 39, 47, 21, 15, 26, 26, 19, 51, 25, 49, 65, 14, 34, 46, 14, 18, 13, 20, 20, 13, 19, 15, 46, 70, 19, 16, 10, 9, 39, 54, 36, 29, 9, 43, 27, 6, 41, 15, 13, 46, 16, 34, 13, 35, 17, 14, 20, 34, 18, 25, 13, 67, 24, 22, 23, 35, 31, 33, 35, 12, 14, 16, 41, 22, 16, 18, 12, 9, 10, 30, 25, 17, 23, 39, 7, 32, 42, 67, 16, 33, 22, 8, 23, 22, 34, 17, 12, 18, 15, 21, 13, 33, 15, 13, 61, 13, 14, 33, 47, 17, 44, 10, 15, 8, 74, 16, 14, 41, 40, 24, 38, 18, 36, 19, 54, 18, 24, 37, 29, 18, 16, 14, 38, 18, 32, 35, 38, 36, 40, 49, 22, 11, 6, 37, 13, 27, 11, 13, 25, 36, 16, 40, 22, 15, 19, 19, 18, 17, 39, 25, 21, 22, 20, 16, 24, 24, 33, 27, 12, 12, 34, 18, 38, 16, 42, 24, 52, 21, 18, 34, 20, 28, 19, 14, 10, 19, 29, 15, 13, 55, 23, 34, 21, 12, 26, 21, 11, 38, 19, 12, 63, 52, 32, 37, 18, 17, 9, 35, 26, 18, 47, 24, 40, 46, 8, 21, 28, 35, 41, 18, 42, 17, 53, 10, 68, 55, 23, 50, 22, 14, 33, 21, 8, 43, 13, 18, 22, 65, 39, 11, 27, 28, 32, 17, 18, 36, 19, 21, 16, 55, 28, 36, 24, 39, 31, 14, 15, 22, 20, 11, 17, 12, 35, 29, 12, 16, 16, 22, 39, 24, 18, 22, 43, 22, 36, 17, 18, 45, 26, 27, 22, 25, 62, 37, 31, 25, 9, 29, 6, 15, 37, 15, 12, 66, 17, 22, 33, 63, 17, 12, 16, 21, 16, 11, 10, 12, 26, 16, 40, 23, 22, 24, 13, 13, 32, 22, 20, 23, 45, 53, 10, 14, 21, 9, 14, 9, 70, 11, 9, 30, 14, 44, 14, 32, 20, 21, 20, 43, 12, 10, 22, 16, 24, 37, 19, 23, 35, 32, 52, 13, 55, 32, 32, 10, 21, 23, 20, 45, 21, 14, 26, 9, 16, 12, 38, 44, 30, 24, 37, 20, 17, 38, 51, 30, 17, 15, 18, 26, 36, 55, 7, 61, 48, 13, 23, 18, 19, 47, 42, 43, 18, 36, 19, 5, 28, 6, 24, 16, 8, 8, 32, 7, 21, 45, 39, 55, 31, 12, 25, 12, 10, 18, 15, 20, 33, 22, 34, 27, 13, 24, 11, 25, 31, 23, 8, 55, 35, 68, 12, 25, 59, 32, 20, 9, 16, 11, 26, 9, 27, 13, 33, 59, 22, 12, 18, 13, 29, 8, 25, 11, 60, 18, 11, 25, 16, 56, 29, 16, 20, 32, 18, 42, 54, 14, 27, 23, 17, 28, 72, 10, 18, 42, 50, 13, 22, 11, 12, 9, 36, 40, 53, 21, 44, 30, 14, 43, 22, 25, 34, 8, 7, 33, 20, 12, 27, 43, 26, 40, 48, 28, 8, 11, 43, 17, 11, 52, 17, 28, 24, 22, 27, 14, 12, 16, 41, 30, 23, 17, 31, 42, 9, 26, 26, 7, 14, 12, 22, 35, 53, 30, 37, 31, 13, 24, 6, 51, 19, 36, 40, 20, 12, 11, 10, 33, 20, 8, 31, 21, 17, 35, 56, 18, 32, 32, 44, 11, 40, 9, 19, 21, 14, 17, 12, 46, 24, 43, 11, 24, 20, 12, 10, 31, 23, 8, 13, 7, 39, 27, 25, 34, 18, 20, 14, 5, 20, 16, 49, 36, 31, 23, 30, 38, 12, 16, 13, 13, 30, 25, 27, 22, 15, 20, 8, 27, 30, 23, 40, 32, 50, 14, 10, 14, 18, 21, 23, 32, 22, 22, 23, 15, 24, 43, 18, 33, 48, 12, 36, 7, 11, 58, 41, 24, 10, 20, 42, 33, 40, 17, 13, 34, 41, 43, 41, 49, 11, 66, 31, 33, 13, 20, 33, 28, 37, 56, 8, 15, 59, 23, 9, 54, 39, 23, 15, 23, 51, 30, 31, 43, 38, 15, 19, 12, 13, 23, 32, 33, 20, 26, 33, 38, 47, 10, 12, 15, 21, 16, 22, 39, 23, 18, 57, 47, 25, 12, 27, 18, 10, 12, 20, 13, 19, 10, 26, 57, 13, 14, 31, 37, 31, 14, 43, 24, 19, 32, 43, 18, 20, 53, 19, 17, 36, 33, 26, 16, 50, 11, 55, 16, 28, 24, 12, 23, 36, 30, 23, 32, 31, 10, 7, 27, 7, 15, 20, 35, 21, 36, 30, 33, 37, 42, 23, 10, 30, 9, 32, 12, 7, 17, 12, 23, 33, 49, 22, 34, 18, 30, 8, 9, 61, 16, 11, 19, 30, 11, 44, 40, 15, 32, 41, 59, 58, 11, 21, 36, 27, 37, 16, 26, 18, 20, 26, 13, 35, 19, 18, 31, 41, 14, 38, 21, 33, 30, 44, 17, 9, 18, 40, 20, 9, 11, 46, 41, 33, 53, 42, 33, 6, 41, 30, 21, 16, 17, 29, 29, 38, 31, 34, 27, 29, 31, 63, 23, 32, 9, 18, 21, 14, 34, 23, 13, 52, 33, 13, 27, 10, 38, 19, 9, 19, 45, 26, 30, 24, 11, 26, 13, 12, 14, 39, 12, 21, 35, 29, 36, 8, 18, 17, 32, 58, 40, 18, 14, 12, 28, 17, 11, 20, 10, 12, 46, 9, 29, 14, 26, 45, 7, 48, 38, 37, 25, 14, 31, 28, 23, 9, 20, 14, 24, 38, 12, 22, 47, 39, 21, 22, 11, 17, 7, 33, 12, 19, 32, 31, 40, 19, 28, 20, 54, 40, 29, 12, 41, 20, 27, 42, 14, 29, 29, 18, 20, 8, 24, 39, 12, 18, 21, 24, 45, 47, 61, 17, 23, 37, 25, 23, 76, 29, 22, 48, 38, 16, 32, 39, 8, 22, 27, 17, 43, 25, 26, 16, 13, 57, 6, 8, 6, 15, 7, 19, 20, 26, 30, 11, 7, 34, 17, 17, 34, 25, 16, 25, 46, 12, 25, 11, 17, 48, 13, 26, 54, 35, 43, 16, 26, 19, 27, 27, 25, 41, 13, 12, 16, 10, 10, 18, 48, 17, 38, 17, 8, 24, 50, 23, 48, 10, 27, 27, 15, 19, 29, 18, 13, 38, 22, 62, 33, 15, 17, 32, 38, 18, 9, 15, 65, 32, 15, 18, 9, 17, 15, 34, 54, 22, 21, 11, 49, 35, 31, 54, 17, 10, 24, 43, 6, 25, 48, 26, 19, 11, 18, 22, 23, 57, 13, 41, 25, 36, 39, 24, 26, 67, 15, 28, 56, 15, 45, 29, 18, 50, 12, 28, 18, 48, 17, 2, 25, 8, 13, 36, 31, 18, 14, 31, 44, 22, 13, 20, 17, 21, 17, 30, 52, 40, 16, 52, 57, 33, 26, 14, 27, 17, 7, 8, 19, 15, 15, 20, 17, 15, 28, 23, 33, 35, 36, 33, 24, 25, 39, 15, 20, 39, 39, 14, 16, 15, 22, 38, 15, 27, 13, 24, 25, 49, 15, 22, 22, 14, 34, 16, 43, 15, 12, 49, 32, 16, 18, 39, 12, 10, 18, 32, 6, 56, 45, 27, 42, 17, 23, 48, 27, 21, 18, 19, 11, 60, 18, 11, 11, 52, 19, 10, 30, 49, 25, 20, 16, 41, 23, 41, 61, 58, 12, 20, 10, 36, 26, 18, 34, 13, 9, 22, 25, 24, 12, 12, 9, 37, 27, 12, 41, 14, 18, 28, 39, 20, 7, 14, 26, 25, 17, 39, 28, 37, 23, 35, 13, 50, 49, 8, 34, 10, 35, 15, 33, 10, 6, 21, 9, 9, 22, 50, 18, 12, 36, 12, 24, 38, 59, 23, 53, 41, 28, 10, 12, 26, 28, 24, 11, 9, 53, 5, 18, 31, 9, 16, 44, 17, 65, 9, 44, 36, 38, 13, 31, 31, 19, 45, 57, 25, 22, 36, 53, 22, 8, 24, 22, 27, 11, 24, 34, 15, 55, 37, 15, 44, 14, 66, 60, 23, 25, 27, 16, 62, 25, 57, 31, 37, 36, 23, 25, 10, 17, 18, 15, 20, 45, 63, 36, 16, 17, 42, 38, 29, 12, 25, 30, 21, 49, 23, 24, 10, 28, 33, 50, 22, 10, 41, 15, 17, 45, 23, 11, 31, 33, 32, 24, 16, 24, 22, 50, 21, 12, 40, 21, 16, 39, 10, 13, 14, 9, 14, 29, 15, 25, 18, 26, 20, 34, 24, 65, 14, 44, 34, 17, 26, 46, 27, 15, 31, 10, 9, 20, 27, 15, 14, 16, 14, 18, 21, 33, 11, 38, 64, 9, 17, 15, 17, 16, 19, 45, 44, 29, 49, 30, 52, 26, 18, 17, 46, 53, 12, 15, 15, 15, 12, 20, 9, 26, 22, 37, 23, 32, 63, 52, 14, 27, 20, 28, 19, 24, 47, 21, 35, 13, 36, 20, 52, 25, 8, 12, 18, 19, 18, 44, 13, 14, 27, 44, 50, 15, 12, 14, 26, 46, 43, 26, 15, 19, 17, 50, 24, 22, 35, 19, 21, 15, 15, 53, 21, 24, 30, 20, 15, 57, 38, 25, 37, 12, 39, 35, 47, 22, 44, 14, 22, 12, 30, 20, 21, 29, 16, 8, 24, 10, 41, 24, 19, 13, 31, 21, 53, 27, 8, 21, 7, 13, 10, 15, 29, 22, 69, 20, 30, 12, 25, 11, 8, 18, 22, 17, 41, 13, 58, 18, 22, 43, 41, 43, 29, 20, 12, 13, 26, 26, 22, 13, 23, 11, 32, 6, 17, 27, 50, 13, 23, 45, 19, 10, 17, 40, 28, 21, 41, 9, 22, 27, 19, 24, 20, 46, 53, 20, 13, 35, 29, 18, 11, 47, 38, 50, 54, 36, 13, 38, 14, 41, 14, 28, 20, 14, 18, 22, 18, 37, 19, 13, 10, 7, 18, 32, 30, 12, 17, 31, 10, 33, 14, 33, 22, 43, 21, 32, 47, 37, 10, 29, 33, 11, 8, 10, 10, 16, 18, 18, 65, 12, 25, 47, 17, 19, 14, 12, 17, 22, 26, 52, 8, 14, 33, 40, 26, 40, 25, 48, 48, 28, 29, 49, 21, 27, 23, 12, 34, 13, 28, 68, 17, 20, 29, 21, 46, 12, 27, 19, 7, 18, 26, 9, 22, 9, 31, 31, 15, 64, 14, 33, 20, 11, 8, 53, 32, 13, 22, 13, 28, 26, 26, 31, 48, 50, 22, 29, 58, 13, 39, 28, 36, 12, 23, 28, 15, 20, 29, 32, 31, 38, 27, 12, 18, 13, 10, 18, 24, 20, 35, 17, 22, 20, 27, 11, 34, 10, 19, 24, 16, 20, 18, 32, 45, 8, 28, 26, 18, 29, 16, 28, 33, 34, 42, 28, 8, 28, 29, 21, 39, 23, 13, 28, 19, 19, 23, 28, 13, 20, 28, 18, 25, 11, 20, 33, 30, 23, 21, 20, 51, 9, 11, 18, 47, 14, 13, 6, 32, 37, 16, 11, 15, 27, 21, 20, 32, 10, 27, 25, 43, 40, 43, 12, 39, 10, 24, 49, 26, 13, 7, 40, 17, 19, 24, 9, 28, 18, 10, 10, 10, 25, 25, 12, 21, 19, 23, 30, 14, 17, 30, 17, 53, 18, 26, 33, 14, 10, 13, 24, 29, 26, 17, 20, 13, 31, 10, 37, 16, 17, 16, 32, 14, 31, 18, 34, 41, 21, 30, 57, 27, 38, 36, 13, 16, 20, 27, 20, 25, 15, 21, 54, 35, 18, 18, 17, 6, 13, 12, 20, 9, 31, 18, 9, 15, 55, 27, 27, 11, 9, 24, 18, 19, 45, 48, 32, 16, 18, 41, 9, 13, 26, 16, 31, 41, 26, 60, 38, 18, 31, 17, 12, 16, 11, 11, 13, 9, 28, 35, 21, 18, 17, 15, 19, 32, 25, 18, 23, 9, 40, 29, 15, 8, 29, 11, 40, 11, 12, 30, 36, 17, 31, 12, 41, 16, 30, 45, 32, 33, 25, 32, 32, 32, 46, 26, 21, 25, 59, 15, 18, 53, 54, 20, 35, 21, 18, 11, 36, 56, 24, 27, 30, 26, 21, 25, 10, 56, 34, 12, 11, 17, 46, 9, 30, 22, 22, 30, 29, 29, 43, 17, 30, 14, 19, 24, 24, 27, 23, 6, 18, 15, 22, 17, 26, 34, 9, 65, 28, 30, 11, 18, 21, 16, 36, 32, 16, 11, 32, 36, 15, 17, 44, 58, 21, 27, 14, 25, 27, 30, 29, 26, 44, 54, 39, 18, 17, 55, 17, 23, 31, 7, 31, 21, 38, 48, 18, 31, 14, 18, 22, 20, 19, 42, 27, 17, 32, 30, 57, 8, 16, 39, 20, 32, 13, 13, 22, 13, 17, 20, 12, 16, 32, 22, 19, 34, 18, 12, 23, 47, 25, 41, 11, 32, 30, 28, 19, 38, 28, 9, 33, 39, 19, 29, 11, 16, 28, 25, 14, 40, 24, 26, 21, 17, 42, 25, 20, 15, 25, 6, 38, 28, 35, 22, 12, 12, 23, 11, 50, 21, 28, 21, 24, 20, 28, 46, 10, 20, 56, 16, 38, 18, 42, 25, 25, 16, 36, 24, 47, 14, 29, 30, 47, 59, 40, 11, 49, 19, 23, 63, 16, 50, 19, 23, 25, 14, 29, 14, 28, 28, 33, 15, 15, 53, 25, 5, 29, 18, 14, 30, 21, 11, 24, 62, 25, 63, 16, 22, 7, 35, 18, 30, 47, 20, 40, 15, 32, 16, 20, 20, 22, 16, 18, 16, 20, 16, 29, 17, 15, 32, 41, 40, 40, 18, 46, 14, 12, 19, 26, 80, 19, 26, 27, 18, 14, 33, 20, 17, 36, 20, 9, 22, 37, 16, 30, 14, 27, 37, 34, 29, 10, 26, 16, 9, 13, 11, 17, 36, 42, 37, 17, 6, 30, 26, 17, 46, 25, 24, 10, 12, 15, 20, 56, 66, 34, 20, 17, 36, 13, 19, 29, 15, 32, 16, 14, 13, 16, 18, 8, 15, 6, 15, 14, 46, 42, 32, 37, 49, 34, 17, 9, 20, 9, 76, 14, 42, 40, 11, 26, 8, 17, 14, 17, 38, 20, 37, 43, 35, 34, 18, 12, 13, 37, 17, 15, 6, 11, 15, 31, 40, 34, 65, 18, 34, 15, 60, 24, 33, 17, 53, 11, 63, 26, 16, 16, 28, 38, 13, 60, 34, 32, 21, 18, 37, 36, 51, 22, 17, 40, 29, 23, 18, 22, 29, 27, 41, 9, 10, 39, 9, 13, 13, 11, 26, 64, 68, 13, 46, 36, 31, 23, 47, 22, 40, 26, 8, 39, 8, 33, 27, 25, 37, 26, 45, 28, 22, 25, 19, 19, 17, 32, 13, 21, 34, 26, 47, 7, 16, 17, 42, 34, 20, 20, 36, 12, 11, 14, 17, 12, 20, 13, 24, 21, 16, 28, 11, 17, 10, 47, 12, 21, 31, 20, 25, 49, 40, 44, 7, 10, 20, 21, 11, 24, 15, 69, 15, 8, 60, 26, 26, 37, 12, 39, 18, 19, 8, 16, 52, 16, 38, 17, 38, 20, 37, 53, 28, 26, 32, 41, 20, 60, 44, 65, 52, 19, 18, 32, 24, 42, 56, 29, 10, 13, 12, 27, 21, 26, 39, 23, 11, 39, 54, 49, 5, 21, 14, 11, 30, 22, 30, 28, 51, 21, 21, 10, 20, 25, 22, 52, 37, 19, 14, 19, 32, 11, 26, 26, 27, 20, 20, 29, 31, 13, 8, 17, 16, 68, 15, 18, 18, 45, 66, 23, 9, 16, 13, 48, 43, 28, 51, 18, 19, 30, 25, 35, 22, 9, 38, 60, 13, 32, 22, 33, 29, 21, 32, 34, 31, 16, 10, 16, 14, 26, 34, 15, 26, 13, 22, 13, 62, 26, 9, 13, 19, 12, 33, 21, 41, 13, 19, 28, 47, 47, 25, 40, 22, 9, 12, 19, 39, 40, 38, 51, 23, 34, 70, 7, 16, 13, 35, 23, 13, 10, 9, 20, 14, 8, 8, 14, 5, 33, 34, 9, 17, 24, 30, 22, 37, 15, 22, 54, 50, 8, 23, 29, 21, 20, 14, 17, 20, 54, 54, 31, 15, 36, 26, 34, 14, 27, 54, 23, 12, 24, 31, 18, 53, 25, 11, 13, 32, 30, 29, 11, 26, 14, 31, 27, 44, 14, 31, 65, 30, 42, 9, 30, 29, 26, 6, 30, 59, 27, 40, 11, 13, 52, 15, 13, 22, 26, 25, 12, 36, 39, 31, 24, 28, 25, 44, 15, 16, 25, 17, 43, 19, 16, 13, 13, 27, 39, 21, 24, 17, 39, 10, 18, 62, 31, 14, 35, 51, 30, 19, 19, 19, 13, 28, 42, 29, 20, 7, 39, 25, 23, 23, 34, 17, 23, 55, 48, 47, 33, 66, 28, 11, 13, 60, 23, 25, 18, 10, 15, 8, 9, 43, 41, 14, 43, 10, 43, 10, 16, 24, 18, 28, 31, 50, 22, 17, 65, 52, 36, 35, 22, 40, 63, 19, 9, 25, 48, 29, 57, 13, 35, 9, 34, 34, 36, 26, 18, 7, 49, 9, 28, 32, 18, 37, 21, 12, 13, 30, 18, 23, 19, 31, 20, 15, 36, 10, 26, 33, 24, 13, 25, 35, 33, 22, 16, 16, 24, 17, 33, 32, 44, 41, 18, 36, 11, 8, 12, 14, 42, 15, 23, 43, 15, 10, 20, 47, 29, 34, 29, 28, 21, 25, 28, 18, 52, 24, 22, 46, 38, 19, 19, 12, 70, 27, 25, 56, 19, 15, 9, 31, 14, 16, 14, 18, 16, 14, 9, 27, 14, 34, 32, 20, 10, 33, 33, 27, 20, 12, 29, 33, 18, 32, 28, 62, 20, 34, 19, 34, 17, 20, 19, 19, 42, 14, 21, 20, 11, 14, 61, 12, 29, 17, 27, 16, 15, 12, 15, 46, 21, 35, 27, 31, 18, 17, 39, 7, 31, 45, 16, 18, 21, 39, 21, 12, 50, 14, 26, 11, 24, 14, 14, 29, 16, 38, 19, 20, 14, 57, 6, 25, 17, 55, 15, 36, 15, 25, 66, 6, 9, 35, 8, 23, 11, 10, 13, 25, 49, 18, 26, 7, 56, 43, 12, 15, 8, 29, 11, 33, 15, 7, 38, 24, 11, 25, 15, 15, 27, 20, 12, 27, 20, 25, 7, 30, 14, 42, 46, 24, 19, 13, 35, 24, 12, 47, 16, 20, 26, 6, 30, 14, 64, 14, 15, 18, 14, 14, 16, 32, 12, 23, 28, 15, 49, 28, 22, 24, 35, 16, 27, 39, 11, 22, 18, 18, 11, 28, 13, 25, 22, 41, 13, 15, 22, 28, 22, 49, 38, 19, 24, 19, 16, 9, 29, 20, 22, 14, 32, 10, 31, 16, 33, 50, 32, 24, 38, 15, 12, 41, 15, 9, 26, 15, 46, 21, 21, 18, 34, 23, 64, 11, 14, 16, 23, 9, 33, 41, 23, 30, 20, 23, 27, 12, 12, 30, 35, 25, 20, 14, 24, 18, 40, 15, 32, 20, 11, 14, 37, 8, 37, 27, 39, 10, 51, 16, 46, 31, 19, 9, 62, 27, 21, 10, 34, 14, 23, 17, 24, 22, 27, 24, 14, 20, 19, 6, 17, 16, 25, 14, 24, 43, 39, 11, 27, 22, 28, 13, 33, 48, 15, 37, 23, 35, 44, 24, 25, 24, 34, 13, 14, 21, 17, 67, 34, 12, 15, 19, 33, 64, 15, 53, 14, 32, 15, 29, 32, 24, 23, 31, 9, 17, 32, 22, 27, 16, 14, 14, 26, 17, 27, 22, 12, 16, 26, 58, 26, 7, 22, 17, 8, 28, 23, 32, 22, 25, 25, 13, 45, 29, 40, 21, 38, 36, 17, 23, 24, 14, 36, 23, 36, 18, 36, 13, 53, 16, 17, 15, 32, 34, 25, 8, 17, 15, 29, 19, 41, 13, 45, 45, 27, 34, 43, 41, 17, 10, 16, 18, 24, 7, 16, 25, 38, 14, 25, 10, 33, 17, 40, 30, 24, 21, 37, 8, 12, 36, 22, 25, 35, 67, 29, 15, 50, 35, 57, 25, 42, 12, 17, 12, 12, 23, 18, 18, 36, 9, 11, 46, 15, 33, 15, 33, 21, 28, 11, 24, 5, 25, 18, 12, 45, 24, 20, 20, 45, 21, 11, 18, 9, 37, 21, 56, 11, 10, 40, 35, 17, 37, 7, 14, 21, 12, 8, 31, 26, 11, 23, 30, 15, 11, 29, 11, 19, 28, 34, 26, 13, 17, 21, 27, 39, 61, 17, 8, 6, 30, 18, 36, 6, 12, 17, 9, 40, 20, 15, 10, 49, 45, 40, 27, 14, 20, 48, 40, 13, 11, 12, 29, 31, 26, 28, 14, 20, 20, 27, 8, 13, 16, 31, 35, 25, 10, 20, 12, 38, 10, 18, 39, 45, 14, 20, 32, 9, 28, 12, 32, 18, 13, 27, 9, 26, 23, 55, 28, 23, 48, 62, 54, 30, 11, 49, 24, 15, 27, 21, 17, 42, 24, 29, 33, 19, 10, 37, 27, 16, 37, 23, 36, 53, 12, 18, 49, 33, 23, 22, 21, 17, 11, 40, 35, 8, 20, 28, 19, 13, 54, 60, 25, 21, 36, 9, 23, 51, 28, 26, 13, 21, 23, 34, 11, 32, 27, 23, 13, 29, 19, 37, 61, 23, 71, 22, 19, 6, 34, 7, 26, 23, 18, 25, 56, 20, 13, 46, 23, 10, 28, 45, 41, 35, 12, 20, 27, 11, 33, 13, 57, 7, 22, 10, 8, 12, 16, 35, 12, 19, 28, 40, 14, 20, 34, 53, 24, 10, 41, 25, 16, 15, 11, 8, 20, 37, 24, 47, 66, 64, 12, 8, 15, 32, 12, 28, 19, 14, 20, 14, 18, 35, 57, 37, 51, 29, 16, 21, 13, 20, 23, 32, 42, 27, 8, 28, 33, 20, 14, 9, 18, 30, 49, 49, 32, 56, 17, 28, 29, 37, 25, 18, 12, 41, 14, 18, 33, 14, 37, 18, 31, 35, 37, 37, 28, 13, 31, 41, 24, 22, 30, 32, 14, 16, 20, 37, 22, 20, 34, 21, 31, 32, 14, 41, 9, 59, 35, 17, 21, 38, 8, 31, 21, 13, 25, 32, 22, 12, 10, 37, 27, 30, 19, 21, 34, 8, 21, 27, 12, 8, 17, 47, 11, 24, 22, 59, 24, 50, 15, 18, 16, 22, 26, 37, 19, 23, 9, 9, 59, 11, 21, 54, 63, 46, 22, 38, 23, 35, 11, 34, 20, 10, 8, 27, 18, 34, 7, 16, 15, 30, 19, 38, 33, 35, 17, 31, 35, 20, 14, 15, 32, 15, 46, 9, 10, 17, 33, 16, 22, 58, 19, 35, 14, 24, 16, 34, 17, 26, 45, 26, 13, 28, 13, 12, 25, 15, 24, 37, 15, 65, 39, 26, 26, 28, 12, 39, 33, 18, 17, 19, 22, 12, 23, 39, 23, 15, 29, 20, 22, 28, 17, 45, 14, 16, 71, 44, 40, 12, 11, 27, 9, 40, 8, 28, 11, 56, 20, 12, 14, 14, 26, 38, 24, 37, 30, 15, 64, 14, 48, 38, 18, 56, 17, 19, 25, 28, 20, 22, 15, 35, 30, 27, 27, 19, 16, 22, 23, 26, 50, 27, 9, 13, 39, 32, 21, 16, 50, 12, 25, 8, 26, 15, 33, 48, 57, 19, 20, 50, 16, 40, 35, 17, 33, 17, 35, 12, 19, 19, 23, 18, 14, 22, 26, 40, 36, 11, 14, 20, 11, 10, 43, 20, 16, 25, 37, 17, 24, 58, 38, 41, 40, 14, 12, 35, 12, 21, 17, 19, 18, 12, 54, 21, 14, 28, 31, 30, 17, 17, 50, 11, 12, 18, 18, 24, 29, 33, 28, 18, 37, 17, 27, 11, 32, 46, 36, 30, 36, 40, 20, 26, 45, 18, 14, 16, 29, 17, 16, 33, 36, 26, 20, 27, 17, 54, 34, 54, 11, 8, 32, 24, 14, 10, 33, 10, 61, 13, 20, 22, 27, 22, 24, 13, 23, 21, 18, 14, 63, 14, 29, 8, 22, 13, 18, 29, 19, 24, 33, 53, 19, 20, 21, 10, 17, 24, 17, 14, 40, 22, 28, 28, 37, 21, 66, 49, 28, 68, 28, 23, 36, 20, 19, 26, 21, 31, 19, 21, 68, 25, 19, 45, 16, 38, 12, 12, 28, 32, 21, 33, 15, 6, 22, 38, 40, 24, 47, 12, 19, 23, 11, 9, 5, 10, 21, 24, 30, 9, 31, 16, 56, 23, 13, 25, 21, 33, 19, 34, 32, 30, 30, 34, 40, 10, 29, 48, 32, 14, 22, 13, 13, 20, 19, 27, 33, 32, 23, 21, 32, 15, 36, 17, 42, 33, 19, 65, 13, 16, 33, 15, 29, 33, 18, 38, 48, 17, 46, 20, 19, 15, 21, 24, 25, 18, 19, 13, 12, 31, 14, 43, 26, 14, 12, 32, 12, 14, 10, 25, 35, 18, 59, 21, 20, 21, 38, 14, 22, 35, 13, 15, 41, 28, 37, 13, 17, 19, 19, 16, 18, 39, 62, 28, 14, 14, 26, 25, 24, 40, 29, 21, 53, 20, 23, 26, 33, 62, 8, 16, 15, 20, 38, 12, 51, 11, 20, 9, 15, 21, 24, 74, 41, 25, 9, 43, 30, 22, 44, 44, 28, 48, 9, 35, 18, 25, 33, 46, 16, 28, 11, 25, 26, 22, 22, 45, 29, 23, 10, 26, 9, 8, 17, 16, 9, 31, 10, 13, 10, 35, 17, 25, 9, 70, 7, 39, 23, 40, 14, 37, 28, 16, 14, 43, 21, 31, 20, 37, 37, 14, 58, 12, 24, 42, 24, 19, 25, 10, 18, 32, 35, 25, 71, 24, 44, 37, 18, 20, 25, 18, 17, 6, 36, 33, 23, 26, 42, 43, 29, 32, 46, 64, 27, 67, 24, 17, 32, 30, 17, 12, 12, 30, 48, 12, 35, 12, 13, 20, 61, 61, 16, 39, 18, 16, 12, 54, 31, 11, 21, 15, 29, 44, 25, 11, 31, 49, 17, 7, 40, 22, 17, 21, 22, 11, 52, 18, 25, 22, 24, 29, 27, 21, 18, 30, 16, 5, 19, 20, 36, 13, 19, 9, 32, 13, 23, 33, 15, 25, 31, 46, 18, 21, 25, 40, 17, 12, 9, 13, 18, 9, 16, 24, 36, 11, 11, 30, 32, 20, 27, 22, 15, 15, 35, 49, 31, 30, 14, 47, 14, 3, 18, 15, 22, 15, 26, 6, 9, 27, 21, 39, 18, 33, 14, 16, 28, 18, 15, 24, 15, 34, 15, 22, 5, 9, 27, 21, 37, 15, 32, 56, 10, 31, 24, 20, 17, 23, 19, 21, 25, 24, 6, 54, 19, 33, 30, 29, 35, 20, 20, 18, 27, 39, 24, 41, 40, 14, 14, 29, 40, 48, 16, 40, 69, 35, 11, 51, 15, 34, 17, 20, 35, 20, 29, 11, 14, 22, 24, 12, 17, 39, 17, 20, 31, 52, 9, 26, 38, 17, 20, 8, 26, 31, 59, 24, 25, 13, 11, 24, 24, 38, 15, 28, 29, 14, 45, 57, 20, 57, 8, 21, 33, 19, 18, 10, 28, 20, 25, 18, 22, 51, 17, 20, 25, 32, 51, 53, 18, 11, 18, 62, 43, 14, 13, 13, 17, 19, 57, 17, 18, 7, 29, 57, 16, 33, 8, 16, 27, 31, 31, 35, 38, 51, 32, 21, 11, 34, 60, 46, 14, 24, 18, 19, 18, 26, 40, 10, 24, 10, 7, 14, 26, 26, 20, 22, 18, 7, 24, 17, 17, 65, 28, 37, 13, 44, 16, 43, 16, 49, 52, 13, 24, 28, 36, 13, 14, 21, 48, 15, 21, 13, 53, 19, 33, 12, 13, 7, 16, 34, 12, 13, 17, 36, 15, 25, 17, 12, 22, 13, 12, 32, 23, 17, 18, 42, 57, 14, 29, 13, 34, 32, 39, 16, 21, 11, 22, 8, 20, 58, 8, 25, 23, 10, 54, 40, 32, 23, 13, 12, 15, 46, 41, 26, 21, 32, 27, 14, 18, 42, 49, 14, 25, 23, 22, 30, 26, 23, 42, 20, 22, 29, 5, 12, 16, 10, 18, 24, 7, 17, 24, 11, 28, 24, 20, 19, 18, 45, 16, 58, 23, 10, 19, 11, 14, 19, 16, 23, 39, 36, 15, 10, 15, 20, 27, 37, 18, 23, 28, 39, 19, 38, 63, 31, 39, 28, 17, 32, 19, 12, 17, 25, 28, 14, 33, 43, 12, 13, 30, 19, 39, 6, 11, 17, 14, 46, 30, 19, 23, 24, 14, 16, 9, 17, 16, 15, 18, 38, 27, 48, 29, 19, 60, 12, 10, 14, 32, 10, 15, 8, 10, 31, 30, 15, 36, 11, 8, 39, 15, 17, 13, 33, 20, 29, 22, 31, 28, 15, 59, 22, 18, 24, 16, 17, 22, 40, 38, 34, 37, 17, 17, 16, 18, 11, 34, 18, 19, 28, 46, 50, 35, 30, 8, 12, 10, 23, 32, 16, 21, 15, 40, 20, 37, 36, 43, 48, 36, 34, 26, 21, 15, 35, 10, 14, 44, 49, 37, 57, 28, 17, 14, 19, 14, 33, 11, 15, 36, 9, 19, 45, 25, 21, 20, 18, 17, 17, 24, 33, 16, 13, 24, 21, 24, 33, 10, 55, 18, 10, 9, 17, 30, 43, 33, 28, 19, 11, 44, 15, 52, 10, 7, 23, 25, 17, 21, 14, 12, 22, 27, 12, 18, 33, 14, 22, 15, 31, 19, 19, 50, 15, 19, 23, 45, 32, 29, 30, 29, 19, 46, 19, 31, 15, 60, 18, 16, 30, 24, 19, 31, 27, 40, 13, 7, 7, 25, 20, 43, 25, 40, 29, 34, 26, 35, 36, 49, 13, 20, 14, 9, 16, 32, 42, 5, 29, 35, 25, 45, 24, 20, 28, 9, 9, 19, 26, 67, 44, 9, 18, 33, 7, 14, 52, 10, 51, 22, 14, 20, 22, 38, 19, 37, 17, 37, 65, 32, 25, 25, 9, 17, 29, 37, 33, 20, 20, 14, 15, 24, 19, 15, 29, 34, 10, 14, 10, 37, 25, 17, 12, 28, 14, 18, 25, 35, 40, 26, 46, 43, 44, 14, 19, 10, 30, 21, 16, 34, 23, 14, 15, 27, 10, 16, 14, 17, 19, 41, 56, 18, 15, 15, 11, 15, 27, 17, 36, 21, 22, 27, 43, 25, 22, 39, 31, 20, 19, 27, 39, 40, 27, 17, 50, 9, 16, 10, 66, 8, 17, 67, 69, 21, 10, 32, 19, 13, 8, 57, 9, 21, 33, 21, 24, 21, 14, 37, 18, 14, 16, 27, 16, 37, 26, 30, 35, 16, 42, 15, 32, 10, 10, 18, 50, 41, 16, 9, 23, 63, 64, 42, 26, 17, 69, 44, 21, 33, 22, 13, 16, 20, 15, 16, 22, 33, 12, 12, 41, 27, 34, 30, 26, 6, 12, 22, 25, 10, 18, 20, 17, 8, 7, 34, 31, 30, 38, 46, 47, 31, 10, 31, 34, 19, 28, 25, 19, 20, 26, 28, 14, 53, 28, 11, 14, 15, 30, 27, 30, 16, 24, 21, 19, 44, 20, 10, 27, 39, 19, 30, 14, 35, 7, 16, 16, 14, 26, 30, 24, 23, 10, 47, 33, 42, 21, 24, 18, 7, 53, 49, 28, 22, 39, 16, 29, 14, 16, 27, 23, 22, 11, 26, 47, 35, 38, 18, 22, 10, 21, 17, 16, 43, 23, 14, 18, 25, 30, 66, 33, 31, 21, 14, 18, 25, 11, 40, 17, 19, 15, 20, 26, 20, 36, 20, 42, 39, 31, 33, 16, 46, 12, 13, 37, 32, 22, 18, 27, 16, 16, 30, 26, 27, 36, 24, 26, 44, 26, 35, 30, 14, 12, 25, 20, 18, 20, 24, 66, 29, 45, 8, 35, 25, 34, 26, 12, 62, 20, 71, 12, 47, 13, 47, 31, 15, 6, 8, 24, 34, 22, 15, 15, 38, 12, 27, 50, 19, 6, 27, 37, 52, 18, 23, 25, 49, 37, 22, 11, 8, 41, 29, 13, 59, 38, 17, 14, 19, 13, 12, 27, 30, 22, 21, 43, 31, 21, 21, 19, 31, 39, 14, 36, 13, 17, 29, 29, 32, 23, 19, 61, 38, 19, 11, 24, 30, 17, 43, 25, 12, 24, 25, 25, 13, 31, 10, 20, 13, 13, 15, 23, 12, 13, 17, 15, 18, 14, 12, 26, 15, 18, 41, 16, 33, 46, 17, 27, 23, 47, 21, 25, 57, 17, 42, 11, 20, 44, 20, 6, 33, 9, 39, 10, 9, 27, 14, 39, 19, 24, 14, 19, 27, 28, 10, 25, 34, 26, 16, 8, 15, 50, 22, 17, 17, 16, 23, 12, 29, 26, 30, 12, 11, 8, 23, 13, 24, 61, 11, 36, 10, 27, 28, 12, 26, 38, 13, 22, 25, 12, 42, 12, 13, 12, 27, 16, 37, 36, 33, 28, 24, 31, 11, 28, 31, 28, 11, 10, 11, 44, 21, 20, 10, 17, 10, 10, 42, 36, 19, 14, 11, 27, 23, 46, 18, 57, 37, 54, 14, 25, 12, 61, 26, 17, 29, 28, 19, 23, 23, 34, 16, 22, 8, 46, 11, 26, 68, 18, 24, 40, 33, 13, 17, 8, 12, 15, 9, 43, 10, 40, 23, 23, 20, 19, 15, 18, 55, 55, 19, 28, 29, 22, 15, 23, 17, 27, 13, 49, 9, 22, 37, 53, 32, 25, 73, 19, 18, 23, 54, 30, 38, 28, 13, 39, 68, 10, 18, 56, 41, 23, 31, 28, 45, 30, 59, 29, 49, 22, 19, 19, 13, 12, 23, 28, 44, 12, 65, 34, 15, 45, 13, 34, 53, 46, 50, 15, 12, 16, 41, 15, 19, 18, 6, 25, 7, 41, 17, 29, 19, 40, 25, 20, 58, 29, 12, 32, 39, 14, 32, 22, 12, 12, 36, 28, 31, 30, 39, 18, 27, 29, 16, 29, 30, 51, 13, 42, 46, 31, 20, 27, 11, 9, 9, 23, 29, 25, 13, 25, 17, 20, 18, 52, 9, 53, 9, 15, 36, 24, 11, 51, 18, 11, 18, 48, 21, 31, 9, 39, 12, 29, 35, 25, 17, 20, 44, 34, 19, 11, 48, 27, 34, 55, 10, 23, 21, 52, 16, 54, 21, 43, 9, 12, 13, 17, 21, 15, 22, 9, 21, 21, 19, 28, 28, 19, 57, 11, 12, 44, 54, 10, 21, 22, 39, 16, 21, 21, 20, 25, 27, 35, 25, 18, 48, 15, 20, 32, 30, 29, 14, 52, 15, 13, 19, 28, 19, 20, 20, 17, 14, 18, 21, 17, 16, 33, 16, 37, 18, 19, 26, 69, 18, 15, 27, 25, 22, 15, 25, 19, 17, 20, 33, 29, 33, 18, 21, 15, 60, 24, 26, 55, 14, 12, 24, 26, 37, 14, 10, 6, 65, 14, 15, 24, 30, 12, 11, 18, 11, 10, 29, 15, 25, 34, 27, 21, 12, 17, 20, 56, 23, 65, 18, 19, 23, 25, 17, 64, 19, 27, 36, 53, 34, 45, 49, 43, 27, 11, 38, 47, 14, 29, 29, 21, 20, 25, 17, 10, 26, 49, 19, 17, 13, 9, 21, 23, 11, 54, 38, 13, 25, 8, 17, 29, 16, 20, 12, 26, 20, 15, 16, 25, 6, 32, 25, 11, 10, 15, 10, 36, 15, 17, 17, 18, 12, 23, 16, 28, 14, 19, 32, 17, 23, 16, 25, 17, 17, 29, 22, 32, 37, 40, 34, 30, 22, 53, 21, 53, 29, 45, 20, 9, 13, 26, 29, 10, 35, 34, 25, 41, 16, 17, 30, 40, 47, 16, 8, 11, 23, 21, 14, 18, 34, 32, 52, 34, 9, 30, 29, 32, 14, 7, 22, 57, 18, 27, 15, 33, 19, 15, 31, 20, 15, 16, 26, 26, 14, 33, 24, 40, 16, 26, 13, 16, 11, 33, 30, 22, 8, 33, 43, 15, 54, 41, 13, 33, 19, 14, 14, 59, 34, 40, 16, 12, 16, 30, 23, 12, 30, 10, 32, 23, 18, 8, 13, 21, 20, 29, 12, 12, 20, 39, 14, 22, 23, 28, 28, 11, 11, 34, 43, 48, 51, 25, 26, 10, 52, 20, 31, 27, 11, 32, 15, 11, 19, 22, 56, 32, 13, 24, 17, 19, 25, 60, 23, 7, 12, 33, 18, 31, 58, 11, 13, 13, 19, 14, 65, 28, 34, 24, 11, 17, 47, 20, 34, 14, 21, 10, 10, 12, 29, 16, 30, 23, 17, 14, 28, 22, 16, 35, 10, 40, 14, 26, 42, 25, 7, 49, 47, 21, 20, 35, 28, 55, 29, 26, 27, 26, 42, 37, 30, 19, 14, 19, 15, 32, 11, 18, 34, 37, 8, 21, 9, 26, 18, 6, 39, 17, 22, 17, 25, 41, 18, 19, 28, 23, 24, 23, 16, 10, 21, 30, 42, 23, 25, 20, 31, 23, 12, 44, 16, 36, 20, 18, 33, 11, 27, 17, 26, 13, 14, 17, 32, 69, 25, 13, 28, 22, 45, 76, 63, 13, 26, 27, 30, 24, 26, 9, 57, 30, 8, 19, 48, 17, 54, 45, 34, 17, 17, 30, 7, 12, 33, 31, 15, 17, 34, 12, 43, 6, 9, 43, 24, 22, 35, 17, 32, 36, 16, 53, 28, 31, 42, 28, 30, 20, 12, 42, 9, 41, 38, 28, 17, 39, 9, 9, 45, 28, 25, 18, 15, 27, 29, 32, 8, 17, 40, 14, 17, 21, 10, 42, 50, 16, 14, 29, 32, 25, 13, 37, 13, 23, 15, 38, 18, 19, 18, 33, 18, 32, 28, 29, 18, 27, 17, 11, 19, 21, 35, 16, 20, 31, 28, 35, 49, 16, 14, 24, 33, 34, 29, 27, 12, 22, 19, 31, 37, 8, 16, 55, 70, 35, 46, 34, 16, 12, 20, 20, 36, 45, 64, 22, 28, 22, 11, 11, 14, 26, 14, 66, 21, 31, 18, 39, 21, 24, 71, 16, 14, 14, 12, 23, 25, 7, 14, 29, 51, 53, 35, 14, 12, 56, 23, 56, 22, 9, 32, 28, 18, 12, 43, 45, 7, 14, 19, 29, 11, 32, 40, 32, 30, 20, 12, 25, 40, 14, 33, 31, 37, 31, 25, 16, 58, 12, 7, 18, 8, 29, 22, 20, 9, 12, 29, 10, 24, 19, 19, 40, 18, 22, 17, 18, 66, 14, 18, 45, 15, 9, 19, 25, 17, 26, 18, 56, 8, 17, 12, 37, 13, 31, 18, 25, 34, 32, 41, 22, 13, 36, 27, 29, 49, 19, 24, 32, 8, 26, 61, 13, 17, 16, 37, 18, 29, 21, 43, 16, 63, 37, 35, 13, 53, 50, 10, 45, 24, 22, 16, 38, 34, 18, 21, 18, 13, 25, 21, 23, 11, 23, 39, 27, 59, 23, 36, 27, 12, 8, 26, 26, 11, 31, 10, 5, 24, 24, 29, 44, 41, 12, 17, 7, 19, 18, 21, 48, 25, 29, 12, 24, 38, 18, 20, 16, 42, 15, 20, 24, 16, 54, 26, 20, 12, 33, 14, 29, 28, 20, 20, 15, 19, 15, 32, 27, 6, 49, 13, 37, 25, 27, 15, 49, 10, 19, 42, 13, 18, 16, 41, 51, 16, 33, 18, 59, 33, 74, 26, 51, 29, 15, 18, 40, 46, 19, 46, 11, 19, 28, 21, 11, 12, 30, 38, 10, 45, 45, 40, 12, 11, 19, 63, 12, 33, 35, 25, 20, 26, 51, 28, 16, 26, 11, 59, 16, 41, 69, 23, 20, 9, 39, 32, 40, 24, 11, 15, 24, 12, 54, 36, 48, 52, 27, 23, 27, 19, 56, 18, 16, 22, 17, 19, 23, 53, 18, 31, 13, 11, 17, 15, 21, 28, 26, 24, 20, 16, 17, 12, 38, 56, 23, 12, 21, 54, 21, 41, 26, 15, 19, 23, 17, 18, 22, 15, 37, 25, 38, 15, 29, 47, 22, 15, 20, 15, 10, 26, 57, 14, 33, 22, 11, 55, 51, 50, 18, 13, 76, 20, 42, 39, 20, 10, 34, 22, 11, 31, 46, 11, 25, 33, 27, 19, 47, 31, 28, 40, 10, 33, 11, 8, 31, 50, 17, 15, 20, 32, 15, 16, 17, 65, 42, 28, 40, 50, 9, 18, 41, 51, 14, 29, 54, 65, 29, 9, 50, 25, 32, 25, 33, 28, 21, 55, 17, 35, 27, 22, 11, 26, 54, 12, 18, 31, 31, 13, 17, 43, 24, 11, 31, 39, 29, 6, 47, 40, 36, 18, 31, 13, 47, 25, 23, 17, 15, 15, 20, 27, 18, 16, 15, 18, 8, 16, 24, 15, 17, 14, 10, 15, 12, 20, 24, 17, 64, 9, 10, 19, 26, 17, 7, 16, 12, 9, 8, 23, 57, 14, 44, 30, 13, 39, 35, 28, 8, 36, 15, 23, 28, 41, 28, 17, 20, 13, 45, 21, 62, 12, 10, 20, 15, 29, 31, 14, 14, 35, 42, 19, 22, 9, 53, 12, 40, 21, 20, 14, 21, 31, 13, 29, 18, 23, 17, 46, 18, 14, 23, 29, 24, 21, 29, 28, 17, 22, 17, 29, 26, 14, 14, 53, 47, 23, 22, 39, 39, 48, 11, 53, 13, 44, 15, 34, 23, 19, 27, 34, 39, 46, 21, 15, 51, 44, 26, 13, 9, 31, 19, 18, 43, 44, 58, 48, 8, 8, 17, 44, 13, 24, 19, 39, 52, 29, 40, 15, 13, 30, 19, 11, 15, 16, 10, 28, 37, 39, 28, 35, 13, 29, 22, 13, 14, 50, 53, 16, 12, 19, 23, 36, 15, 30, 18, 24, 54, 10, 21, 21, 15, 24, 47, 29, 22, 42, 29, 11, 12, 43, 14, 20, 29, 43, 15, 30, 15, 9, 28, 29, 39, 32, 11, 55, 20, 20, 12, 42, 11, 49, 24, 33, 32, 12, 23, 15, 47, 17, 20, 43, 37, 48, 37, 24, 17, 58, 46, 34, 36, 46, 29, 42, 28, 18, 15, 15, 58, 9, 50, 31, 10, 17, 25, 35, 8, 8, 20, 35, 36, 63, 21, 16, 13, 12, 14, 49, 15, 22, 43, 14, 23, 45, 64, 23, 15, 10, 34, 32, 40, 22, 23, 20, 38, 40, 17, 39, 26, 16, 28, 34, 54, 23, 12, 21, 20, 9, 17, 23, 13, 17, 13, 33, 16, 17, 29, 30, 11, 63, 10, 25, 16, 16, 32, 21, 32, 18, 26, 36, 24, 33, 26, 14, 40, 37, 14, 20, 29, 10, 30, 19, 12, 18, 38, 25, 12, 16, 28, 10, 14, 32, 25, 13, 25, 9, 28, 19, 7, 26, 63, 23, 31, 18, 17, 9, 33, 16, 18, 12, 50, 25, 30, 28, 18, 40, 39, 19, 18, 14, 17, 8, 12, 54, 29, 24, 38, 6, 21, 10, 20, 24, 21, 33, 6, 13, 25, 17, 39, 27, 42, 20, 18, 23, 32, 17, 49, 35, 28, 17, 12, 13, 35, 40, 14, 25, 37, 13, 12, 13, 22, 19, 26, 38, 14, 15, 38, 31, 25, 36, 28, 28, 32, 17, 21, 11, 23, 20, 48, 20, 38, 9, 51, 17, 30, 8, 9, 49, 37, 10, 16, 31, 39, 27, 19, 7, 41, 29, 26, 14, 29, 46, 49, 16, 26, 18, 19, 32, 49, 25, 45, 39, 32, 29, 27, 48, 22, 16, 30, 22, 7, 8, 41, 23, 29, 24, 25, 18, 48, 17, 13, 19, 23, 14, 11, 43, 21, 46, 52, 15, 11, 12, 26, 36, 17, 32, 42, 13, 20, 15, 13, 8, 24, 12, 21, 14, 17, 12, 28, 51, 25, 14, 16, 43, 15, 32, 49, 13, 45, 22, 56, 21, 52, 9, 47, 17, 33, 19, 24, 40, 21, 26, 17, 13, 15, 31, 17, 36, 20, 16, 72, 23, 19, 33, 12, 18, 15, 24, 36, 24, 10, 41, 30, 15, 13, 11, 8, 24, 25, 17, 18, 13, 13, 20, 27, 37, 33, 13, 15, 24, 13, 19, 23, 50, 19, 19, 17, 32, 18, 22, 12, 8, 25, 28, 22, 8, 19, 37, 17, 35, 24, 10, 18, 18, 12, 11, 34, 29, 29, 22, 12, 10, 13, 30, 25, 32, 10, 49, 19, 32, 23, 17, 24, 20, 10, 31, 11, 20, 26, 24, 26, 17, 13, 6, 20, 35, 7, 17, 11, 18, 22, 31, 28, 22, 24, 26, 22, 33, 18, 15, 21, 27, 33, 18, 28, 26, 19, 13, 18, 17, 6, 23, 36, 11, 14, 65, 23, 38, 21, 20, 13, 40, 28, 14, 12, 26, 14, 11, 24, 29, 20, 13, 15, 22, 18, 21, 22, 27, 31, 8, 19, 21, 25, 8, 18, 18, 36, 49, 26, 28, 16, 12, 17, 34, 24, 12, 18, 52, 39, 9, 37, 7, 10, 21, 29, 30, 22, 39, 14, 16, 27, 16, 22, 33, 36, 62, 9, 18, 39, 59, 28, 39, 48, 41, 16, 21, 15, 40, 10, 18, 15, 59, 46, 18, 19, 52, 58, 8, 28, 13, 22, 10, 37, 12, 37, 26, 21, 43, 26, 31, 58, 17, 20, 33, 14, 21, 30, 34, 8, 44, 16, 51, 11, 25, 23, 10, 19, 12, 27, 24, 48, 33, 22, 40, 66, 61, 43, 11, 20, 31, 9, 17, 28, 22, 24, 42, 22, 12, 36, 59, 7, 25, 19, 22, 36, 16, 15, 29, 10, 27, 11, 29, 41, 34, 17, 17, 28, 13, 23, 56, 20, 57, 39, 44, 11, 30, 13, 21, 12, 15, 43, 55, 26, 18, 20, 20, 33, 8, 18, 18, 12, 21, 13, 37, 30, 27, 17, 19, 52, 24, 45, 14, 4, 40, 29, 50, 30, 16, 49, 21, 33, 25, 32, 16, 49, 36, 53, 8, 18, 39, 14, 9, 17, 37, 17, 20, 29, 23, 15, 23, 16, 14, 14, 38, 34, 21, 19, 56, 11, 22, 49, 41, 17, 28, 32, 10, 28, 23, 21, 15, 12, 25, 19, 25, 14, 26, 17, 16, 38, 13, 19, 27, 8, 17, 47, 18, 13, 30, 24, 43, 37, 41, 27, 15, 9, 17, 23, 12, 7, 16, 31, 28, 15, 16, 6, 37, 32, 30, 21, 26, 40, 21, 28, 25, 14, 29, 18, 35, 38, 29, 12, 35, 13, 41, 20, 31, 18, 17, 18, 36, 28, 12, 32, 30, 58, 67, 10, 67, 33, 14, 16, 50, 21, 12, 10, 9, 23, 24, 30, 48, 24, 25, 34, 22, 14, 33, 16, 48, 32, 12, 23, 24, 18, 12, 10, 13, 16, 24, 65, 22, 29, 10, 14, 14, 18, 9, 24, 25, 5, 17, 21, 14, 28, 43, 39, 9, 21, 7, 37, 24, 49, 31, 32, 16, 46, 27, 13, 29, 7, 37, 33, 34, 21, 14, 26, 17, 19, 20, 21, 27, 31, 8, 41, 23, 34, 44, 31, 35, 43, 32, 7, 15, 15, 15, 18, 34, 9, 15, 29, 18, 42, 10, 42, 10, 23, 24, 28, 9, 15, 12, 13, 27, 26, 7, 13, 13, 34, 14, 47, 12, 22, 64, 46, 12, 26, 50, 20, 59, 60, 21, 8, 60, 27, 18, 39, 28, 16, 61, 21, 20, 56, 16, 44, 21, 25, 18, 35, 29, 19, 16, 20, 28, 13, 23, 23, 29, 29, 17, 22, 10, 47, 43, 42, 16, 19, 13, 17, 22, 9, 35, 39, 61, 33, 11, 39, 58, 25, 43, 37, 18, 28, 20, 38, 6, 12, 17, 20, 22, 15, 48, 7, 42, 12, 9, 19, 35, 25, 32, 16, 31, 12, 44, 33, 9, 33, 42, 15, 18, 35, 20, 24, 31, 38, 16, 8, 29, 26, 36, 34, 42, 33, 34, 17, 13, 21, 40, 29, 13, 64, 11, 35, 22, 26, 16, 53, 37, 26, 14, 20, 17, 12, 39, 7, 27, 19, 27, 19, 24, 39, 17, 18, 35, 66, 44, 45, 20, 17, 18, 30, 56, 47, 19, 45, 32, 11, 8, 27, 14, 46, 48, 39, 33, 41, 30, 59, 32, 44, 15, 45, 16, 37, 30, 27, 29, 31, 15, 37, 22, 7, 37, 35, 27, 10, 6, 12, 11, 11, 18, 8, 39, 34, 46, 31, 61, 22, 25, 9, 37, 12, 23, 18, 18, 39, 26, 19, 48, 35, 34, 34, 57, 76, 35, 30, 29, 24, 22, 18, 15, 12, 13, 15, 16, 15, 33, 32, 43, 29, 17, 48, 24, 28, 9, 39, 27, 15, 54, 22, 6, 8, 15, 17, 22, 10, 15, 16, 18, 6, 26, 15, 13, 12, 15, 50, 15, 11, 35, 8, 30, 28, 26, 32, 42, 41, 9, 25, 30, 27, 36, 25, 25, 15, 37, 20, 8, 24, 41, 13, 55, 15, 18, 13, 14, 13, 44, 23, 24, 7, 29, 32, 35, 18, 42, 17, 45, 19, 42, 22, 13, 14, 62, 29, 9, 36, 38, 12, 51, 42, 47, 27, 26, 19, 19, 46, 33, 11, 25, 36, 59, 57, 10, 16, 14, 19, 36, 15, 8, 18, 16, 44, 15, 16, 16, 13, 10, 34, 20, 19, 37, 31, 9, 17, 16, 22, 13, 45, 37, 28, 37, 18, 12, 38, 13, 13, 38, 36, 9, 20, 29, 22, 17, 5, 31, 13, 13, 19, 38, 19, 23, 11, 17, 27, 31, 9, 12, 40, 46, 13, 27, 14, 30, 50, 30, 6, 9, 17, 23, 35, 16, 19, 58, 19, 24, 7, 14, 32, 40, 11, 42, 14, 13, 22, 9, 18, 28, 28, 33, 14, 10, 31, 20, 7, 19, 48, 10, 10, 64, 50, 28, 21, 18, 8, 42, 21, 61, 12, 26, 26, 46, 44, 32, 22, 23, 21, 27, 15, 18, 36, 20, 6, 17, 17, 25, 16, 10, 53, 22, 31, 33, 36, 18, 42, 21, 46, 20, 43, 22, 15, 68, 10, 11, 14, 58, 24, 31, 30, 44, 48, 20, 29, 17, 31, 10, 48, 17, 18, 10, 10, 39, 24, 57, 9, 11, 8, 19, 47, 42, 16, 12, 29, 9, 6, 30, 10, 11, 21, 15, 20, 49, 19, 47, 8, 26, 34, 41, 10, 15, 13, 53, 16, 17, 25, 26, 44, 38, 19, 28, 32, 18, 23, 28, 14, 11, 54, 19, 12, 15, 32, 14, 16, 44, 54, 15, 17, 23, 16, 23, 22, 38, 10, 27, 23, 16, 40, 28, 43, 26, 19, 20, 36, 31, 9, 13, 28, 21, 17, 21, 49, 21, 32, 28, 18, 24, 13, 27, 17, 9, 13, 39, 13, 9, 55, 36, 10, 21, 67, 20, 18, 53, 22, 22, 20, 22, 19, 22, 7, 21, 45, 18, 55, 72, 35, 19, 25, 19, 18, 8, 42, 14, 16, 15, 45, 31, 72, 28, 28, 12, 58, 14, 32, 15, 18, 9, 23, 51, 14, 19, 16, 32, 26, 8, 28, 18, 64, 65, 12, 11, 25, 18, 20, 30, 34, 41, 8, 18, 28, 10, 19, 65, 22, 20, 32, 30, 61, 33, 36, 13, 19, 29, 20, 31, 34, 36, 18, 13, 30, 58, 27, 27, 34, 12, 24, 10, 29, 48, 29, 22, 11, 21, 26, 31, 18, 5, 46, 25, 53, 24, 26, 21, 14, 8, 54, 23, 18, 34, 10, 44, 44, 16, 19, 18, 13, 37, 13, 20, 60, 44, 31, 28, 29, 33, 26, 20, 35, 37, 30, 15, 65, 13, 13, 55, 18, 22, 20, 15, 20, 17, 10, 30, 28, 12, 29, 26, 9, 36, 35, 16, 13, 8, 44, 16, 30, 17, 27, 23, 8, 10, 15, 31, 21, 14, 32, 41, 21, 12, 25, 35, 44, 13, 23, 17, 17, 10, 11, 14, 5, 12, 31, 17, 43, 7, 31, 19, 8, 17, 16, 9, 20, 59, 27, 18, 31, 15, 18, 19, 43, 9, 13, 22, 16, 20, 10, 18, 12, 13, 31, 31, 17, 37, 18, 62, 15, 39, 27, 15, 26, 30, 8, 37, 15, 44, 51, 47, 20, 19, 13, 22, 50, 40, 27, 38, 25, 40, 24, 42, 15, 29, 17, 17, 18, 36, 28, 16, 32, 60, 48, 40, 14, 16, 30, 33, 36, 9, 21, 31, 17, 35, 32, 10, 22, 25, 30, 44, 23, 33, 10, 29, 23, 71, 19, 11, 36, 23, 35, 18, 16, 17, 11, 43, 10, 43, 26, 23, 24, 30, 32, 40, 34, 19, 49, 21, 10, 18, 15, 44, 23, 18, 31, 27, 21, 18, 13, 16, 15, 21, 12, 34, 34, 25, 20, 49, 23, 43, 11, 41, 21, 20, 48, 16, 39, 11, 25, 41, 28, 16, 63, 23, 27, 8, 13, 14, 7, 16, 16, 59, 20, 40, 10, 21, 11, 21, 22, 14, 33, 17, 36, 45, 46, 65, 28, 10, 21, 43, 26, 13, 12, 34, 49, 61, 12, 32, 33, 9, 9, 33, 24, 35, 14, 30, 14, 64, 39, 34, 15, 39, 13, 14, 15, 15, 16, 15, 19, 25, 15, 20, 21, 17, 9, 14, 20, 38, 42, 30, 38, 27, 15, 16, 15, 15, 19, 39, 30, 60, 29, 10, 44, 32, 14, 34, 13, 11, 37, 40, 18, 7, 21, 19, 20, 6, 9, 41, 35, 12, 10, 34, 30, 15, 11, 26, 9, 24, 10, 21, 14, 44, 32, 17, 9, 21, 36, 25, 25, 54, 21, 28, 26, 39, 40, 50, 22, 24, 17, 27, 14, 61, 14, 19, 20, 49, 28, 34, 12, 18, 32, 45, 17, 24, 40, 34, 9, 12, 15, 63, 7, 26, 12, 26, 53, 12, 16, 23, 11, 18, 12, 23, 40, 28, 42, 52, 19, 13, 15, 44, 37, 44, 25, 26, 15, 10, 51, 13, 49, 23, 44, 17, 10, 26, 11, 56, 61, 35, 21, 13, 28, 28, 27, 12, 12, 14, 19, 23, 41, 30, 43, 35, 23, 59, 24, 19, 15, 11, 24, 23, 41, 21, 17, 17, 24, 26, 60, 41, 37, 17, 12, 37, 19, 17, 26, 20, 16, 33, 29, 19, 8, 34, 13, 23, 21, 45, 40, 38, 8, 22, 16, 12, 16, 55, 45, 24, 34, 54, 21, 31, 31, 46, 30, 59, 28, 22, 14, 40, 31, 26, 16, 40, 17, 13, 41, 25, 11, 37, 25, 23, 25, 21, 29, 46, 13, 32, 17, 27, 14, 53, 20, 37, 21, 16, 12, 10, 25, 25, 11, 27, 20, 13, 40, 16, 14, 20, 17, 17, 34, 53, 11, 69, 20, 25, 49, 33, 13, 40, 38, 39, 17, 12, 55, 41, 25, 37, 24, 24, 17, 12, 38, 16, 16, 57, 21, 33, 31, 32, 13, 14, 33, 19, 20, 62, 29, 17, 19, 57, 18, 38, 17, 15, 29, 37, 25, 13, 18, 18, 23, 15, 6, 51, 35, 56, 14, 47, 7, 31, 30, 38, 15, 45, 14, 10, 37, 48, 17, 19, 34, 25, 10, 16, 33, 19, 38, 33, 28, 20, 40, 49, 17, 23, 33, 7, 61, 27, 9, 55, 17, 24, 9, 55, 25, 18, 17, 25, 41, 15, 27, 14, 27, 21, 14, 28, 23, 28, 27, 16, 10, 38, 22, 38, 37, 15, 26, 12, 13, 51, 32, 48, 25, 34, 32, 27, 23, 37, 34, 28, 29, 64, 29, 51, 51, 20, 29, 26, 9, 63, 10, 42, 23, 29, 15, 48, 19, 58, 19, 50, 16, 26, 22, 32, 32, 38, 17, 29, 14, 29, 35, 24, 36, 12, 15, 10, 12, 24, 12, 33, 11, 36, 65, 12, 34, 8, 63, 38, 11, 10, 22, 10, 67, 9, 32, 16, 33, 27, 22, 21, 20, 21, 27, 42, 18, 22, 14, 39, 33, 16, 9, 25, 18, 21, 40, 53, 18, 26, 43, 39, 9, 23, 25, 20, 17, 22, 20, 18, 17, 47, 36, 34, 9, 32, 15, 63, 22, 57, 20, 24, 17, 39, 17, 22, 45, 10, 20, 18, 21, 16, 12, 11, 10, 32, 31, 9, 25, 18, 18, 35, 26, 20, 42, 37, 28, 44, 19, 36, 19, 24, 11, 23, 58, 11, 24, 17, 22, 20, 13, 21, 13, 15, 19, 15, 25, 27, 12, 17, 17, 26, 47, 21, 53, 17, 11, 15, 33, 31, 52, 58, 30, 26, 17, 38, 18, 43, 27, 16, 7, 22, 30, 38, 19, 16, 30, 23, 28, 26, 29, 12, 12, 26, 16, 27, 14, 20, 23, 12, 9, 25, 28, 37, 50, 25, 45, 23, 51, 21, 28, 44, 23, 10, 50, 38, 35, 9, 20, 36, 14, 48, 26, 12, 30, 43, 18, 48, 28, 18, 40, 56, 52, 21, 19, 63, 14, 21, 28, 20, 12, 59, 54, 28, 35, 68, 17, 16, 32, 16, 12, 37, 38, 40, 35, 14, 11, 41, 15, 7, 42, 26, 16, 23, 24, 14, 12, 29, 24, 36, 60, 71, 46, 66, 29, 18, 15, 18, 28, 38, 15, 7, 18, 16, 37, 7, 12, 24, 34, 10, 9, 37, 19, 31, 11, 7, 28, 41, 18, 27, 23, 19, 14, 7, 50, 26, 24, 24, 18, 22, 34, 31, 15, 40, 17, 44, 45, 15, 34, 11, 16, 51, 16, 31, 26, 35, 16, 11, 35, 13, 28, 35, 26, 13, 26, 19, 46, 14, 30, 19, 13, 37, 24, 18, 10, 41, 21, 25, 23, 53, 23, 26, 15, 38, 9, 61, 50, 27, 16, 33, 20, 55, 9, 11, 54, 17, 41, 56, 24, 19, 15, 23, 15, 25, 29, 30, 54, 25, 17, 18, 49, 17, 7, 11, 39, 23, 63, 29, 21, 10, 18, 13, 20, 16, 13, 14, 8, 12, 17, 12, 37, 9, 30, 38, 17, 33, 15, 14, 23, 20, 24, 43, 42, 21, 9, 33, 30, 30, 14, 41, 12, 14, 17, 31, 11, 18, 31, 13, 47, 20, 29, 15, 32, 12, 19, 37, 18, 23, 60, 10, 25, 12, 13, 19, 50, 55, 30, 21, 7, 37, 20, 25, 27, 11, 21, 33, 22, 13, 22, 46, 31, 24, 27, 17, 29, 21, 13, 21, 6, 28, 15, 20, 31, 27, 9, 25, 17, 12, 30, 35, 33, 10, 17, 34, 21, 38, 17, 26, 15, 39, 14, 21, 15, 9, 33, 28, 13, 54, 21, 14, 47, 18, 20, 19, 35, 56, 11, 42, 21, 17, 26, 11, 11, 17, 21, 57, 34, 14, 30, 39, 21, 11, 70, 16, 13, 20, 27, 21, 18, 13, 21, 74, 33, 25, 18, 45, 32, 22, 12, 28, 18, 58, 13, 26, 26, 19, 34, 28, 30, 32, 23, 44, 29, 30, 27, 24, 11, 18, 40, 9, 37, 41, 16, 27, 28, 18, 11, 22, 8, 15, 52, 30, 46, 11, 58, 23, 34, 18, 7, 18, 9, 46, 20, 17, 41, 14, 31, 31, 32, 22, 13, 8, 43, 16, 28, 52, 13, 32, 9, 48, 28, 12, 25, 27, 22, 12, 9, 9, 66, 19, 9, 37, 16, 39, 23, 6, 11, 6, 12, 30, 29, 8, 11, 29, 17, 17, 55, 41, 9, 8, 29, 12, 28, 18, 16, 39, 15, 20, 42, 37, 8, 28, 16, 9, 27, 11, 16, 23, 12, 17, 67, 54, 22, 14, 35, 17, 12, 23, 14, 24, 35, 26, 61, 53, 20, 31, 30, 11, 31, 44, 29, 18, 25, 15, 17, 22, 11, 40, 7, 46, 10, 11, 21, 45, 45, 12, 10, 32, 26, 23, 22, 50, 12, 36, 20, 34, 21, 49, 10, 25, 23, 31, 32, 47, 20, 36, 46, 12, 15, 28, 25, 26, 49, 44, 31, 22, 22, 24, 43, 14, 27, 15, 24, 58, 45, 43, 30, 40, 14, 12, 12, 23, 25, 14, 24, 11, 16, 9, 20, 12, 20, 30, 24, 36, 18, 15, 52, 16, 71, 38, 30, 34, 8, 27, 31, 22, 32, 10, 51, 20, 33, 34, 54, 23, 18, 42, 50, 22, 31, 20, 20, 30, 12, 29, 11, 32, 10, 30, 39, 8, 31, 23, 32, 27, 16, 31, 14, 33, 41, 19, 11, 29, 24, 26, 37, 42, 39, 14, 15, 12, 30, 35, 32, 18, 45, 27, 22, 38, 48, 61, 23, 31, 8, 14, 20, 69, 14, 61, 11, 20, 23, 21, 32, 49, 12, 20, 22, 21, 16, 20, 29, 24, 22, 27, 18, 45, 20, 13, 18, 27, 52, 24, 52, 34, 38, 24, 11, 49, 13, 60, 7, 39, 9, 38, 18, 17, 13, 12, 19, 25, 30, 57, 28, 27, 22, 9, 26, 12, 26, 10, 34, 11, 25, 17, 30, 35, 23, 12, 8, 28, 21, 20, 9, 61, 18, 19, 17, 58, 24, 10, 16, 36, 45, 58, 28, 53, 24, 35, 31, 24, 35, 17, 22, 9, 24, 52, 16, 28, 23, 23, 64, 14, 23, 64, 41, 21, 18, 62, 25, 55, 14, 41, 18, 19, 36, 6, 23, 32, 23, 46, 42, 9, 22, 23, 34, 31, 33, 12, 16, 10, 27, 16, 16, 16, 7, 23, 27, 11, 16, 18, 29, 34, 12, 37, 14, 13, 10, 43, 12, 18, 30, 24, 35, 36, 19, 49, 36, 46, 14, 46, 22, 22, 40, 26, 20, 17, 16, 21, 14, 8, 15, 26, 26, 22, 13, 16, 30, 22, 8, 50, 11, 41, 16, 18, 25, 40, 32, 12, 9, 11, 8, 28, 10, 28, 13, 20, 46, 13, 17, 15, 21, 55, 11, 31, 24, 23, 13, 18, 40, 10, 18, 28, 10, 32, 12, 25, 25, 36, 21, 21, 30, 17, 40, 25, 20, 21, 27, 10, 14, 45, 29, 19, 16, 50, 13, 10, 23, 46, 20, 34, 14, 63, 44, 35, 26, 15, 16, 42, 17, 21, 14, 44, 36, 18, 15, 66, 18, 21, 13, 30, 23, 24, 9, 21, 22, 29, 22, 20, 42, 17, 20, 21, 17, 22, 13, 23, 13, 15, 37, 15, 17, 25, 26, 12, 13, 46, 15, 14, 14, 47, 18, 17, 20, 22, 22, 11, 11, 18, 35, 6, 18, 23, 50, 33, 36, 14, 32, 14, 14, 24, 28, 32, 23, 10, 44, 14, 16, 28, 33, 37, 30, 7, 49, 55, 16, 54, 18, 22, 37, 34, 20, 14, 54, 18, 23, 9, 10, 10, 23, 23, 24, 42, 35, 11, 49, 15, 22, 64, 42, 24, 13, 40, 10, 18, 15, 11, 23, 16, 23, 48, 12, 8, 21, 14, 17, 11, 20, 21, 17, 37, 12, 38, 31, 25, 63, 11, 24, 8, 8, 40, 11, 51, 19, 12, 31, 30, 12, 20, 16, 25, 23, 14, 21, 40, 13, 44, 11, 29, 17, 58, 48, 20, 18, 12, 21, 47, 30, 36, 23, 24, 30, 75, 25, 33, 40, 12, 16, 41, 17, 10, 46, 35, 12, 23, 15, 28, 29, 25, 9, 20, 33, 13, 8, 17, 8, 23, 61, 48, 12, 29, 35, 35, 37, 16, 51, 12, 14, 33, 32, 33, 12, 10, 26, 20, 10, 18, 33, 22, 26, 43, 11, 41, 34, 37, 43, 12, 14, 16, 16, 9, 22, 11, 16, 23, 15, 10, 21, 27, 20, 21, 22, 51, 13, 12, 30, 71, 60, 19, 32, 12, 17, 33, 26, 28, 25, 17, 33, 28, 39, 20, 19, 9, 19, 44, 13, 33, 33, 24, 17, 21, 42, 34, 40, 21, 49, 46, 15, 29, 29, 14, 10, 26, 16, 6, 42, 10, 12, 12, 10, 17, 27, 22, 46, 18, 35, 30, 19, 30, 24, 63, 36, 47, 12, 31, 17, 36, 20, 9, 44, 46, 26, 11, 45, 35, 11, 32, 45, 16, 29, 11, 17, 19, 7, 14, 64, 29, 17, 36, 16, 18, 15, 35, 53, 35, 28, 14, 20, 55, 19, 41, 38, 12, 16, 6, 43, 39, 15, 65, 16, 19, 19, 10, 56, 27, 14, 12, 16, 26, 13, 15, 15, 64, 19, 38, 13, 31, 14, 16, 8, 15, 46, 24, 44, 24, 25, 22, 17, 33, 30, 29, 21, 48, 13, 31, 22, 13, 10, 51, 35, 49, 25, 11, 27, 48, 14, 20, 21, 36, 19, 19, 42, 17, 28, 28, 13, 22, 7, 31, 47, 9, 22, 13, 28, 14, 19, 25, 22, 60, 44, 12, 8, 7, 8, 10, 13, 27, 31, 11, 67, 14, 15, 9, 18, 22, 31, 23, 42, 22, 35, 19, 39, 21, 23, 53, 21, 12, 29, 14, 21, 15, 30, 29, 29, 15, 43, 11, 16, 16, 10, 20, 12, 56, 7, 12, 20, 28, 17, 20, 14, 36, 14, 38, 23, 22, 31, 21, 13, 25, 43, 11, 39, 9, 18, 36, 37, 9, 35, 9, 16, 18, 46, 58, 33, 44, 33, 53, 20, 33, 15, 39, 13, 22, 32, 49, 68, 9, 15, 24, 24, 9, 23, 29, 34, 18, 18, 22, 16, 10, 11, 37, 6, 22, 20, 16, 13, 19, 28, 11, 32, 21, 27, 9, 25, 33, 40, 19, 47, 47, 17, 18, 15, 29, 12, 19, 28, 12, 8, 19, 12, 26, 23, 31, 29, 11, 9, 33, 16, 13, 39, 18, 41, 15, 21, 32, 32, 66, 18, 19, 37, 17, 13, 19, 18, 23, 17, 27, 25, 12, 19, 12, 11, 15, 26, 19, 21, 29, 8, 26, 15, 40, 23, 30, 21, 15, 7, 7, 16, 19, 30, 23, 14, 48, 29, 12, 30, 18, 32, 18, 10, 9, 53, 28, 24, 28, 12, 52, 11, 23, 18, 26, 22, 9, 10, 32, 27, 37, 18, 27, 15, 22, 62, 21, 29, 14, 20, 20, 33, 41, 15, 12, 24, 22, 25, 11, 24, 35, 42, 10, 53, 40, 33, 17, 60, 28, 12, 17, 30, 10, 37, 22, 16, 54, 23, 57, 26, 5, 48, 23, 8, 28, 7, 16, 26, 21, 33, 9, 27, 44, 64, 13, 37, 32, 24, 37, 38, 16, 17, 21, 15, 22, 39, 55, 10, 18, 47, 37, 54, 30, 37, 32, 8, 15, 15, 40, 31, 13, 24, 15, 33, 39, 54, 27, 46, 14, 27, 53, 27, 17, 9, 29, 40, 16, 29, 12, 18, 26, 22, 30, 25, 44, 24, 49, 45, 65, 26, 42, 50, 25, 24, 24, 10, 16, 37, 46, 35, 12, 6, 64, 29, 14, 22, 64, 56, 18, 14, 27, 18, 16, 23, 45, 26, 17, 75, 18, 37, 56, 61, 37, 28, 42, 21, 18, 14, 36, 12, 15, 24, 27, 25, 16, 22, 23, 43, 42, 13, 10, 28, 29, 14, 14, 30, 53, 14, 22, 24, 31, 13, 27, 70, 12, 20, 21, 13, 36, 61, 17, 20, 26, 22, 21, 37, 29, 14, 22, 22, 69, 7, 26, 20, 33, 9, 33, 45, 17, 12, 36, 13, 29, 16, 13, 38, 22, 21, 18, 37, 26, 12, 25, 20, 38, 29, 14, 22, 25, 64, 35, 48, 12, 14, 34, 48, 37, 8, 29, 19, 13, 34, 37, 33, 16, 14, 11, 13, 17, 12, 23, 23, 17, 40, 20, 34, 14, 44, 27, 56, 29, 21, 42, 70, 13, 11, 19, 15, 11, 24, 25, 20, 35, 37, 30, 33, 19, 39, 30, 11, 8, 17, 31, 25, 10, 28, 16, 9, 14, 16, 15, 11, 26, 16, 35, 36, 32, 15, 14, 45, 15, 26, 23, 24, 27, 14, 26, 53, 30, 9, 21, 24, 34, 26, 26, 8, 27, 47, 18, 28, 30, 19, 35, 11, 24, 13, 22, 22, 14, 16, 38, 14, 11, 15, 7, 10, 17, 16, 21, 73, 42, 50, 14, 15, 26, 29, 34, 50, 30, 15, 45, 38, 32, 26, 35, 20, 12, 37, 24, 51, 11, 20, 32, 23, 33, 41, 14, 47, 20, 16, 11, 23, 35, 40, 27, 15, 37, 16, 47, 30, 18, 18, 55, 19, 21, 15, 40, 38, 26, 23, 20, 22, 49, 11, 21, 13, 12, 55, 34, 20, 35, 14, 13, 19, 23, 14, 23, 19, 16, 14, 33, 11, 28, 33, 21, 6, 41, 30, 24, 31, 22, 11, 68, 31, 44, 38, 30, 70, 30, 7, 16, 9, 46, 6, 43, 22, 15, 20, 27, 10, 14, 14, 14, 23, 14, 21, 48, 24, 35, 23, 25, 37, 8, 29, 17, 39, 35, 26, 18, 13, 21, 12, 30, 19, 23, 16, 13, 46, 68, 30, 61, 51, 16, 35, 41, 15, 10, 31, 20, 58, 14, 20, 17, 16, 13, 20, 26, 10, 25, 71, 16, 7, 55, 59, 19, 14, 10, 21, 38, 19, 59, 35, 33, 27, 20, 21, 19, 30, 35, 30, 24, 20, 11, 19, 16, 61, 37, 28, 9, 41, 14, 48, 31, 16, 24, 17, 39, 35, 10, 28, 29, 15, 19, 43, 7, 48, 11, 46, 15, 15, 11, 19, 22, 18, 30, 12, 33, 50, 21, 25, 9, 13, 12, 60, 35, 28, 57, 11, 20, 30, 32, 37, 25, 20, 5, 21, 38, 18, 11, 17, 25, 27, 30, 36, 17, 24, 43, 35, 10, 23, 13, 21, 21, 16, 7, 35, 21, 41, 47, 33, 55, 10, 35, 16, 21, 70, 8, 41, 52, 19, 39, 23, 37, 40, 35, 29, 11, 15, 5, 24, 31, 24, 16, 34, 22, 12, 32, 29, 12, 14, 22, 24, 43, 18, 21, 15, 18, 7, 8, 29, 8, 27, 25, 49, 32, 17, 11, 9, 57, 25, 14, 9, 29, 26, 10, 17, 21, 16, 27, 9, 26, 19, 42, 17, 19, 32, 9, 19, 21, 27, 9, 15, 12, 19, 8, 9, 47, 41, 9, 22, 39, 19, 24, 25, 8, 27, 18, 46, 32, 22, 14, 18, 53, 21, 14, 73, 55, 27, 17, 18, 21, 22, 29, 19, 25, 15, 24, 46, 18, 17, 43, 19, 28, 36, 12, 23, 10, 16, 13, 20, 17, 32, 33, 11, 32, 24, 49, 5, 28, 17, 28, 17, 6, 55, 66, 13, 29, 53, 21, 21, 14, 9, 21, 25, 20, 13, 61, 17, 11, 25, 19, 14, 27, 43, 33, 21, 12, 42, 31, 29, 19, 20, 23, 41, 14, 21, 13, 24, 32, 13, 57, 13, 9, 20, 47, 26, 29, 19, 50, 31, 12, 13, 37, 13, 25, 11, 27, 36, 19, 25, 6, 32, 16, 29, 26, 22, 16, 5, 38, 15, 14, 33, 15, 25, 22, 17, 26, 38, 25, 19, 39, 36, 48, 17, 16, 32, 30, 33, 35, 26, 14, 30, 9, 12, 23, 7, 19, 11, 17, 18, 28, 58, 68, 21, 36, 10, 29, 10, 10, 23, 70, 32, 30, 18, 27, 13, 49, 9, 14, 15, 53, 22, 35, 35, 19, 25, 18, 37, 61, 15, 62, 24, 17, 48, 48, 14, 16, 18, 33, 9, 10, 25, 24, 20, 11, 32, 15, 18, 29, 22, 29, 37, 33, 60, 28, 14, 30, 25, 50, 14, 36, 13, 30, 12, 31, 10, 20, 19, 39, 19, 55, 14, 17, 25, 35, 22, 14, 19, 19, 7, 34, 11, 43, 20, 28, 19, 35, 24, 18, 33, 19, 28, 11, 24, 33, 42, 26, 12, 8, 13, 36, 12, 15, 42, 15, 9, 15, 10, 57, 8, 34, 9, 39, 34, 46, 19, 49, 19, 60, 15, 15, 18, 21, 8, 23, 12, 21, 55, 25, 22, 23, 37, 24, 34, 17, 13, 48, 13, 14, 22, 37, 11, 24, 40, 10, 11, 13, 39, 18, 57, 28, 50, 41, 11, 30, 38, 9, 46, 12, 12, 34, 19, 21, 25, 59, 8, 9, 16, 11, 29, 30, 38, 10, 60, 26, 29, 20, 39, 35, 33, 15, 15, 27, 24, 13, 23, 19, 11, 38, 16, 25, 16, 15, 50, 19, 35, 24, 12, 25, 14, 40, 21, 22, 32, 60, 12, 21, 33, 33, 34, 24, 18, 18, 24, 30, 26, 17, 8, 28, 17, 15, 10, 20, 38, 29, 50, 35, 26, 8, 23, 30, 41, 55, 27, 35, 24, 31, 15, 36, 67, 19, 22, 23, 26, 9, 40, 18, 57, 30, 53, 38, 26, 13, 31, 19, 14, 60, 67, 16, 20, 34, 27, 9, 34, 24, 30, 25, 30, 31, 33, 14, 12, 28, 8, 54, 14, 17, 9, 14, 40, 14, 13, 52, 10, 12, 17, 13, 14, 21, 45, 29, 17, 63, 10, 18, 29, 59, 24, 10, 22, 45, 9, 24, 17, 22, 25, 24, 15, 22, 15, 11, 29, 10, 17, 34, 28, 25, 27, 29, 8, 20, 76, 27, 42, 30, 44, 18, 22, 19, 20, 34, 9, 21, 24, 36, 44, 25, 26, 28, 21, 32, 18, 32, 26, 11, 27, 14, 49, 25, 24, 18, 37, 26, 30, 53, 63, 33, 58, 12, 22, 15, 12, 13, 32, 15, 59, 10, 16, 25, 38, 26, 20, 12, 8, 49, 24, 26, 15, 42, 31, 15, 34, 14, 29, 50, 24, 17, 41, 22, 49, 19, 32, 28, 7, 11, 22, 50, 31, 17, 31, 32, 35, 16, 24, 18, 44, 12, 21, 27, 18, 23, 21, 8, 13, 26, 10, 32, 17, 25, 12, 38, 11, 22, 34, 59, 13, 9, 7, 22, 24, 16, 22, 37, 26, 19, 29, 23, 23, 21, 27, 28, 10, 37, 23, 29, 10, 12, 35, 40, 21, 46, 35, 29, 52, 27, 20, 20, 53, 48, 16, 20, 25, 25, 22, 46, 49, 41, 29, 14, 13, 11, 24, 50, 23, 21, 25, 24, 29, 11, 53, 28, 18, 12, 12, 18, 38, 24, 12, 11, 9, 45, 14, 23, 14, 72, 44, 15, 36, 10, 65, 10, 24, 43, 22, 30, 21, 17, 33, 28, 18, 27, 32, 14, 21, 10, 36, 48, 8, 7, 35, 32, 6, 22, 46, 37, 23, 13, 15, 26, 13, 18, 16, 32, 10, 17, 28, 13, 29, 41, 39, 18, 26, 63, 21, 18, 36, 14, 11, 18, 46, 17, 13, 10, 34, 12, 17, 26, 43, 21, 11, 52, 32, 10, 28, 18, 12, 26, 67, 26, 63, 40, 24, 27, 9, 49, 14, 34, 10, 21, 48, 20, 38, 9, 51, 54, 8, 32, 24, 12, 39, 30, 14, 22, 9, 11, 16, 27, 24, 29, 48, 11, 38, 30, 25, 22, 14, 36, 43, 27, 24, 15, 11, 18, 26, 12, 25, 18, 12, 11, 21, 28, 31, 32, 26, 16, 33, 13, 21, 10, 33, 59, 17, 21, 22, 43, 63, 13, 29, 31, 21, 15, 16, 14, 19, 14, 24, 12, 30, 21, 26, 32, 22, 10, 25, 16, 31, 11, 20, 21, 9, 31, 31, 26, 34, 12, 10, 20, 16, 17, 36, 24, 45, 40, 53, 9, 32, 47, 11, 16, 45, 46, 39, 31, 32, 27, 19, 29, 27, 35, 15, 33, 71, 36, 14, 25, 23, 49, 44, 12, 13, 12, 34, 15, 24, 32, 20, 13, 14, 20, 27, 16, 17, 13, 41, 17, 41, 41, 14, 43, 41, 24, 19, 54, 13, 30, 27, 16, 38, 30, 13, 25, 40, 34, 11, 21, 22, 8, 11, 24, 17, 25, 8, 10, 20, 33, 17, 35, 26, 32, 18, 27, 10, 23, 16, 14, 24, 66, 19, 37, 42, 26, 37, 21, 52, 7, 61, 33, 12, 20, 29, 8, 33, 29, 20, 25, 33, 25, 28, 35, 12, 26, 25, 22, 9, 28, 16, 34, 23, 16, 29, 22, 30, 15, 41, 13, 19, 54, 37, 47, 30, 16, 19, 21, 14, 6, 14, 57, 28, 25, 57, 21, 20, 21, 37, 10, 40, 13, 44, 52, 8, 32, 16, 34, 34, 11, 46, 21, 40, 15, 29, 17, 19, 12, 13, 26, 17, 8, 40, 33, 17, 20, 40, 19, 39, 24, 37, 19, 42, 7, 27, 13, 12, 13, 12, 71, 11, 54, 29, 28, 30, 21, 12, 10, 34, 19, 13, 11, 38, 11, 27, 20, 47, 43, 39, 18, 6, 9, 16, 58, 13, 30, 24, 42, 62, 26, 25, 11, 22, 19, 16, 14, 24, 14, 20, 14, 38, 11, 10, 16, 34, 38, 14, 43, 34, 7, 21, 11, 18, 12, 24, 17, 11, 38, 17, 8, 22, 24, 37, 35, 10, 8, 15, 49, 15, 18, 19, 27, 14, 30, 25, 24, 16, 46, 13, 9, 15, 13, 17, 27, 14, 27, 40, 21, 11, 23, 10, 14, 10, 19, 11, 21, 54, 17, 36, 19, 25, 42, 14, 13, 10, 20, 14, 28, 23, 20, 51, 56, 27, 10, 25, 43, 15, 17, 23, 31, 16, 42, 10, 22, 54, 32, 33, 22, 12, 41, 18, 31, 39, 48, 24, 29, 18, 9, 43, 13, 32, 17, 22, 49, 12, 16, 19, 17, 12, 21, 24, 16, 17, 30, 23, 24, 45, 23, 51, 11, 13, 23, 26, 54, 46, 25, 29, 19, 25, 12, 13, 38, 14, 41, 13, 14, 27, 19, 19, 12, 20, 9, 28, 27, 19, 46, 18, 25, 31, 27, 36, 19, 13, 12, 48, 35, 30, 37, 45, 7, 23, 39, 37, 22, 14, 69, 25, 17, 40, 14, 13, 13, 12, 19, 34, 20, 29, 12, 32, 43, 22, 12, 16, 26, 23, 18, 13, 38, 24, 17, 15, 64, 26, 27, 69, 37, 14, 48, 17, 60, 24, 37, 38, 31, 28, 12, 41, 10, 39, 21, 10, 66, 16, 10, 17, 16, 20, 48, 29, 13, 7, 11, 22, 17, 11, 13, 36, 22, 19, 13, 22, 46, 14, 16, 19, 44, 32, 58, 9, 12, 33, 17, 26, 28, 26, 9, 31, 40, 23, 12, 13, 20, 40, 11, 17, 30, 7, 8, 16, 38, 28, 19, 19, 16, 23, 42, 11, 22, 13, 58, 11, 25, 17, 18, 17, 14, 26, 30, 17, 22, 52, 20, 31, 14, 14, 33, 12, 7, 13, 27, 23, 22, 38, 22, 13, 43, 46, 19, 63, 24, 27, 29, 17, 20, 7, 12, 21, 27, 15, 23, 43, 18, 21, 13, 31, 66, 15, 17, 14, 51, 40, 21, 14, 45, 58, 19, 7, 59, 26, 33, 68, 14, 17, 43, 34, 10, 26, 32, 17, 14, 27, 11, 8, 19, 24, 27, 16, 45, 21, 30, 30, 10, 20, 12, 19, 14, 20, 14, 16, 19, 31, 16, 66, 34, 10, 42, 29, 33, 22, 17, 16, 9, 22, 18, 13, 10, 26, 19, 27, 34, 51, 15, 30, 29, 36, 20, 14, 13, 10, 18, 20, 6, 28, 15, 18, 25, 15, 10, 46, 21, 30, 32, 27, 37, 13, 53, 16, 14, 44, 23, 39, 33, 23, 15, 17, 39, 16, 10, 27, 27, 44, 19, 8, 20, 46, 32, 31, 15, 14, 49, 24, 14, 33, 40, 21, 31, 55, 8, 25, 8, 15, 46, 19, 17, 13, 36, 65, 34, 57, 32, 64, 19, 14, 22, 5, 20, 39, 26, 28, 23, 42, 10, 10, 17, 19, 38, 34, 27, 36, 42, 21, 17, 20, 11, 9, 49, 24, 41, 15, 14, 27, 15, 9, 26, 16, 49, 54, 27, 31, 14, 10, 40, 18, 15, 10, 11, 40, 28, 24, 41, 18, 58, 41, 41, 47, 48, 43, 31, 39, 10, 10, 29, 11, 20, 53, 25, 29, 15, 23, 56, 24, 25, 10, 16, 16, 14, 46, 27, 11, 14, 17, 10, 19, 66, 29, 61, 26, 37, 12, 24, 26, 12, 26, 23, 26, 14, 40, 11, 11, 52, 75, 19, 13, 10, 10, 36, 34, 36, 23, 50, 28, 14, 14, 28, 11, 30, 24, 13, 22, 29, 21, 7, 42, 27, 22, 40, 22, 26, 17, 21, 32, 39, 17, 29, 16, 22, 25, 19, 29, 19, 19, 34, 51, 30, 28, 26, 14, 13, 64, 46, 29, 34, 10, 12, 10, 38, 11, 53, 16, 30, 15, 59, 23, 26, 16, 17, 16, 26, 9, 16, 15, 25, 27, 48, 34, 9, 11, 33, 33, 40, 17, 17, 14, 21, 45, 31, 13, 13, 60, 25, 25, 8, 9, 37, 39, 42, 23, 21, 44, 14, 25, 15, 12, 23, 19, 33, 38, 26, 10, 31, 41, 15, 33, 12, 17, 27, 24, 12, 12, 6, 19, 8, 24, 10, 62, 43, 63, 34, 38, 30, 23, 33, 33, 22, 9, 14, 18, 18, 42, 16, 25, 18, 34, 11, 13, 29, 9, 65, 11, 55, 44, 45, 18, 22, 10, 17, 29, 31, 13, 16, 60, 26, 28, 14, 27, 6, 22, 33, 26, 14, 11, 23, 30, 18, 43, 20, 37, 56, 31, 37, 33, 30, 50, 11, 32, 15, 14, 12, 11, 9, 13, 10, 42, 18, 12, 38, 10, 62, 48, 13, 13, 23, 22, 17, 20, 27, 29, 15, 19, 33, 15, 18, 46, 13, 21, 17, 28, 24, 26, 19, 26, 18, 65, 16, 16, 41, 11, 18, 46, 24, 19, 32, 12, 47, 27, 32, 24, 42, 6, 23, 18, 26, 10, 40, 7, 29, 18, 12, 18, 10, 51, 39, 63, 19, 16, 14, 33, 30, 18, 22, 31, 32, 13, 20, 24, 23, 29, 25, 56, 30, 22, 18, 47, 25, 7, 27, 13, 8, 7, 29, 36, 21, 24, 17, 26, 24, 33, 20, 13, 21, 18, 28, 39, 25, 17, 58, 15, 29, 12, 13, 50, 15, 28, 17, 42, 13, 12, 32, 12, 44, 32, 47, 19, 23, 19, 53, 43, 16, 27, 32, 19, 37, 14, 14, 15, 34, 11, 47, 10, 22, 17, 27, 39, 26, 16, 23, 22, 11, 15, 11, 34, 22, 14, 17, 23, 45, 13, 47, 11, 48, 14, 19, 28, 36, 20, 40, 30, 50, 26, 38, 10, 23, 26, 32, 13, 23, 64, 26, 15, 29, 18, 26, 12, 40, 19, 17, 32, 28, 51, 22, 54, 26, 14, 29, 12, 22, 25, 41, 5, 14, 25, 25, 12, 36, 29, 24, 20, 37, 29, 33, 20, 8, 10, 52, 12, 34, 23, 38, 44, 37, 16, 25, 24, 16, 20, 17, 17, 21, 21, 28, 45, 18, 19, 28, 28, 29, 29, 27, 29, 25, 32, 35, 34, 28, 14, 42, 31, 15, 10, 21, 40, 28, 48, 30, 38, 40, 12, 29, 23, 18, 14, 19, 11, 32, 11, 43, 13, 51, 12, 31, 39, 13, 10, 28, 35, 17, 20, 16, 15, 52, 7, 55, 18, 17, 23, 33, 37, 17, 42, 7, 28, 29, 39, 30, 38, 23, 18, 43, 26, 32, 23, 30, 34, 24, 36, 19, 37, 46, 32, 27, 13, 41, 29, 17, 16, 48, 8, 12, 45, 12, 21, 33, 23, 57, 20, 7, 46, 34, 26, 18, 36, 14, 43, 25, 24, 31, 41, 19, 29, 31, 19, 16, 55, 15, 17, 11, 14, 30, 32, 12, 48, 24, 22, 42, 55, 26, 42, 25, 42, 12, 23, 10, 13, 42, 44, 16, 8, 30, 26, 25, 20, 13, 28, 15, 14, 20, 35, 14, 12, 17, 19, 20, 33, 45, 31, 41, 23, 10, 10, 36, 16, 32, 57, 50, 11, 10, 19, 20, 27, 16, 14, 7, 36, 12, 11, 64, 25, 28, 15, 38, 13, 13, 14, 38, 32, 53, 20, 19, 23, 63, 18, 13, 11, 36, 25, 17, 40, 18, 57, 26, 10, 17, 16, 71, 6, 32, 36, 7, 13, 36, 14, 21, 13, 16, 54, 27, 22, 23, 57, 22, 51, 12, 29, 34, 22, 39, 12, 25, 25, 60, 29, 16, 16, 12, 50, 36, 11, 53, 64, 40, 12, 28, 21, 25, 25, 21, 18, 16, 11, 35, 22, 29, 17, 26, 36, 37, 8, 42, 32, 23, 33, 13, 37, 15, 37, 23, 26, 13, 29, 32, 33, 14, 24, 11, 19, 10, 29, 9, 24, 25, 9, 45, 13, 18, 29, 18, 37, 17, 19, 25, 38, 13, 37, 13, 18, 10, 24, 41, 48, 25, 45, 23, 30, 31, 16, 8, 31, 22, 49, 10, 9, 43, 19, 6, 14, 30, 14, 64, 30, 71, 11, 17, 25, 27, 41, 31, 33, 63, 28, 37, 19, 13, 25, 12, 24, 14, 12, 40, 44, 14, 18, 25, 23, 25, 11, 23, 21, 27, 15, 38, 34, 24, 25, 17, 43, 15, 45, 30, 9, 51, 58, 25, 32, 25, 8, 52, 25, 10, 63, 28, 39, 43, 49, 13, 34, 18, 34, 13, 18, 29, 16, 25, 21, 33, 29, 13, 25, 51, 47, 23, 15, 29, 23, 15, 28, 14, 32, 20, 29, 20, 13, 26, 27, 17, 18, 19, 45, 26, 35, 13, 29, 14, 26, 20, 35, 13, 11, 12, 40, 51, 17, 11, 15, 22, 8, 25, 22, 34, 21, 24, 25, 14, 19, 20, 14, 15, 25, 5, 43, 49, 19, 20, 12, 9, 9, 13, 19, 65, 43, 16, 65, 19, 16, 16, 14, 16, 44, 6, 9, 22, 15, 14, 6, 34, 56, 11, 15, 12, 27, 11, 43, 18, 25, 8, 11, 11, 32, 49, 24, 15, 12, 23, 13, 17, 25, 36, 16, 36, 12, 22, 37, 21, 16, 22, 14, 29, 50, 38, 24, 50, 27, 25, 32, 9, 21, 27, 40, 36, 21, 47, 9, 28, 52, 9, 14, 11, 12, 19, 27, 16, 25, 31, 22, 18, 18, 9, 34, 20, 14, 16, 38, 49, 56, 23, 17, 15, 29, 20, 63, 22, 13, 10, 8, 18, 22, 8, 11, 33, 11, 17, 33, 36, 18, 43, 47, 20, 26, 12, 17, 11, 42, 16, 7, 18, 33, 14, 32, 11, 10, 74, 54, 8, 39, 17, 52, 33, 35, 48, 37, 20, 13, 19, 7, 16, 15, 16, 17, 13, 14, 59, 16, 36, 18, 19, 18, 15, 8, 25, 18, 37, 9, 48, 6, 21, 12, 10, 22, 23, 31, 28, 21, 20, 21, 11, 16, 22, 61, 25, 12, 24, 19, 28, 11, 13, 25, 16, 11, 36, 21, 15, 39, 14, 23, 65, 20, 18, 52, 25, 15, 12, 42, 17, 36, 15, 31, 62, 36, 15, 17, 19, 14, 10, 24, 21, 23, 43, 24, 11, 20, 32, 26, 15, 39, 10, 59, 30, 54, 24, 23, 15, 34, 40, 17, 25, 15, 41, 15, 42, 33, 32, 23, 54, 22, 19, 27, 32, 13, 11, 43, 34, 25, 72, 15, 19, 36, 21, 14, 31, 26, 16, 18, 49, 31, 10, 12, 43, 23, 13, 22, 35, 28, 22, 25, 25, 71, 22, 21, 18, 18, 23, 19, 27, 45, 29, 28, 25, 36, 26, 20, 16, 31, 20, 16, 46, 38, 40, 21, 15, 19, 14, 66, 39, 11, 10, 48, 23, 11, 14, 49, 44, 15, 21, 24, 12, 60, 19, 12, 34, 59, 15, 15, 19, 13, 14, 28, 30, 10, 32, 23, 15, 8, 15, 27, 46, 51, 29, 35, 33, 13, 12, 40, 16, 39, 18, 18, 11, 16, 32, 17, 13, 41, 15, 30, 21, 14, 27, 12, 38, 41, 36, 35, 34, 40, 13, 39, 17, 23, 16, 15, 41, 24, 29, 28, 46, 34, 30, 19, 10, 9, 45, 21, 72, 13, 23, 30, 35, 55, 15, 22, 13, 47, 46, 15, 20, 21, 14, 70, 31, 24, 24, 7, 13, 13, 10, 22, 13, 17, 9, 32, 49, 32, 39, 21, 20, 9, 32, 26, 16, 22, 16, 10, 21, 11, 33, 10, 12, 19, 21, 12, 14, 24, 24, 40, 14, 24, 20, 18, 26, 19, 11, 58, 41, 18, 47, 27, 53, 45]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total No. of summary : {len(summary_tokens)}\")\n",
    "print(f\"Each summary Tokens : {summary_token_lens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].hist(dialogue_token_lens, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(summary_token_lens, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The provided function convert_examples_to_features and its application via dataset_samsum.map() is used for preprocessing and tokenizing a dataset to prepare it for training a model. This preprocessing step is essential in machine learning tasks, especially when dealing with text data. It converts raw text into token IDs that the model can understand and process.\n",
    "\n",
    "#### This function, convert_examples_to_features, takes in a batch of examples from the dataset and returns tokenized features suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    # Encode the input dialogues\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "    # Encode the summaries (targets) using the tokenizer with target-specific settings\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    # Return the encoded features\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'dataset_samsum_pt' is the final dataset where each example has been preprocessed and tokenized according to the function. It will contain the tokenized dialogues, attention masks, and target labels, to pass model for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3db0c11bb44f6dba6f265a5c7ffdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4144: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf091a1498e14be48c6f0831b0486e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85499091b22346d889798690c8ed77f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the dataset_samsum_pt Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.', 'input_ids': [12195, 151, 125, 7091, 3659, 107, 842, 119, 245, 181, 152, 10508, 151, 7435, 147, 12195, 151, 125, 131, 267, 650, 119, 3469, 29344, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [12195, 7091, 3659, 111, 138, 650, 10508, 181, 3469, 107, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_samsum_pt[\"train\"][0])  # Print the first example from the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a data collator for sequence-to-sequence tasks\n",
    "#### This handles dynamic padding for input and target sequences and creates attention masks.\n",
    "#### It ensures all sequences in a batch have the same length and correctly masks padding tokens.\n",
    "#### The collator also uses the provided tokenizer to handle any model-specific padding requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m----> 3\u001b[0m trainer_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/pegasus-samsum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Directory to save model checkpoints and logs.\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Number of epochs to train the model.\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Number of steps to perform learning rate warmup.\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Batch size for training per device (e.g., GPU).\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Batch size for evaluation per device.\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Weight decay (L2 regularization) applied during training.\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Number of steps between logging events.\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Evaluation strategy: evaluate the model every `eval_steps` steps.\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Number of steps between evaluations.\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Number of steps between saving model checkpoints.\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Number of steps to accumulate gradients before performing a backward/update pass.\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:131\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\training_args.py:1730\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1733\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1737\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\training_args.py:2227\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2226\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\utils\\generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\saima\\Documents\\virtualenvs\\text_summarization_huggingFace\\Lib\\site-packages\\transformers\\training_args.py:2103\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2106\u001b[0m         )\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2108\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='/content/pegasus-samsum',            # Directory to save model checkpoints and logs.\n",
    "    num_train_epochs=1,                     # Number of epochs to train the model.\n",
    "    warmup_steps=500,                      # Number of steps to perform learning rate warmup.\n",
    "    per_device_train_batch_size=1,         # Batch size for training per device (e.g., GPU).\n",
    "    per_device_eval_batch_size=1,          # Batch size for evaluation per device.\n",
    "    weight_decay=0.01,                     # Weight decay (L2 regularization) applied during training.\n",
    "    logging_steps=10,                     # Number of steps between logging events.\n",
    "    evaluation_strategy='steps',           # Evaluation strategy: evaluate the model every `eval_steps` steps.\n",
    "    eval_steps=500,                        # Number of steps between evaluations.\n",
    "    save_steps=1e6,                        # Number of steps between saving model checkpoints.\n",
    "    gradient_accumulation_steps=16         # Number of steps to accumulate gradients before performing a backward/update pass.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the Trainer with training arguments and datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_pegasus,                       \u001b[38;5;66;03m# The pre-trained PEGASUS model to be trained\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     args\u001b[38;5;241m=\u001b[39m\u001b[43mtrainer_args\u001b[49m,                         \u001b[38;5;66;03m# Training arguments including hyperparameters and configurations\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,                       \u001b[38;5;66;03m# The tokenizer used for encoding the input and output texts\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mseq2seq_data_collator,       \u001b[38;5;66;03m# Data collator to dynamically pad inputs and labels for batches\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset_samsum_pt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# The training dataset with tokenized input and output sequences\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mdataset_samsum_pt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# The validation dataset for evaluating model performance during training\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer_args' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer with training arguments and datasets\n",
    "trainer = Trainer(\n",
    "    model=model_pegasus,                       # The pre-trained PEGASUS model to be trained\n",
    "    args=trainer_args,                         # Training arguments including hyperparameters and configurations\n",
    "    tokenizer=tokenizer,                       # The tokenizer used for encoding the input and output texts\n",
    "    data_collator=seq2seq_data_collator,       # Data collator to dynamically pad inputs and labels for batches\n",
    "    train_dataset=dataset_samsum_pt[\"train\"],  # The training dataset with tokenized input and output sequences\n",
    "    eval_dataset=dataset_samsum_pt[\"validation\"] # The validation dataset for evaluating model performance during training\n",
    ")\n",
    "\n",
    "# Explanation:\n",
    "# The Trainer class is a high-level API provided by the transformers library, designed to simplify the training and evaluation\n",
    "# of transformer models. Here, it is configured with the PEGASUS model, tokenizer, data collator, and the tokenized datasets.\n",
    "# This setup allows for easy management of the training process, including batching, padding, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summarization_huggingFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
